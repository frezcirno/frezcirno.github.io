[{"title":"reading-books","url":"/2024/04/15/reading-books/","content":"值得一看的技术类书籍列表\n"},{"title":"LLVM 中间语言参考","url":"/2023/12/22/llvm/","content":"#概述\nLLVM is a Static Single Assignment (SSA) based representation that provides type safety, low-level operations, flexibility, and the capability of representing ‘all’ high-level languages cleanly\n更严谨, 更 formal 的 C 语言\nLLVM IR 有三种形态: 位于 llvm 工具内存中, 以二进制格式存储(.bc 文件), 以文本格式存储(.ll 文件).\n用 llvm-as 可以把 .ll 格式转换为 .bc 格式, llvm-dis 反之\nLLVM IR 由和高级语言类似的语句组成.\n变量采用 SSA 模式, 不能被重复赋值\n#注释\n从 ; 到行尾的部分是注释\n#局部变量\nLLVM IR 中的变量存储临时结果\n变量名字以 % 开头, 变量名中可以有 . 等字符, 可以是纯数字\n变量不需要提前声明, 赋值即定义变量\n%1 = mul i32 %0, 7          ; v1 = v0 * 7%2 = zext i1 %1 to i32      ; v2 = (int)v1\n#全局变量\n全局变量名字以 @ 开头, 其他与局部变量类似\n#类型\nLLVM IR 必须指定每个变量, 函数的类型\n不会发生自动类型转换 避免歧义\n#函数\ndeclare 声明函数\ndeclare &lt;ResultType&gt; @Name (argument*)argument ::= &lt;type&gt; [parameter Attrs] [name]\ndefine 定义函数\ndefine &lt;ResultType&gt; @Name (argument*) &#123;    ...&#125;argument ::= &lt;type&gt; [parameter Attrs] [name]\n#字符串\n和 c 语言的一样\n#标签 label\nLLVM IR 的标签和 C 语言中类似\n#类型系统\n\n\nvoid 无值 无大小\n\n\niN N 位整数\n\n\nhalf/bfloat/float/double/fp128 浮点数\n\n\n&lt;ty&gt;* 指针类型\n\n\nlabel 标签类型\n\n\ntoken token 类型\n\n\n[&lt;size&gt; x &lt;ty&gt;] 数组类型\n@array = global [42 x i32] zeroinitializer\n\n\ntype &#123; &lt;ty&gt; [, &lt;ty&gt;]* &#125; 结构体类型\n\n\ntype &lt;&#123; &lt;ty&gt; [, &lt;ty&gt;]* &#125;&gt; 压缩结构体类型 不做对齐\n\n\n#常量 Const\n\ntrue/false i1 类型\nnull 指针类型\nnone token 类型\n\n#指令系统\n#控制流指令\n\nret\nbr\nswitch\nindirectbr\ninvoke\n\n#算术指令\n\nfneg\nadd/fadd\nsub/fsub\nmul/fmul\nudiv/sdiv/fdiv\nurem/srem/frem\nshl\nlshr/ashr\nand/or/xor\n\n#内存指令\n#alloca 分配栈内存\n%ptr = alloca &lt;type&gt;\n背后是栈指针的改变\n#load store 读取/写入值\n&lt;result&gt; = load [volatile] &lt;ty&gt;, ptr &lt;pointer&gt;[, align &lt;alignment&gt;]store [volatile] &lt;ty&gt; &lt;value&gt;, ptr &lt;pointer&gt;[, align &lt;alignment&gt;]\n#指针偏移 Get Element Pointer (GEP)\n&lt;result&gt; = getelementptr &lt;ty&gt;, &lt;ty&gt;* &lt;ptrval&gt;, [i32 &lt;idx&gt;]+; 语法: getelementptr 结果类型  数组类型* 数组变量  索引(可以有多个)\nGEP 仅计算指针加上偏移后的值, 本身并不进行任何数据的访问或修改\n偏移按照指向对象的类型, 相当于 C 语言里面的 数组+偏移量 的模式\n#类型转换指令\n\ntrunc … to\nzext … to\nsext … to\n\n#杂项指令\n\nicmp 整数向量比较\nfcmp 浮点数向量比较\nselect 类似三元表达式\nfreeze : stop propagation of undef and poison values\ncall 函数调用\nva_arg 变长参数\n\n#Phi 指令\n有时需要对已经定义的变量赋一个新的值 (例如循环变量)\nSSA 不允许这样做 -&gt; phi 运算\n&lt;result&gt; = phi &lt;ty&gt; [&lt;val0&gt;, &lt;label0&gt;], [&lt;val1&gt;, &lt;label1&gt;] …\n根据前一个基本块是哪一个 选择对应的值\n#内置函数\n\nva_start/va_end/va_copy\nabs/max/min/sin/cos/…\nctlz/fshl/…\n\n","categories":["compiler-principle"],"tags":["编译原理","程序分析"]},{"title":"《编译原理》（龙书）第二版阅读记录","url":"/2024/04/15/dragon-book/","content":"标注:\n\n❓ = 不了解的\n❗️ = 想看的\n\n#Chapter 1. 编译器结构\n\nCISC &amp; RISC\n词法分析\n…\n\n#Chapter 2. 简易语法制导翻译器实现\n使用 java 实现的编译器前端\n整体流程: 源代码 – 词法分析 – 语法分析 – 中间代码生成 – 三地址代码\n#语法\n文法 (a.k.a. 上下文无关文法 CFG) 用来描述某一类语言的模式\n文法不允许二义性 i.e., 对同一段文本串(终结符号串)有多种解释方式\n利用附加规则 (结合性 &amp; 优先级) 消除二义性\n#语法制导翻译\n语法制导翻译: 在文法产生式上 进行某些逻辑处理 实现翻译\n#语法分析\n\n对于任何上下文无关文法，我们都可以构造出一个时间复杂度为O(n3)O(n^3)O(n3)的语法分析器，它最多使用O(n3)O(n^3)O(n3)的时间就可以完成一个长度为 n 的符号串的语法分析。\n\n但是现实中往往设计简单的文法 以实现 O(n)的语法分析\n语法分析方法: 自顶向下 自底向上\n#词法分析\n词素 lexem\n#符号表\n符号表一般与编程语言中的作用域(scope)相对应\n可以利用最近嵌套规则\n#生成中间代码\n两种最重要的 IR 的形式\n\n树形表示：包括 语法分析树 和 AST\n线性表示：尤其是三地址(代)码\n\n三地址码：x=y op zx = y~op~zx=y op z\n或数组形式\nx[y]=zx [ y ] = zx[y]=z\nx=y[z]x = y [ z ]x=y[z]\nlvalue和rvalue伪代码\n\n\n#Chapter 3. 词法分析\n\n正则表达式\nLex\n自动机, DFA &amp; NFA\nNFA 转 DFA\n\n#Chapter 4. 语法分析\n\n上下文无关文法\nLL(1) 文法\nLR 语法分析\nLALR\nYacc\n\n#Chapter 5. 语法制导翻译\n\n语法制导定义 SDD\n合成属性 继承属性\n语法制导翻译 SDT\n\n#Chapter 6. 中间代码生成\n本章关注怎么从 AST 生成 IR，主要方法就是灵活运用 SDT\n\nC 常常被用作 IR\n\n#6.1 DAG 表示语法树\n好处：自动提取公共子表达式，适合优化\n#6.2 三地址(代)码\n两个基本元素：\n\n地址\n\n变量名字\n常量\n临时变量\n\n\n指令\n\n三地址码的实现：\n\n结构体/对象\n四元式 quadruple：result = arg1 op arg2\n三元式 triple：每条指令隐含产生一个结果，参数是指令下标\n间接三元式 indirect triple：解决三元式不适合在中间插入删除的问题\n\n静态单赋值形式 SSA：\n\n静态单赋值形式是另一种 IR 的形式\n需要 Phi 函数解决多分支问题\n三地址码 =&gt; SSA 转换方法\n\n#6.3 类型和声明\n类型表达式 type expression：\n\n表示编程语言中的类型的代数系统\n是树形结构\n一个典型的类型表达式系统：\n\n基本类型 boolean char integer float void\n类名\n数组类型 array(N,类型表达式)array(N, 类型表达式)array(N,类型表达式)\n记录类型 record(Name,类型表达式)record(Name, 类型表达式)record(Name,类型表达式)\n函数类型 s→ts\\rightarrow ts→t\n笛卡儿积(一般是列表或元组) s×ts\\times ts×t\n\n\n\n类型 结构等价 与 名字等价：\n\n❓ 没看懂\n\n类型的翻译：\n\n局部变量的存储布局：\n\n类型宽度：类型的一个对象所需的存储单元数量\n\n计算宽度的 SDT 方案：\n\n暂时不考虑内存对齐要求\n\n\n根据类型可以给变量分配相对地址\n\n分配地址的 SDT 方案\n\n\n\n#6.4 表达式翻译方法\n\n增量翻译：直接输出code而非存储起来\n\n数组的翻译\n#6.5 类型检查\n\n类型检查：给源程序每个部分一个类型表达式，然后确定这些表达式是否符合一组逻辑规则。\n\n能够发现程序中的错误\n\n\nTODO: ❓\n\n#6.6 控制流\n\n直接参考就行了\n\n\n#6.7 回填 &amp; 6.8 语句翻译 &amp; 6.9 过程翻译\n在翻译控制流语句时，经常需要生成向后跳转的跳转指令，到还没有翻译完成的地方。\n例如翻译 if (B) S 指令，第一遍翻译时不知道S后面的指令地址，因此就没办法生成goto的目标；对于switch语句问题则更加明显，需要翻译完switch body才能知道跳转的目标。\n回填 backpatching 技术是一遍扫描内解决这个问题的一个方法，每个跳转指令在生成时都暂时不指定目标，而是后续回填这个目标。\n每个非终结符号新增合成属性：\n\ntruelist，falselist 记录需要回填的跳转目标\nnextlist 记录下一条指令\n\n辅助函数：\n\nmakelist(i) 创建一个列表，包含需要回填的指令i\nmerge(p1, p2) 合并两个列表\nbackpatch(p, i) 把标号i回填到链表p包含的每条指令中\n\n\n\ngoto 语句的翻译：\n\n为每个 label 维护一个patchlist，在知道 label 的目标之后进行回填即可\n\nbreak 和 continue 语句的翻译：\n\n跟踪循环语句\n将 break 语句添加到循环语句的nextlist中\n\nSwitch 的翻译：\n\n翻译选择\n\ncase 数据较少时可以用条件跳转\n较多时可以使用散列表\n直接把值映射到下标\n\n\n条件跳转：把分支语句放在开头会不方便\n\n可以把跳转块放在末尾\n也可以边执行边跳转\n\n\n\n\n过程的翻译：\n\n三地址码表示函数：n=f(a[i])\n翻译成\n\nt_1 = i * 4t_2 = a[t_1]param t_2t_3 = call f, 1n = t_3\n\n函数类型：s-&gt;t\n符号表：\n\n原符号表中增加一个新函数符号；然后压入一个新的函数符号表\n函数形参可以用类似 record 字段的方式处理\n\n\n类型检查\n函数调用\n\n#Chapter 7. 运行时\n\n栈\n堆\n内存分配\nGC ❗️\n\n#Chapter 8. 代码生成\n\n目标机器模型\n基本块\n基本代码生成\n窥孔优化\n寄存器分配\n表达式生成优化\n\n#Chapter 9. 机器无关优化\n\n数据流分析\n常量传播\n冗余消除\n循环 ❓\n基于区域的分析 ❓\n符号分析 ❗️\n\n#Chapter 10. 指令级并行\n#Chapter 11. 并行 &amp; 局部性优化\n#Chapter 12. 过程间分析\n\nDatalog ❗️\n指针分析 ❗️\n\n","categories":["compiler-principle"],"tags":["编译原理"]},{"title":"自己动手写编译器","url":"/2023/12/19/make-a-c-compiler/","content":"\nWhat I cannot create, I do not understand. – Richard Feynman\n\n#目标\n\n实现一个 C-to-assembly compiler\n可以有 pre-processer\n不需要 linker, 生成二进制通过已有的 Assembler\n支持常用的 C99 语法\n需要有编译优化\n\n#词法分析, 语法分析\n\n用已有的工具 flex, bison\n\n#类型系统\nstruct TypeKind &#123; VOID, ...&#125;\n#中间表示 - CMinor\nIR 定义：\n\nARITH arg1 arg2 result\n\nADD/SUB/MUL/MOD/SHL/SHR/AND/OR/XOR\nLT/LE/GT/GE/EQ/NE\n\n\nARITH arg result\n\nNEG/NOT/CAST\n\n\nMOV arg result\nGOTO addr\nIF_GOTO arg label\nUNLESS_GOTO arg label\nIF_REL arg1 arg2 label\n\nLT/LE/GT/GE/EQ/NE\n\n\nPARAM x\ny = CALL p, n\nRETURN y\nGET_ELEMENT array index result\nSTORE_ELEMENT array index value\nADDR arg result\nLOAD arg result\nSTORE arg result\n\n#关键数据结构\n\n\n类型(Type)结构\n\n\n结构体/联合体成员(Member)结构\n\n\n全局变量/函数定义对象(Obj)结构\n\n\n","categories":["compiler-principle"],"tags":["编译原理","编译器"]},{"title":"MimiC编译器源码阅读","url":"/2024/05/06/mimic-reading/","content":"MimiC is a compiler of C subset (extended SysY language) by USTB NSCSCC team.\n#使用说明\n#源语言定义\ncomp_unit       ::= &#123;decl | type_def | func_def&#125;;decl            ::= var_decl | func_decl;type_def        ::= struct_def | enum_def | type_alias;func_def        ::= func_header block;var_decl        ::= type var_def &#123;&quot;,&quot; var_def&#125; &quot;;&quot;;var_def         ::= ID_VAL &#123;&quot;[&quot; expr &quot;]&quot;&#125; [&quot;=&quot; init_val];init_val        ::= expr | &quot;&#123;&quot; [init_val &#123;&quot;,&quot; init_val&#125;] &quot;&#125;&quot;;func_decl       ::= func_header &quot;;&quot;;func_header     ::= type ID_VAL &quot;(&quot; [func_params] &quot;)&quot;;func_params     ::= func_param &#123;&quot;,&quot; func_param&#125;;func_param      ::= type ID_VAL [&quot;[&quot; [expr] &quot;]&quot; &#123;&quot;[&quot; expr &quot;]&quot;&#125;];struct_def      ::= &quot;struct&quot; ID_VAL &quot;&#123;&quot; &#123;struct_elem&#125; &quot;&#125;&quot; &quot;;&quot;;enum_def        ::= &quot;enum&quot; [ID_VAL] &quot;&#123;&quot; enum_elems &quot;&#125;&quot; &quot;;&quot;;type_alias      ::= &quot;typedef&quot; type ID_VAL &quot;;&quot;;struct_elem     ::= type struct_elem_def &#123;&quot;,&quot; struct_elem_def&#125; &quot;;&quot;;struct_elem_def ::= ID_VAL &#123;&quot;[&quot; expr &quot;]&quot;&#125;;enum_elems      ::= ID_VAL [&quot;=&quot; expr] [&quot;,&quot; enum_elems] [&quot;,&quot;];block           ::= &quot;&#123;&quot; &#123;block_item&#125; &quot;&#125;&quot;;block_item      ::= decl | type_def | stmt;stmt            ::= bare | block | if_else | while | control;bare            ::= expr &quot;;&quot;;if_else         ::= &quot;if&quot; &quot;(&quot; expr &quot;)&quot; stmt [&quot;else&quot; stmt];while           ::= &quot;while&quot; &quot;(&quot; expr &quot;)&quot; stmt;control         ::= (&quot;break&quot; | &quot;continue&quot; | (&quot;return&quot; [expr])) &quot;;&quot;;expr            ::= cast &#123;bin_op cast&#125;;cast            ::= &#123;&quot;(&quot; type &quot;)&quot;&#125; unary;unary           ::= &#123;unary_op&#125; factor | &quot;sizeof&quot; (factor | &quot;(&quot; type &quot;)&quot;);factor          ::= value | index | func_call | access | &quot;(&quot; expr &quot;)&quot;;bin_op          ::= &quot;+&quot;   | &quot;-&quot;   | &quot;*&quot;   | &quot;/&quot;   | &quot;%&quot;   | &quot;&amp;&quot;                  | &quot;|&quot;   | &quot;^&quot;   | &quot;&amp;&amp;&quot;  | &quot;||&quot;  | &quot;&lt;&lt;&quot;  | &quot;&gt;&gt;&quot;                  | &quot;==&quot;  | &quot;!=&quot;  | &quot;&lt;&quot;   | &quot;&lt;=&quot;  | &quot;&gt;&quot;   | &quot;&gt;=&quot;                  | &quot;=&quot;   | &quot;+=&quot;  | &quot;-=&quot;  | &quot;*=&quot;  | &quot;/=&quot;  | &quot;%=&quot;                  | &quot;&amp;=&quot;  | &quot;|=&quot;  | &quot;^=&quot;  | &quot;&lt;&lt;=&quot; | &quot;&gt;&gt;=&quot;;unary_op        ::= &quot;+&quot;   | &quot;-&quot;   | &quot;!&quot;   | &quot;~&quot;   | &quot;*&quot;   | &quot;&amp;&quot;;value           ::= INT_VAL | CHAR_VAL | STR_VAL | ID_VAL;index           ::= expr &quot;[&quot; expr &quot;]&quot;;func_call       ::= expr &quot;(&quot; [expr &#123;&quot;,&quot; expr&#125;] &quot;)&quot;;access          ::= factor (&quot;.&quot; | &quot;-&gt;&quot;) ID_VAL;type            ::= prim_type | struct_type | enum_type | const | pointer                  | user_type;prim_type       ::= &quot;void&quot; | [&quot;unsigned&quot;] &quot;int&quot; | &quot;char&quot;;struct_type     ::= &quot;struct&quot; ID_VAL;enum_type       ::= &quot;enum&quot; ID_VAL;const           ::= &quot;const&quot; type;pointer         ::= type &quot;*&quot; &#123;&quot;*&quot;&#125;;user_type       ::= ID_VAL;\n#目标语言\n\nC\nriscv32\naarch32\n\n#构建&amp;使用\nmkdir build &amp;&amp; cd buildcmake ..make -j32\n./mmcc -hUsage: mmcc &lt;INPUT&gt; [OPTIONS...]Arguments:  input                         input source fileOptions:  -h, --help                    show this message  -v, --version                 show version info  -S, --asm                     dump assembly  -O2, --opt-2                  enable level-2 optimization  -o, --output &lt;ARG&gt;            output file, default to stdout  -V, --verbose                 use verbose output  -Werror, --warn-error         treat warnings as errors  -Wall, --warn-all             enable all warnings  -da, --dump-ast               dump AST to output  -di, --dump-ir                dump IR to output  -ps, --pass-stage &lt;ARG&gt;       optimize until specific stage  -ta, --target-arch &lt;ARG&gt;      specify target architecture\n#实现\n#模块划分\n\nback/\ndefine/\ndriver\nfront\nmid\nopt\nutils\n\n#类设计\n\n#Pass 注册\n利用static变量最先执行的特性，由PassManager提供一个注册函数，用于保存当前 pass。\nclass PassManager &#123; public:  // register a new pass  template &lt;typename T&gt;  static PassInfo &amp;RegisterPass(std::string_view name) &#123;    static_assert(!std::is_base_of_v&lt;HelperPass, T&gt;,                  &quot;helper pass is unregisterable&quot;);    auto &amp;passes = GetPasses();    assert(!passes.count(name) &amp;&amp; &quot;pass has already been registered&quot;);    return passes.insert(&#123;name, PassInfo(std::make_unique&lt;T&gt;(), name)&#125;)        .first-&gt;second;  &#125; private:  // get pass info list  static PassInfoMap &amp;GetPasses();&#125;;// In passman.cppPassManager::PassInfoMap &amp;PassManager::GetPasses() &#123;  static PassInfoMap passes;  return passes;&#125;// register a pass#define REGISTER_PASS(cls, name) \\  static PassInfo &amp;pass_##name = PassManager::RegisterPass&lt;cls&gt;(#name)\n// register current passREGISTER_PASS(PhiSimplifyPass, phi_simp)    .set_min_opt_level(1)    .set_stages(PassStage::Promote);\n","categories":["compiler-principle"],"tags":["编译原理","编译器"]},{"title":"编译原理课程笔记 - 大纲","url":"/2020/12/03/note0-index/","content":"#课程简介\n总评 = 考试 * 60% + 作业 * 30% + 平时 * 10%\n#程序语言的发展\n机器语言 -&gt; 汇编语言 -&gt; 高级语言\n#程序的两种执行方式\n\n\n解释方式\n\n\n编译方式\n\n\nJava 认为是解释型语言\n#编译的步骤\n编译过程基本分为五个基本阶段:\n\n词法分析\n语法分析\n语义分析和中间代码生成\n优化\n目标代码生成\n\n#1. 词法分析\n\n词法分析程序又称扫描程序(Scanner)。\n\n任务：读源程序的字符流、识别单词（也称单词符号，或简称符号），如标识符、关键字、常量、界限符等，并转换成内部形式。\n输入：源程序中的字符流\n输出：等长的内部形式，即属性字（单词类型 Token-name, 单词属性 Attribute-value），其中单词属性指向符号表\n\n\n\n输入: 字符流\ncppcode = `int a, b;a = a + 2;`;\n输出: Token 流和对应的符号表\ntokenList = [    &lt;int&gt;    &lt;id,1&gt;    &lt;,&gt;    &lt;id,2&gt;    &lt;;&gt;    &lt;id,1&gt;    &lt;op,EQ&gt;    &lt;id,1&gt;    &lt;+&gt;    &lt;2&gt;    &lt;;&gt;]tokenTable = [    &#123;name: &#x27;a&#x27;, ...&#125;,    &#123;name: &#x27;b&#x27;, ...&#125;]\n\n在词法分析阶段工作所依循的是语言的词法规则。\n描述词法规则的有效工具是正规式和有限自动机。\n方法：状态图；DFA；NFA\n\nDFA 模拟代码：\ns = s0;c = nextChar() ;while ( c != eof ) &#123;    s = move(s, c);    c = nextChar() ;&#125;if ( s is in F ) return &quot; yes &quot; ;else return &quot;no &quot; ;\n#2. 语法分析\n\n语法分析程序又称识别程序(Parser)。\n\n任务：读入由词法分析程序识别出的符号，根据给定语法规则，识别出各个语法单位（如：短语、子句、语句、程序段、程序）,并生成另一种内部表示。\n输入：由词法分析程序识别出并转换的符号\n输出：另一种内部表示，如语法分析树或其它中间表示。\n\n\n语法规则通常用上下文无关文法描述。\n方法：递归子程序法、LR 分析法、算符优先分析法。\n\n输入: 符号流\nsum := first + count * 10\n输出: 语法树\n\n#3.1 语义分析\n#3.2 中间代码生成\n#4. 优化\n\n优化的任务在于对前段产生的中间代码进行加工，把它变换成功能相同，但功效更高的优化了的中间表示代码，以期在最后阶段产生更为高效（省时间和空间）的代码\n优化所依循的原则是程序的等价变换规则\n其方法有：公共子表达式的提取、循环优化、删除无用代码等等。\n\n#5. 目标代码生成\n#遍(Pass)\n对源程序或源程序的中间结果从头到尾扫描一次，并做相关处理，生成新的中间结果或目标程序的过程。\n“遍”是处理数据的一个完整周期，每遍工作从外存上获得前一遍的中间结果（如源程序），完成它所含的有关工作之后，再把结果记录于外存。\n一个编译程序可由一遍、两遍或多遍完成。每一遍可完成不同的阶段或多个阶段的工作。\n\n\n\n\n从时间和空间角度看\n\n\n\n\n多遍编译\n少占内存，多耗时间\n\n\n一遍编译\n多占内存，少耗时间\n\n\n\n#T 形图\n\n\nS:源语言(程序)，Source language(program)\nT:目标语言(程序), target/object language(program)\nI:实现语言, implementation language\n\n用 T 形图表示编译器移植\n\n#特殊\n自编译:\n交叉编译\n自动编译:\nlex, yacc\n#复习\n乔姆斯基文法:\n0/1/2/3\n词法分析: 3 型语法分析: 2 型\nA 卷 - 简单缓考不考-&gt;B 卷-&gt;难\n语法分析:\nLR,SLR,二义文法\n就 5 道大题 5 个部分 占 40%\n平时 60%\n","categories":["compiler-principle"]},{"title":"编译原理课程笔记 - Chapter 1 词法分析","url":"/2020/12/17/note1-lex/","content":"#提纲\n\n#概述\n\n单词级别分析和翻译源程序\n任务: 作为字符串的源程序-&gt;单词符号串\n\n#词法分析器的要求\n单词符号表示通常使用二元式 (单词种别, 单词符号的属性值)\n\n单词种别: 语法分析需要的信息, 通常使用整数编码\n单词(符号属性)值: 编译其他阶段使用\n\n单词如何分类取决于处理上的方便:\n\n标识符: 一般通归为一种\n常数: 按类型分种\n关键字: 可全体为一种, 也可一字一种(更方便)\n运算符: 一般一符一种, 可以把具有一定共性的视为一类\n界符: 一般一符一种\n\n单词符号的属性:\n\n标识符: 存放它符号表项的指针/内部字符串\n常数: 存放它常数表项的指针/二进制形式\n关键字/运算符/界符: 不需要属性\n\n\n实现方式:\n\n完全独立: lex 作为单独一遍\n\n结构简洁, 清晰, 条理化\n\n\n相对独立: 作为 parser 的一个独立子程序, 需要新符号时调用\n\n避免中间文件, 提高效率\n\n\n\n#词法分析器设计\n#lexer 的结构\n源程序 -&gt; 输入缓冲区/预处理子程序/扫描缓冲区/扫描器 -&gt; 单词符号\n预处理子程序: 处理空白符等编辑性字符, 删除注释等\n输入缓冲区: 存放源程序文本输入串的缓冲区\n扫描缓冲区: 设定两个指示器\n#单词符号的识别: 超前搜索\n\n关键字识别\n标识符识别: 一般是字母开头的字母数字串, 一般都跟着算符或者界符, 不难识别\n常数的识别: 有些语言需要使用超前搜索\n算符和界符: 对于 c++中的++,–等需要超前搜索\n\n#词法规则的表示\n大多数 pl 中的单词符号的词法规则可以用正规文法描述\n例如:\n&lt;标识符&gt; -&gt; 字母|&lt;标识符&gt;字母|&lt;标识符&gt;数字&lt;整数&gt; -&gt; 数字|&lt;整数&gt;数字&lt;运算符&gt; -&gt; +|-|×|÷…&lt;界符&gt; -&gt; ;|,|(|)|…\n利用这些规则识别的过程可以用状态转换图来表示, 而状态转换图可以方便地用程序实现\n#TG\n状态转换图 TG: 一个有限有向图, 可用于接受(识别)一定的符号串\n\n结点表示状态, 用圆圈表示\n\n初态: 通常只有一个, 用一条输入弧表示\n终态: 至少有一个, 用双圈表示\n\n\n方向弧表示状态转换, 弧上的标记表示接受的输入字符或字符类\n\n路: 在状态转换图中从初态到某一个终态的弧上的标记序列\n接受(识别): 若存在一条路产生β\\betaβ, 则称状态转换图接受符号串β\\betaβ\n状态转换图能识别的语言: L(TG)能别 TG 接受的符号串的集合\n\n#正规表达式和有限自动机\n#正规式\n字母表Σ\\SigmaΣ上的正规式和正规集递归定义如下：\n\nε\\varepsilonε和φ\\varphiφ都是Σ\\SigmaΣ上的正规式, 它们所表示的正规集分别为{ε}\\{\\varepsilon\\}{ε}和φ\\varphiφ。其中：ε\\varepsilonε为空字符串, φ\\varphiφ为空集\n任意元素a∈Σa\\in\\Sigmaa∈Σ, a 是Σ\\SigmaΣ上的一个正规式, 它所表示的正规集是{a}\\{a\\}{a};\n假定 U 和 V 都是Σ\\SigmaΣ上的正规式, 它们所表示的正规集记为 L(U)和 L(V), 那么(U|V), (U·V)和(U)*都是正规式, 他们所表示的正规集分别记为 L(U)∪L(V), L(U)L(V)和(L(U))*。\n仅由有限次使用上述三步而得到的表达式才是Σ\\SigmaΣ上的正规式, 它们所表示的字集才是Σ\\SigmaΣ上的正规集。\n\n运算:\n\n闭包*\n连接. (可省略)\n或|\n\n运算律:\n\n或交换律\n或结合律\n连接结合律\n或对连接分配律\nεU=Uε=U\\varepsilon U=U \\varepsilon=UεU=Uε=U\n\n例:\n\n以 01 结尾的二进制数串的正规式: (0|1)*01\n能被 5 整除的十进制整数: 0|5|(1|2|3|4|5|6|7|8|9)(0|1|2|3|4|5|6|7|8|9)*(0|5)\n\n#FA\n自动机: 具有离散输入输出的数学模型\n状态 + 输入 + 规则 -&gt; 状态迁移\n\n有限自动机(FA)\n有限状态机(FSM)\n#DFA\nDFA 五元组: M=(S,Σ,δ,S0,F)M=(S,\\Sigma,\\delta,S_0,F)M=(S,Σ,δ,S0​,F)\n\nSSS: 有限的状态集合, 每个元素称为一个状态\nΣ\\SigmaΣ: 有限的输入字母表, 每个元素称为一个输入字符\nδ:S×Σ→S\\delta: S\\times\\Sigma \\rightarrow Sδ:S×Σ→S: 转换函数(状态转移集合)\n\nδ(s,a)=s′\\delta(s, a)=s&#x27;δ(s,a)=s′\n\n\nS0∈SS_0\\in SS0​∈S: 初始状态\nF⊆SF\\subseteq SF⊆S: 终止状态集\n\n状态转换矩阵: DFA 可以用一个矩阵表示, 每行表示一个状态, 每列表示一种输入, 矩阵元素表示δ(s,a)\\delta(s,a)δ(s,a)的值\nDFA 与状态转换图: DFA 可以用 TG 唯一表示\n\n拓展转移函数\n\n接收一个字符串的状态转移函数\nδ′:S×Σ∗→S\\delta&#x27;: S\\times\\Sigma^* \\rightarrow Sδ′:S×Σ∗→S\n\nδ′(s,ε)=s\\delta&#x27;(s, \\varepsilon) = sδ′(s,ε)=s\nδ′(s,ωa)=δ(δ′(s,ω),a)\\delta&#x27;(s, \\omega a) = \\delta(\\delta&#x27;(s,\\omega),a)δ′(s,ωa)=δ(δ′(s,ω),a)\n\n\n\nDFA 接受的字符串\nDFA 接受的语言: L(M)={α∣δ′(s,α)∈F}L(M)=\\{α|\\delta&#x27;(s,α)\\in F\\}L(M)={α∣δ′(s,α)∈F}\n#NFA\nNFA 五元组: M=(S,Σ,δ,S0,F)M=(S,\\Sigma,\\delta,S_0,F)M=(S,Σ,δ,S0​,F)\n\nSSS: 同 DFA\nΣ\\SigmaΣ: 同 DFA\nδ:S×Σ∗→2S(幂集)\\delta: S\\times\\Sigma^* \\rightarrow 2^S(幂集)δ:S×Σ∗→2S(幂集): 转换函数(状态转移集合)\n\nδ(s,a)=S′⊆S\\delta(s, a)=S&#x27;\\subseteq Sδ(s,a)=S′⊆S\n\n\nS0⊆SS_0\\subseteq SS0​⊆S: 非空初态集\nF⊆SF\\subseteq SF⊆S: 终止状态集\n\nNFA 也可以用 TG 和转换矩阵表示\nNFA 的状态是一个集合\n\n\n\n比较\nDFA\nNFA\n\n\n\n\n输入字母\n每个(状态,输入)都有一个δ\\deltaδ\n可能没有δ\\deltaδ/是空转换\n\n\n转移状态\n确定的\n不确定的, 可能有多个\n\n\n\nNFA 的拓展转移函数\n\nδ′:S×Σ∗→2S\\delta&#x27;: S\\times\\Sigma^* \\rightarrow 2^Sδ′:S×Σ∗→2S\n\nδ′(s,ε)={s}\\delta&#x27;(s, \\varepsilon) = \\{s\\}δ′(s,ε)={s}\nδ′(s,ωa)={p∣存在r∈δ′(s,ω)∧p∈δ(r,a)}\\delta&#x27;(s,\\omega a)=\\{p|存在r\\in\\delta&#x27;(s,\\omega )\\wedge p\\in\\delta(r,a)\\}δ′(s,ωa)={p∣存在r∈δ′(s,ω)∧p∈δ(r,a)}\n\n\n\nNFA 接受的字符串\nNFA 接受的语言\n#DFA 和 NFA 的关系 (子集法)\nDFA 是 NFA 的特例, 两者可以相互转化\nε-闭包:\nε_CLOSURE(I)=I∪{q′∣q经任意条ε弧可到达q′,q∈I},I⊆M′\\varepsilon\\_CLOSURE(I)=I\\cup\\{q&#x27;|q经任意条\\varepsilon弧可到达q&#x27;, q\\in I\\}, I\\subseteq M&#x27;\nε_CLOSURE(I)=I∪{q′∣q经任意条ε弧可到达q′,q∈I},I⊆M′\nNFA-&gt;DFA 之子集法:\n\n\n引进新的初态结点 X 和终态结点 Y, 从 X 到所有原始态结点连接一条ε\\varepsilonε弧, 从所有原终态结点到 Y 连接一条ε\\varepsilonε弧\n\n\n对复合的弧进行分裂\n\n\n\n构造状态矩阵\n\n\n\n\n\nI\nIΣiI_{\\Sigma_i}IΣi​​\n…\n\n\n\n\nε_CLOSURE(X)\\varepsilon\\_CLOSURE({X})ε_CLOSURE(X)\n…\n…\n\n\n…\n…\n…\n\n\n\n\n合并相同状态, 重新命名得到新的状态转换矩阵\n\n第一行第一列的状态为始态\n包含 Y 的状态为终态\n\n\n画出状态转换图\n\n例:\n\n\n\n\n\nI\nIaI_{a}Ia​\nIbI_{b}Ib​\n\n\n\n\n{X, 0, Y}\n{1}\n{1}\n\n\n{1}\n{0,Y}\n-\n\n\n{0,Y}\n{1}\n{1}\n\n\n\n\n\n\nI\nIaI_{a}Ia​\nIbI_{b}Ib​\n\n\n\n\n1\n2\n2\n\n\n2\n3\n-\n\n\n3\n2\n2\n\n\n\n\n#正规式和 FA 的关系\n\n正规式和 FA 是等价的\n\n#从 NFA 构造等价的正规式 (简单)\n\n\n引进新的初态结点 X 和终态结点 Y, 从 X 到所有原始态结点连接一条ε\\varepsilonε弧, 从所有原终态结点到 Y 连接一条ε\\varepsilonε弧\n\n\n对复合的弧进行合并\n\n\n\n得到 regex\n\n\n#从正规式构造等价的 NFA (Thompson 算法)\n\n\n例:\n\n#DFA 的化简 (等价状态法)\n将状态集SSS根据能否被输入区分划分为更细的集合\n\n初始划分: Π={终态集I(1),非终态集I(2)}\\Pi=\\{终态集I^{(1)}, 非终态集I^{(2)}\\}Π={终态集I(1),非终态集I(2)}\n检查能否再分\n\n如果对输入字符aaa, 存在I(k)I^{(k)}I(k)接受aaa后不全落在现行Π\\PiΠ的某一个子集中\n就把I(k)I^{(k)}I(k)分成多个组, 使得每个组接受aaa后都落在Π\\PiΠ的同一个子集中\n\n\n重复直到子集数不再增加\n\n\n例题:\n\n初始划分Π0={{A,B,C,D},{E}}\\Pi_0=\\{\\left\\{A,B,C,D\\right\\},\\{E\\}\\}Π0​={{A,B,C,D},{E}}\n考察{A,B,C,D}a={B}⊆{A,B,C,D}\\{A,B,C,D\\}_a=\\{B\\}\\subseteq \\{A,B,C,D\\}{A,B,C,D}a​={B}⊆{A,B,C,D}\n{A,B,C,D}b={C,D,E}\\{A,B,C,D\\}_b=\\{C,D,E\\}{A,B,C,D}b​={C,D,E}, 需要划分, 其中{A,B,C}b={C,D}\\{A,B,C\\}_b=\\{C,D\\}{A,B,C}b​={C,D}, {D}b={E}\\{D\\}_b=\\{E\\}{D}b​={E}\n将{A,B,C,D}\\{A,B,C,D\\}{A,B,C,D}分成{A,B,C}\\{A,B,C\\}{A,B,C}和{D}\\{D\\}{D}, 得Π1={{A,B,C},{D},{E}}\\Pi_1=\\{\\left\\{A,B,C\\right\\},\\{D\\},\\{E\\}\\}Π1​={{A,B,C},{D},{E}}\n考察{A,B,C}b={C,D}\\{A,B,C\\}_b=\\{C,D\\}{A,B,C}b​={C,D}, 需要划分, 其中{A,C}b={C}\\{A,C\\}_b=\\{C\\}{A,C}b​={C}, {B}b={D}\\{B\\}_b=\\{D\\}{B}b​={D}\n将{A,B,C}\\{A,B,C\\}{A,B,C}分为{A,C}\\{A,C\\}{A,C}和{B}\\{B\\}{B}, 得Π2={{A,C},{B},{D},{E}}\\Pi_2=\\{\\left\\{A,C\\right\\},\\{B\\},\\{D\\},\\{E\\}\\}Π2​={{A,C},{B},{D},{E}}\n#正规文法和 FA 的关系\nFA 和左(右)线性正规文法等价\n将 FA 的状态和右线性正规文法的非终结符作为桥梁, 两者等价\n#词法分析器的自动生成\n略\n","categories":["compiler-principle"]},{"title":"编译原理课程笔记 - Chapter 2 语法分析","url":"/2020/12/17/note2-parser/","content":"#提纲\n\n\n#语法分析方法\n\n自上而下分析\n\nLL(1)分析法\n递归下降分析法\n预测分析法\n\n\n自下而上分析\n\n算符优先分析法\nLR 分析法\n\nLR(0)\nSLR\nLR(1)\nLALR\n\n\n\n\n\n#LL(1)\n#左递归消除\n一个文法含有下列形式的产生式时, 称为左递归文法, 不能采用自顶向下分析法\n\n\n直接递归\nA→Aβ,A∈VNA\\rightarrow A\\beta, A\\in V_NA→Aβ,A∈VN​, β\\betaβ∈\\in∈V*\n\n\n间接递归\nA→BβA\\rightarrow B\\betaA→Bβ\nB→Aα,A,B∈VN,α,β∈V∗B\\rightarrow A\\alpha, A,B\\in V_N, \\alpha, \\beta\\in V^*B→Aα,A,B∈VN​,α,β∈V∗\n\n\n左递归消除\nP→Pα1∣Pα2∣...∣Pαm∣β1∣β2∣...∣βnP\\rightarrow P\\alpha_1|P\\alpha_2|...|P\\alpha_m|\\beta_1|\\beta_2|...|\\beta_nP→Pα1​∣Pα2​∣...∣Pαm​∣β1​∣β2​∣...∣βn​\n改写为\nP→β1P′∣β2P′∣...∣βnP′P\\rightarrow\\beta_1 P&#x27;|\\beta_2 P&#x27;|...|\\beta_n P&#x27;P→β1​P′∣β2​P′∣...∣βn​P′\nP′→α1P′∣α2P′∣...∣αmP′∣εP&#x27;\\rightarrow\\alpha_1 P&#x27;|\\alpha_2 P&#x27;|...|\\alpha_m P&#x27;|\\varepsilonP′→α1​P′∣α2​P′∣...∣αm​P′∣ε\n#FIRST, FOLLOW\n终结首符集: FIRST(α)={a∣α⇒∗a...,a∈VT},特别地,如果α⇒∗ε,则规定ε∈FIRST(α)FIRST(\\alpha)=\\{a|\\alpha\\Rightarrow^*a...,a\\in V_T\\}, \\\\特别地, 如果\\alpha\\Rightarrow^*\\varepsilon, 则规定\\varepsilon\\in FIRST(\\alpha)FIRST(α)={a∣α⇒∗a...,a∈VT​},特别地,如果α⇒∗ε,则规定ε∈FIRST(α)\n后继终结符号集: FOLLOW(A)={a∣S⇒∗...Aa...,a∈VT},特别地,如果S⇒∗...A,则规定#∈FOLLOW(S)FOLLOW(A)=\\{a|S\\Rightarrow^*...Aa...,a\\in V_T\\}, \\\\特别地, 如果S\\Rightarrow^*...A, 则规定\\#\\in FOLLOW(S)FOLLOW(A)={a∣S⇒∗...Aa...,a∈VT​},特别地,如果S⇒∗...A,则规定#∈FOLLOW(S)\n#LL(1)文法\n可以进行无回溯的自上而下分析\n\n不含左递归\n产生式右侧的所有非终结符的 FIRST 集无交集\n\nA→α1∣α2∣...∣αn⇒FIRST(αi)∩FIRST(αj)=Φ,(i≠j)A\\rightarrow\\alpha_1|\\alpha_2|...|\\alpha_n\\Rightarrow FIRST(\\alpha_i)\\cap FIRST(\\alpha_j)=\\Phi, (i\\neq j)A→α1​∣α2​∣...∣αn​⇒FIRST(αi​)∩FIRST(αj​)=Φ,(i=j)\n\n\n若ε∈FIRST(A)\\varepsilon\\in FIRST(A)ε∈FIRST(A), 则FIRST(A)∩FOLLOW(A)=ΦFIRST(A)\\cap FOLLOW(A)=\\PhiFIRST(A)∩FOLLOW(A)=Φ\n\n#LL(1)分析方法\n\n当前输入符号为aaa, 要匹配一个AAA, 且A→α1∣α2∣...∣αnA\\rightarrow\\alpha_1|\\alpha_2|...|\\alpha_nA→α1​∣α2​∣...∣αn​\n若a∈FIRST(αia\\in FIRST(\\alpha_ia∈FIRST(αi​)集合, 则直接匹配A→αiA\\rightarrow\\alpha_iA→αi​\n若ε∈FIRST(A)\\varepsilon\\in FIRST(A)ε∈FIRST(A), 但是a∈FOLLOW(A)a\\in FOLLOW(A)a∈FOLLOW(A)中, 仍可以匹配\n否则报错\n\n#LL(1)程序构造\n#递归下降程序\n\n每个递归过程对应一个非终结符\n\n#预测分析程序\n使用分析表和符号栈实现 LL(1)分析\n需要预先构造分析表\n略\n#规范规约\n短语:\n\n对于文法G,开始符号S,若αβδ是一个句型,如果S⇒∗αAδ且A⇒+β,则称β是句型αβδ相对于A的短语对于文法G, 开始符号S, 若\\alpha\\beta\\delta是一个句型, 如果S\\Rightarrow^*\\alpha A\\delta且A\\Rightarrow^+\\beta, 则称\\beta是句型\\alpha\\beta\\delta相对于A的短语对于文法G,开始符号S,若αβδ是一个句型,如果S⇒∗αAδ且A⇒+β,则称β是句型αβδ相对于A的短语\n句型语法树中每棵子树的所有叶子结点左右到右排列起来构成一个该句型相对于子树根(A)的短语句型语法树中每棵子树的所有叶子结点左右到右排列起来构成一个该句型相对于子树根(A)的短语句型语法树中每棵子树的所有叶子结点左右到右排列起来构成一个该句型相对于子树根(A)的短语\n\n直接短语:\n\nA⇒βA\\Rightarrow\\betaA⇒β\n只有父子两代的子树形成的短语,一步推导出终结符的子树只有父子两代的子树形成的短语, 一步推导出终结符的子树只有父子两代的子树形成的短语,一步推导出终结符的子树\n\n句柄:\n\n一个句型的最左直接短语一个句型的最左直接短语一个句型的最左直接短语\n语法树中最左的只有父子两代的子树形成的短语语法树中最左的只有父子两代的子树形成的短语语法树中最左的只有父子两代的子树形成的短语\n\n\n句型E+T*F的\n\n短语为E+T*F(相对于 E), T*F(相对于 T)\n直接短语为T*F\n句柄为T*F\n\n#算符优先分析法\n#算符优先文法\n算符文法: 任一产生式的右部都不包含两个相继(并列)的非终结符\n最多一个优先关系: 对任何一对终结符, (a,b)最多满足一个优先关系\n算符优先文法: 满足最多一个优先关系的算符文法\n#优先关系表\n优先关系的表格\n#FIRSTVT, LASTVT\nFIRSTVT(P)={a∣P⇒+a...或P⇒+Qa...,a∈VT而Q∈VN}FIRSTVT(P)=\\{a|P\\Rightarrow^+a...或P\\Rightarrow^+Qa..., a\\in V_T而Q\\in V_N\\}FIRSTVT(P)={a∣P⇒+a...或P⇒+Qa...,a∈VT​而Q∈VN​}\nLASTVT(P)={a∣P⇒+...a或P⇒+...Qa,a∈VT而Q∈VN}LASTVT(P)=\\{a|P\\Rightarrow^+...a或P\\Rightarrow^+...Qa, a\\in V_T而Q\\in V_N\\}LASTVT(P)={a∣P⇒+...a或P⇒+...Qa,a∈VT​而Q∈VN​}\n#素短语\n素短语: 至少含有一个终结符, 而且除它自身以外不含有任何更小的素短语\n最左素短语: 句型最左边的素短语\n#LR 分析法\nL: 从左到右扫描输入串\nR: 构造最右推导的逆过程\nLR 分析法是严格的规范规约\n原理: 在移进-规约过程中寻找句柄\n模型:\n\n将历史和展望抽象成状态, 整体上是一个 FA\n一张分析表\n\nACTION[s,a]: 状态 s 遇到输入 a 应该采取什么动作\nGOTO[s,X]: 状态 s 遇到文法符号 X 时下一状态是什么, 构成了一个以文法符号为字母表的 DFA\n\n\n\n分类:\n\n总控程序: 所有的 LR 分析器都相同\n分析表: 是自动生成语法分析器的关键\n\nLR(0)表: 基础但有局限性\nSLR 表: 简单 LR 表, 实用\n规范 LR 表: 能力强, 代价大\nLALR 表: 向前 LR 表, 介于 SLR 和规范 LR 之间\n\n\n\nACTION 表:\n\n移进sNsNsN: 将NNN和输入符号aaa进栈, 读取下一个输入\n规约rNrNrN: 用NNN号产生式A⇒βA\\Rightarrow\\betaA⇒β进行规约, 出栈∣β∣|\\beta|∣β∣项, 将GOTO[s.top,A]GOTO[s.top, A]GOTO[s.top,A]和AAA进栈(规约), 输入不动\n接受accaccacc: 分析成功结束\n报错\n\nLR 文法: 能够构造 LR 分析表, 使得每个入口都是唯一确定的文法\nLR(k)文法: 每步至多向前检查 k 个输入符号就能用 LR 分析器进行分析的文法\n\n大多数 PL 符合 LR(1)文法\nk=0 表示不需要展望\n\n#LR(0)\nLR(0)项目: 在文法产生式右部中间间隔处加一个圆点\n\n指明了在分析过程的某一时刻看到了产生式的多大部分\n\n活前缀: 规范句型的最多到句柄(可以包括句柄)的前缀\n\nLR 分析时栈里的符号应该始终构成活前缀\n\n#1. 识别活前缀的 NFA 方法\n\n\n只有项目 1 作为初态, 其他任何项目都认为是终态\n\n\n连接非ε\\varepsilonε弧\n\n状态i为X→X1...Xi−1⋅Xi...Xn状态i为X\\rightarrow X_1...X_{i-1}\\cdot X_i...X_n状态i为X→X1​...Xi−1​⋅Xi​...Xn​\n状态j为X→X1...Xi−1Xi⋅Xi+1...Xn状态j为X\\rightarrow X_1...X_{i-1}X_i\\cdot X_{i+1}...X_n状态j为X→X1​...Xi−1​Xi​⋅Xi+1​...Xn​\n则连接状态iii到状态jjj, 标志为XiX_iXi​\n\n\n\n连接ε\\varepsilonε弧\n\n状态i为X→α⋅Aβ状态i为X\\rightarrow \\alpha\\cdot A\\beta状态i为X→α⋅Aβ\n则连接状态iii到所有状态A→⋅γA\\rightarrow\\cdot\\gammaA→⋅γ, 标志为ε\\varepsilonε\n\n\n\n\n确定化(NFA 转 DFA)\n\n(也可以直接看出来)\n\n\n\n\n#2. LR(0)项目集规范族\n\n\n识别活前缀的 DFA 的项目集的全体称为文法的 LR(0)项目集规范族\n\n规约项目: A→α⋅A\\rightarrow\\alpha\\cdotA→α⋅\n接受项目: S→α⋅S\\rightarrow\\alpha\\cdotS→α⋅\n移进项目: A→α⋅aβA\\rightarrow\\alpha\\cdot a\\betaA→α⋅aβ\n待约项目: A→α⋅BβA\\rightarrow\\alpha\\cdot B\\betaA→α⋅Bβ\n\n\n\n拓广文法\n\n构造一个新的文法G′⊇GG&#x27;\\supseteq GG′⊇G\n引进一个开始符号, 非终结符S′S&#x27;S′\n增加一个产生式S′→SS&#x27;\\rightarrow SS′→S\n唯一接受态: S′→S⋅S&#x27;\\rightarrow S\\cdotS′→S⋅\n\n\n\n项目集的闭包Closure(I)Closure(I)Closure(I):\n\nI∈Closure(I)I\\in Closure(I)I∈Closure(I)\n若(A→α⋅Bβ)∈Closure(I)(A\\rightarrow\\alpha\\cdot B\\beta)\\in Closure(I)(A→α⋅Bβ)∈Closure(I), 则对于任何B→γB\\rightarrow\\gammaB→γ, (B→⋅γ)∈Closure(I)(B\\rightarrow\\cdot\\gamma)\\in Closure(I)(B→⋅γ)∈Closure(I)\n与III同状态的项目集合, 包括子项目\n\n\n\n状态转换函数GO(I,X)GO(I,X)GO(I,X):\n\nGO(I,X)=Closure({A→αX⋅β∣(A→α⋅Xβ)∈I})GO(I, X)=Closure(\\{A\\rightarrow\\alpha X\\cdot\\beta|(A\\rightarrow\\alpha\\cdot X\\beta)\\in I\\})GO(I,X)=Closure({A→αX⋅β∣(A→α⋅Xβ)∈I})\n若III是对活前缀γ\\gammaγ有效的项目集, 那么GO(I,X)GO(I, X)GO(I,X)就是对γX\\gamma XγX有效的项目集\n(接受XXX之后的ClosureClosureClosure集合)\n\n\n\n构造 DFA 算法\nPROCEDURE ITEMSETS(G&#x27;)BEGIN  C:=&#123;Closure(&#123;S&#x27;\\rightarrow\\cdot S&#125;)&#125;  REPEAT    FOR C中每个项目集I和G&#x27;的每个符号X DO      IF GO(I, X)非空且不属于C THEN        C += GO(I, X)  UNTIL C 不再增大END\n\n\nLR(0)文法:\n\n拓广文法的识别活前缀的 dfa 的项目集(LR(0)项目集规范族)不包含任何冲突的文法\n\n冲突\n\n既含移进项目又含规约项目\n\nE-&gt;E·*E\nE-&gt;E+E·\n\n\n含有多个规约项目\n\nP-&gt;A·\nQ-&gt;A·\n\n\n\n构造 LR(0)分析表:\n\n每个项目集为一个状态\n包含S′→⋅SS&#x27;\\rightarrow\\cdot SS′→⋅S的集合为初态\n\n构造 LR(0)的 ACTION 和 GOTO:\n\n若(A→α⋅aβ)∈Ik(A\\rightarrow\\alpha\\cdot a\\beta)\\in I_k(A→α⋅aβ)∈Ik​且GO(Ik,a)=IjGO(I_k, a)=I_jGO(Ik​,a)=Ij​, 则ACTION[k,a]=sjACTION[k, a]=sjACTION[k,a]=sj\n若(A→α⋅)∈Ik(A\\rightarrow\\alpha\\cdot)\\in I_k(A→α⋅)∈Ik​, 则ACTION[k,a]=rjACTION[k, a]=rjACTION[k,a]=rj\n若(S′→S)∈Ik(S&#x27;\\rightarrow S)\\in I_k(S′→S)∈Ik​, 则ACTION[k,a]=accACTION[k, a]=accACTION[k,a]=acc\n若GO(Ik,A)=IjGO(I_k,A)=I_jGO(Ik​,A)=Ij​, 则GOTO[k,a]=jGOTO[k, a]=jGOTO[k,a]=j\n其他均为报错\n\n\n#SLR\nLR(0)可能会误判: 即使存在项目冲突, 也不一定不合法\n假定 LR(0)规范族的一个项目集\nI={A1→α⋅a1β1,A2→α⋅a2β2,...Am→α⋅amβm,B1→α⋅,B2→α⋅,...Bn→α⋅}\\begin{aligned}\nI=\\{\n&amp;A_1\\rightarrow\\alpha\\cdot a_1\\beta_1,\\\\\n&amp;A_2\\rightarrow\\alpha\\cdot a_2\\beta_2,\\\\\n&amp;...\\\\\n&amp;A_m\\rightarrow\\alpha\\cdot a_m\\beta_m,\\\\\n&amp;B_1\\rightarrow\\alpha\\cdot,\\\\\n&amp;B_2\\rightarrow\\alpha\\cdot,\\\\\n&amp;...\\\\\n&amp;B_n\\rightarrow\\alpha\\cdot\\}\n\\end{aligned}\nI={​A1​→α⋅a1​β1​,A2​→α⋅a2​β2​,...Am​→α⋅am​βm​,B1​→α⋅,B2​→α⋅,...Bn​→α⋅}​\n如果集合{a1,...,am},FOLLOW(B1),...,FOLLOW(Bn)\\{a_1, ..., a_m\\}, FOLLOW(B_1), ..., FOLLOW(B_n){a1​,...,am​},FOLLOW(B1​),...,FOLLOW(Bn​)两两不相交(包括不得有两个 FOLLOW 集合有#), 则\n\n若 a 是某个 ai, i=1,2,…,m, 则移进\n若a∈FOLLOW(Bi),i=1,2,...,na\\in FOLLOW(B_i), i=1,2,...,na∈FOLLOW(Bi​),i=1,2,...,n, 则用产生式Bi→αB_i\\rightarrow\\alphaBi​→α进行归约\n此外, 报错。\n\n冲突性动作的这种解决办法叫做 SLR(1)解决办法。上述方法构造出的 ACTION 与 GOTO 表如果不含多重入口，则称该文法为SLR(1)文法\n#LR(1)\n#LALR\n不考, 略\n#二义文法的应用\n#二义文法\n\n不是 LR 文法\n简洁、自然\n可以用文法以外的信息来消除二义\n语法分析的效率高（基于消除二义后得到的分析表）\n\n举例: E → E + E | E * E | (E) | id\n消除二义性:\n\n使用文法以外信息来解决分析动作冲突\n\n","categories":["compiler-principle"]},{"title":"编译原理课程笔记 - Chapter 3 语义分析和中间代码生成","url":"/2020/12/17/note3-ir/","content":"#提纲\n\n\n#属性文法和语法制导翻译\n#属性文法\n属性文法: 在上下文无关文法的基础上给每个文法符号增加若干属性\n语义规则: 对于文法的每个产生式配备了一组属性的计算规则\n\nb:=f(c1,c2,...,ck)b:=f(c_1,c_2,...,c_k)b:=f(c1​,c2​,...,ck​)\n产生式左边的综合属性和右边的继承属性必须提供语义规则\n语义规则所描述的工作可以包括属性计算、符号表操作、静态语义检查、代码生成等等。\n\n属性加工的过程即是语义处理的过程\n属性\n\n综合属性: 在语法树中, 通过子节点的属性计算出来的属性(自下而上)\n继承属性: 在语法树中, 通过父节点和兄弟节点的属性计算出来的属性(自上而下)\n终结符只有综合属性, 有 lexer 提供\n\n#S-属性文法\n只包含综合属性的文法\n#L-属性文法\n如果每个产生式 A→X1…Xj-1Xj…Xn 的每条语义规则计算的属性或者是 A 的综合属性；或者是 Xj 的继承属性，但它仅依赖：\n\n该产生式中 Xj 左边符号 X1, X2, …, Xj-1 的属性；\nA 的继承属性\n\n#语法制导翻译\n语法制导翻译法:\n\n基于属性文法的处理过程: 输入串-&gt;语法树-&gt;依赖图-&gt;语义规则计算次序\n由源程序的语法结构所驱动\n\n作用\n\n产生中间代码\n产生目标指令\n对输入串进行解释执行\n\n#依赖图\n用 DAG 表示属性依赖关系\n\n每个属性一个结点\n语义规则中左边属性依赖右边每一个属性\n\n\n\n\n树遍历法\n\n无环图\nDFS\n从左到右\n\n\n\n一遍扫描法\n\n在语法分析的同时计算属性值\nS-属性文法适合于一遍扫描的自下而上分析\nL-属性文法适合于一遍扫描的自上而下分析\n\n\n\n#语义分析和中间代码生成\n#语义分析\n#中间语言\n中间语言是复杂性界于源语言和目标语言之间的语言\n好处:\n\n便于进行与机器无关的代码优化工作\n易于移植\n使编译程序的结构在逻辑上更为简单明确\n\n常用的中间语言表示法\n\n后缀式(逆波兰式)\n图表示\n\nDAG\nAST\n\n\n三地址代码\n\n三元式\n四元式\n间接三元式\n\n\n\n后缀式:\n\n二元操作符后置\n(一元操作符后置)\n去括号\n\na+b*(c+d/e)a b*(c+d/e) +a b (c+d/e) * +a b c d/e + * +a b c d e / + * +\nb:=-c*a+-c*ab -c*a+-c*a :=b -c*a -c*a + :=b -c a * -c a * + :=b c Neg a * c Neg a * + :=\n抽象语法树: 去掉对翻译不必要的信息, 更有效表示源程序的语法树\n\nDAG\n\n一个内部结点代表一个操作符，它的孩子代表操作数\n一个 DAG 中代表公共子表达式的结点具有多个父结点\n\n\n三地址代码: x:=y op z\n\n三地址代码可以看成是抽象语法树或 DAG 的一种线性表示\n种类\n\nx:=y op z\nx:=op y\nx:=y\ngoto L\nif x rop y goto L\nif a goto L\n\n\n计算机表示\n\n四元式\n\n\n\n\n\n\n\nOp\narg1\narg2\nresult\n\n\n\n\n(0)\numinus\nc\n\nT1\n\n\n(1)\n*\nb\nT1\nT2\n\n\n…\n…\n…\n…\n…\n\n\n\n\n三元式\n\n\n\n\n\nOp\narg1\narg2\n\n\n\n\n(0)\numinus\nc\n\n\n\n(1)\n*\nb\n(0)\n\n\n…\n…\n…\n…\n\n\n(4)\n+\n(1)\n(3)\n\n\n…\n…\n…\n…\n\n\n\n\n间接三元式\n\n使用间接代码表来压缩存储相同的三元式\n好处:\n\n调整语句顺序不需要改动三元式表\n节省存储空间\n\n\n\n\n\n#说明语句的翻译\n#赋值语句的翻译\n\n答案:\n\n三地址代码\nT1:=uminus CT2:=T1+DT3:=B*T2A:=T3\n#布尔表达式的翻译\n基本作用:\n\n逻辑运算\n控制语句条件\n\n翻译方法\n\n算数方式\n短路计算\n\n#逻辑运算中的布尔表达式翻译\n\n答案:\n三地址代码\n0: if a&gt;b goto 31: T1:=02: goto 43: T1:=14: if a&gt;b goto 75: T1:=06: goto 87: T1:=18: T3=T1 and T2\n#条件控制语句中的布尔表达式翻译\n两种出口: 一真一假\n改写产生式\n\nE -&gt; E and M E\nE -&gt; E or M E\nM -&gt; epsilon\n\n增加属性\n\ntruelist/falselist: 布尔表达式中需要回填地址的四元式构成的(反向)列表, 从最后的需要回填地址四元式指向更靠前的四元式\nnextquad: 下一条即将产生的四元式的地址, 如果是 emit 过程中包含那么就是该条语句的地址\n\n增加过程\n\nmakelist(i): 创建一个新链表-&gt;i\nmerge(p1,p2): 合并两个链表\nbackpatch(p,t): 回填地址\n\n\n\n例: 写出布尔式 A or (B and not (C or D))的四元式序列\n答案:\n0: jnz,A,-,01: j,-,-,22: jnz,B,-,43: j,-,-,04: jnz,C,-,05: j,-,-,66: jnz,D,-,07: j,-,-,0\n#控制语句的翻译\n#条件语句\n改写产生式\n\nS -&gt; if E then M S\nS -&gt; if E then M S N else M S\nM -&gt; epsilon\nN -&gt; epsilon\n\n增加属性:\n\nnextlist: 下一条执行语句的地址, 引入此变量是为了优化连续多次跳转\n\n\n#循环语句(while)\n\n#语句列表和语句块\n改写产生式\n\nL -&gt; L; M S\nM -&gt; epsilon\n\n\n\n#标号与 goto 语句的翻译\n#case 语句的翻译\n#过程调用的翻译\n不考, 略\n","categories":["compiler-principle"]},{"title":"编译原理课程笔记 - Chapter 4 目标代码生成","url":"/2020/12/17/note4-codegen/","content":"#目标代码生成\n#任务\n中间代码 --&gt; 代码生成器 --&gt; 目标程序                ^                |              符号表\n#目标代码的三种形式\n\n能立即执行的机器码(所有地址已经定位)\n待装配的机器语言模块(连接装入后即可执行)\n汇编语言代码(经汇编程序汇编后即可执行)\n\n此处选择汇编语言\n#目标机器模型\n此处假设目标计算机:\n\n具有多个通用寄存器, 可以用来运算, 也可以用来取地址\n支持四种指令形式\n\n直接地址型 OP R, M - R OP M =&gt; R\n寄存器型 OP Ri, Rj - Ri OP Rj =&gt; Ri\n变址型 OP Ri, c(Rj) - Ri OP (Rj+c) =&gt; Ri\n间接型\n\nOP Ri, (M) - R OP (M) =&gt; R\nOP Ri, (Rj) - Ri OP (Rj) =&gt; Ri\nOP Ri, (c(Rj)) - Ri OP ((Rj+c)) =&gt; Ri\n\n\n\n\n\nOP 可以是 ADD,SUB,MUL,DIV 等\n#简单代码生成器\n思路: 代码生成器在生成每一条指令时, 必须要知道每个寄存器中存储的是什么, 以及每个变量存储到什么位置\n方法:\n\n引入待用信息\n寄存器描述数组\n变量地址描述数组\n\n#待用信息\n待用信息\n\n在基本块中, 四元式 i 对 A 定值, 四元式 j 对 A 取值, 且 i,j 之间无再对 A 定值, 则称 j 是 i 的变量 A 的待用信息\n\n活跃信息\n\n基本块中的一个名字在某个给定点之后仍被引用, 则称该名字在给定点是活跃的\n\n修改符号表\n\n记录待用信息和活跃信息\n表示: (待用信息[i/^],活跃信息[y/^])\n\n\n\n\n变量名\n待用/活跃信息栏\n\n\n\n\nA\n(…, …)\n\n\nB\n(…, …)\n\n\n…\n(…, …)\n\n\n\n计算待用和活跃信息\n\n开始时, 所有变量均为非待用, 根据基本块之后是否活跃填写活跃或非活跃\n逆序遍历每个四元式i\n\n把A的引用信息附加到四元式i左值上\nA的引用信息附加(^,^)\n把B,C的引用信息附加到四元式i上\nB,C的引用信息附加(^,^)\n\n\n\n例:\nT1:=B-CT2:=A*T1T3:=D+1T4:=E-FT5:=T3*T4W:=T2/T5\n\n\n\n序号\n四元式\n左值\n左操作数\n右操作数\n\n\n\n\n(6)\nW:=T2/T5\n(^,y)\n(,)\n(,)\n\n\n(5)\nT5:=T3*T4\n(6,y)\n(,)\n(,)\n\n\n(4)\nT4:=E-F\n(3,y)\n(,)\n(,)\n\n\n(3)\nT3:=D+1\n(3,y)\n(,)\n(,)\n\n\n(2)\nT2:=A*T1\n(6,y)\n(,)\n(,)\n\n\n(1)\nT1:=B-C\n(2,y)\n(,)\n(,)\n\n\n\n\n\n\n变量名\n待用/活跃信息栏\n\n\n\n\nT5\n(,)-&gt;(6,y)-&gt;(,)\n\n\nT4\n(,)-&gt;(3,y)-&gt;(,)\n\n\nT3\n(,)-&gt;(3,y)-&gt;(,)\n\n\nT2\n(,)-&gt;(6,y)-&gt;(,)\n\n\nT1\n(,)-&gt;(2,y)-&gt;(,)\n\n\nW\n(,y)-&gt;(,^)\n\n\nF\n(,)-&gt;(4,y)\n\n\nE\n(,)-&gt;(4,y)\n\n\nD\n(,)-&gt;(3,y)\n\n\nC\n(,)-&gt;(1,y)\n\n\nB\n(,)-&gt;(1,y)\n\n\nA\n(,)-&gt;(2,y)\n\n\n\n#寄存器分配算法\n寄存器描述和地址描述\n\n寄存器描述数组RVALUE\n\n记录寄存器内存储的变量, 可以是多个\nRVALUE[R1]=&#123;A,B&#125;\n\n\n变量地址描述数组AVALUE\n\n记录变量存储的位置(寄存器/内存)\nAVALUE[A]=&#123;R1,R2,A&#125;\n\n\n\n代码生成算法\n\n对于四元式A:=B op C, 依次执行\n\n\n调用GETREG(i: A:=B op C)获取一个寄存器R\n查询AVALUE[B]和AVALUE[C], 如果在寄存器中, 假设是B'和C'\n如果B'≠R, 先把B加载到R中, 生成代码: LD R,B'\n进行运算, 生成代码: op R,C'\n更新描述信息\n\n如果B'或C'是R, 就要把AVALUE中的记录删除\n设置AVALUE[A]=&#123;R&#125;, RVALUE[R]=&#123;A&#125;\n\n\n如果B或C仍存储在某个寄存器中, 但后续不再被引用且不再活跃, 就从AVALUE和RVALUE中删除B和C的记录\n\n寄存器分配原则\n\n尽可能用 B 独占的寄存器\n尽可能用空闲的寄存器\n抢占用非空闲寄存器\n\nRi的值也存储在内存中\nRi最远才会用到\n\n\n\n寄存器分配算法GETREG(i: A:=B op C)\n\n\n如果 B 独占某个寄存器\n\n如果AB是同一个标识符(即修改 B 的指令B:=B op C)\n\n返回B所在的寄存器\n\n\n如果B在后面不会再引用(非待用, 非活跃)\n\n返回B所在的寄存器\n\n\n\n\n\n如果有空闲寄存器, 返回空闲寄存器\n\n\n抢占一个寄存器Ri,\n\n对于Ri中存储的每一个变量M, 如果存储的不是A, 或者存储的是A == C != B且Ri中不存储B.(M==A &amp;&amp; M==C &amp;&amp; M!=B &amp;&amp; B not in RVALUE[Ri])\n\n如果M仅存储在Ri, 则要存到内存中: ST Ri, M\n如果M == B, 或者M == C &amp;&amp; B in RVALUE[Ri]中, 则令AVALUE[M]=&#123;M,R&#125;, 否则令AVALUE[M]=&#123;M&#125;\n删除M在Ri中的记录, RVALUE[Ri].reLD e(M)\n返回R\n\n\n\n\n\n例:\n寄存器: R0，R1；基本块出口活跃变量: W\nT1:=B-CT2:=A*T1T3:=D+1T4:=E-FT5:=T3*T4W:=T2/T5\n答案:\n【解答】该基本块的目标代码如下(指令后面为相应的注释)：LD  R0, B  /* 取第一个空闲寄存器 R0 */SUB R0, C  /* 运算结束后R0中为T1结果，内存中无该结果 */LD  R1, A  /* 取一个空闲寄存器R1 */MUL R1, R0 /* 运算结束后R1中为T2结果，内存中无该结果 */LD  R0,D  /* 此时R0中结果T1已经没有引用点，且临时单元T1是非活跃的，所以，寄存器R0可作为空闲寄存器使用 */ADD R0, 1 /* 运算结束后R0中为T3结果，内存中无该结果 */ST  R1, T2  /*翻译四元式T4=E-F时，所有寄存器已经分配完毕，寄存器R0中存的T3和寄存器R1中存的T2都是有用的。由于T2的下一个引用点较T3的下一个引用点更远，所以暂时可将寄存器R1中的结果存回到内存的变量T2中，从而将寄存器R1空闲以备使用*/LD  R1, ESUB R1, F  /*运算结束后R1中为T4结果，内存中无该结果*/MUL R0, R1 /*运算结束后R0中为T5结果，内存中无该结果。注意，该指令将寄存器R0中原来的结果T3冲掉了。可以这么做的原因是，T3在该指令后不再有引用点，且是非活跃变量*/LD  R1, T2 /*此时R1中结果T4已经没有引用点，且临时单元T4是非活跃的，因此寄存器R1可作为空闲寄存器使用*/DIV R1, R0/*运算结束后R1中为W结果，内存中无该结果。此时所有指令部分已经翻译完毕*/ST  R1, W/*指令翻译完毕时，寄存器中存有最新的计算结果，必须将它们存回到内存相应的单元中去，否则，在翻译下一个基本块时，所有的寄存器被当成空闲的寄存器使用，从而造成计算结果的丢失。考虑到寄存器R0中的T5和寄存器R1中的W，临时单元T5是非活跃的，因此只要将结果W存回对应单元即可*/\n","categories":["compiler-principle"]},{"title":"编译原理课程笔记 - Chapter 5 优化","url":"/2020/12/17/note5-profile/","content":"#提纲\n\n#概述\n优化：对程序进行各种等价变换，使得从变换后的程序出发，能生成更有效的目标代码。\n原则\n\n等价\n有效\n合算\n\n级别\n\n局部优化\n循环优化\n全局优化\n\n种类\n\n删除公用子表达式\n复写传播\n删除无用赋值\n代码外提\n强度削弱\n变换循环控制条件\n合并已知量\n\n#局部优化\n#基本块\n基本块\n\n指程序中一顺序执行语句序列，其中只有一个入口和一个出口。入口就是其中第一个语句，出口就是其中最后一个语句。\n\n活跃\n\n基本块中的一个名字在某个给定点之后仍被引用, 则称该名字在给定点是活跃的\n\n局部优化\n\n局限在基本块范围内的优化\n\n划分基本块\n\n基本块入口\n\n程序第一条语句\n能转移到的语句\n转移语句后面的语句\n\n\n基本块的出口(包括)\n\n基本块入口的前一条语句\n转移语句(包括)\n停语句(包括)\n\n\n不在基本块中的语句可以从程序中删除\n\n\n基本块中的优化\n\n删除公用子表达式\n删除无用赋值\n合并已知量: T1=2; T2=4*T1; =&gt; T2=8;\n临时变量改名\n交换语句的位置\n代数变换: 删除x=x+0; x=x*1;, x=y^2; =&gt; x=y*y;\n\n#流图\n\n把基本块编号后按执行顺序连接成一张图\n入口语句是流图的首结点\n\n前驱和后继\n\n在程序序列中, 若 A,B 相邻而且 A 最后一条语句不是无条件跳转, 则称 A 是 B 的前驱, B 为 A 的后继\n\n\n#基本块的 DAG 表示\n\n用带有标记或附加信息的 DAG 来表示变量间的关系\n叶结点: 以标识符或常数作为标记\n内部结点: 以运算符作为标记\n每个结点可以有附加标识符: 表示附加标识符具有相同的值\n\n#基本块的 DAG 优化算法\n基本块代码分类\n\n0 型: 单纯赋值语句A:=B\n1 型: 一元运算赋值A:=op B\n2 型: 二元运算赋值A:=B op C/数组取值赋值A:=B[C]\n\n对基本块中每一四元式，依次执行以下步骤:\n\n如果是 0 型, 记NODE(B)的值为n, 转 4\n如果存在任意一个操作数无定义, 则构造该操作数结点\n如果所有操作数都是常数\n\n如果NODE(B)(或NODE(C))是新构造的结点, 删除\n计算op, 记为P\n如果NODE(P)无定义, 构造之, 记为n\n\n\n否则\n\n检查 DAG 中是否已经存在结点op, 如果没有则构造之, 记为n\n\n\n删除无用赋值\n\n如果A已经在某个结点处定义, 删除之, 把A附在n结点上, 令NODE(A)=n\n\n\n\nDAG 优化算法\n\n执行上述算法\n在基本块外被定值的标识符作为叶子结点上的标识符\n在基本块内被定值且在基本块后被引用的标识符作为结点上的附加标识符\n\n#循环优化\n不考, 略\n","categories":["compiler-principle"]},{"title":"现代编译原理 阅读记录","url":"/2024/04/30/tiger-book/","content":"","categories":["compiler-principle"],"tags":["编译原理"]},{"title":"ARP协议","url":"/2021/10/20/arp-protocol/","content":"#ARP 协议\nARP 协议的全称是 Address Resolution Protocol(地址解析协议)，它是一个通过解析网络层地址来找寻数据链路层地址的网络传输协议，它在 IPv4 中极其重要。是 IPv4 能够正常工作的基础。\nARP 协议与 IP 协议一样，位于网络层，ARP 报文在以太网帧结构中的帧类型字段为0x0806\n在 IPv6 中邻居发现协议（NDP）用于代替地址解析协议（ARP）。\n#ARP 报文格式\n以太网首部：\nstruct ether_hdr &#123;    uint8_t target_mac[6];    uint8_t sender_mac[6];    uint16_t ptype; /* equal to 0x0806 */&#125;\nARP 报文：\nstruct arp_hdr &#123;    uint16_t htype;    uint16_t ptype;    uint8_t hlen;    uint8_t plen;    uint16_t opcode;    uint8_t sender_mac[6];    uint8_t sender_ip[4];    uint8_t target_mac[6];    uint8_t target_ip[4];&#125;;\n#ARP 表\n在每台安装有 TCP/IP 协议的电脑或路由器里都有一个 ARP 缓存表，表里的 IP 地址与 MAC 地址是一对应的，表中的每一行表示在当前局域网中，拥有IP地址的设备的 MAC 地址是MAC地址，如下表所示。\n\n\n\n主机名称\nIP 地址\nMAC 地址\n\n\n\n\nA\n192.168.38.10\n00-AA-00-62-D2-02\n\n\nB\n192.168.38.11\n00-BB-00-62-C2-02\n\n\nC\n192.168.38.12\n00-CC-00-62-C2-02\n\n\nD\n192.168.38.13\n00-DD-00-62-C2-02\n\n\nE\n192.168.38.14\n00-EE-00-62-C2-02\n\n\n…\n…\n…\n\n\n\nARP 缓存表采用老化机制，在一段时间内如果表中的某一行没有使用，就会被删除，这样可减少缓存表的长度，加快查询速度。\n#ARP 流程\n\n\n当发送数据时，主机 A 会在自己的 ARP 缓存表中寻找是否有目标 IP 地址。如果找到就知道目标 MAC 地址为（00-BB-00-62-C2-02），直接把目标 MAC 地址写入帧里面发送就可。\n\n\n如果在 ARP 缓存表中没有找到相对应的 IP 地址，主机 A 就会在网络上发送一个广播（ARP request），目标 MAC 地址是“FF.FF.FF.FF.FF.FF”，这表示向同一网段内的所有主机发出这样的询问：“192.168.38.11 的 MAC 地址是什么？”\n\n\n网络上其他主机并不响应 ARP 询问，只有主机 B 接收到这个帧时，才向主机 A 做出这样的回应（ARP response）：“192.168.38.11 的 MAC 地址是 00-BB-00-62-C2-02”，此回应以单播方式。这样，主机 A 就知道主机 B 的 MAC 地址，它就可以向主机 B 发送信息。同时它还更新自己的 ARP 高速缓存（ARP cache），下次再向主机 B 发送信息时，直接从 ARP 缓存表里查找就可。\n\n\n#免费 ARP\n免费 ARP（gratuitous ARP）是指主机发送 ARP 查询（广播）自己的 IP 地址，当 ARP 功能被开启或者是端口初始配置完成，主机向网络发送免费 ARP 来查询自己的 IP 地址确认地址唯一可用。\n作用：\n\n\n确定网络中是否有其他主机使用了 IP 地址，如果有应答则产生错误消息。\n\n\n免费 ARP 可以做更新 ARP 缓存用，网络中的其他主机收到该广播则在缓存中更新条目，收到该广播的主机无论是否存在与 IP 地址相关的条目都会强制更新，如果存在旧条目则会将 MAC 更新为广播包中的 MAC。\n\n\n","categories":["computer-networking"],"tags":["计算机网络","基础架构"]},{"title":"计算机网络课程笔记","url":"/2021/01/05/computer-networking-notes/","content":"#按章节\n#1.9 OSI 七层模型\n设计者: 国际标准化组织 ISO, 国际电信联盟 ITU\nOSI 七层模型: 物理层/数据链路层/网络层/传输层/会话层/表示层/应用层\nTCP/IP 位于: 传输层/网络层\n会话层和表示层几乎没有内容\n#3.4 CS 模型\n\n\n\n服务端程序 S\n客户端程序 A\n\n\n\n\n先启动\n后启动\n\n\n不知道 C\n必须知道 S 的位置\n\n\n积极等待 C 来连接\n需要通讯时初始化连接\n\n\n通过收发数据进行通讯\n通过收发数据进行通讯\n\n\n服务之后继续运行, 等待下一个 C 来连接\n可能结束\n\n\n\nInternet 只提供基本通讯, 实际上是由计算机上的程序来处理连接\n#3.5 C/S 程序的特点\n略\n#3.13 网络编程 Socket API\n事实标准: Socket API\n#4.6 URL 和超链接\n#4.7 HTTP\n#4.17 DNS 和域名\n#4.20 名字解析\n#5.5 与数据传输相关的话题\n\n信息源\n数据源编码/解码器\n加密/解密器\n通道编码/解码器\n复用/解复用器\n调制/解调器\n物理信道和传输\n\n#7.18 低地卫星(LEO)和集群\nLEO 卫星群协同工作以转发消息。集群成员必须知道当前在地球给定区域上的卫星，并将消息转发给适当的成员以传输到地面站。\n#8.6 奇偶校验码\n单奇偶校验（SPC）是信道编码的一种基本形式，其中发送方在每个字节上添加一个额外的位，以使 1 的个数为偶数（或奇数），接收方验证输入的数据是否具有正确的 1 位数。\n#8.12 因特网使用的 16 位校验和\n源数据 M, 任意长度\n\n补零, 直至长度是 16 位的倍数\n将 M 按照 16 位分组并求和, 最多允许溢出到 32 位\n将结果对半相加(高 16 位加低 16 位), 只保留 16 位\n将结果取反\n如果是 0 则再取反\n\n取反的目的: 方便验证, 只要求完相加判断是否溢出剩 0 就行了\n#8.13 CRC\n例: data=10010101010, G=10101\n多项式表示: G=x5+x3+1G=x^5+x^3+1G=x5+x3+1\n\n补零\n\nmdata = data + '0' * (G.length - 1)\n得到mdata = 100101010100000\n\n模 2 除法\n\n      ___101100101100_10101/100101010100000    / 10101 |   | | |     =  11110   | | |        10101   | | |       = 10111  | | |         10101  | | |        =   10010 | |            10101 | |           =  11100 |              10101 |             = 10010|               10101|              =  1110\n得到 CRC=1110\n\n拼接\n\n发送的数据send = data + CRC\nsend = 1000101010 1110\n#9.12 单工/半双工/全双工传输\n信道的三种类型\n单工: 只能单向传输, 单根光纤就是单工, 类比收音机, 电视\n全双工: 可以同时双向传输, 有两个光纤就可以组成全双工, 类比电话\n半双工: 需要一个共享传输介质, 可以双向传输但是不能同时, 类比对讲机\n#10.3 模拟调制\n载波: 信息发送出去时以电波的形式, 负责承载信息的波就是载波\n调制: 根据要发送的信息, 对载波进行的调整\n原始载波(输入1) --\\\n                (调制器) ---&gt; 调制过的载波(输出)\n信息 (输入2)    --/\n\n三种主要调制技术:\n\n振幅调制\n频率调制\n移相调制\n\n#10.12 Modem\n为了方便网络安装, 一般将调制和解调功能集成在一个叫做调制解调器的设备中\n#12.6 局部环路特性和适应\n接入技术: ISP 到用户的连接\n本地用户环路(local subscriber line)/本地环路(local loop): 电话公司交换局到用户之间的物理连接, 一般是使用双绞线 (其实就是电话线路?)\nDSL: 一种利用 local loop 提供网络的技术\nADSL: 不对称 DSL, 利用频分复用将 local loop 的带宽分成三个区域\n\n\n\n频率\n功能\n\n\n\n\n0-4\n普通老式电话业务\n\n\n26-138\n上行频带\n\n\n138-1100\n下行频带\n\n\n\n因为本地环路的电气特性变化各异，ADSL 采用了自适应技术，即一对调制解调器先探测彼此之间连接线路上的许多频率，然后选择在此线路上能产生最优传输质量的频率和相应的调制技术。\n#12.7 ADSL 的传输速率\n上行: 32 - 640 Kbps, 去掉控制信道: 32 - 576 Kbps\n下行: 32 - 8448 Kbps\n#12.9 电缆调制解调器技术\n本地环路具有局限性: 双绞线不抗干扰,\n因此创造了基于 同轴电缆 + FDM + 统计复用 的电缆调制解调器技术, 每一组用户共享一个数据信道, 通过 Modem 判断数据是否属于该用户\n传输速率: 上行 512 Kbps, 下行 52 Mbps\n#13.4 本地和广域包交换网络\n\n\n\n类型\n距离\n\n\n\n\nLAN\n房间/建筑\n\n\nMAN\n大城市\n\n\nWAN\n多个城市\n\n\n\n#13.9 包识别, 解复用, MAC 地址\n地址: 在包交换系统中, 解复用时使用的标识符, 每个包都包含了目的地地址\nIEEE 地址(MAC 地址): 48 位地址, 每个 NIC 都不一样, 分为前 24 位 OUI, 24 位 NIC 地址\n#13.10 单播, 广播, 多播\nIEEE 保留了 MAC 地址中的一位(最高有效字节的最低位)来区分单播(0)和多播(1)\n广播视为一种特殊的多播, 广播地址所有位全 1\n\n\n\n类型\n解释\n\n\n\n\n单播\n唯一标识一台计算机，并规定只有被标识的那台计算机才能接收分组的副本\n\n\n广播\n对应所有的计算机，并规定网络上每台计算机都应该接收分组的副本\n\n\n组播\n标识指定网络上所有计算机的一个子集，并规定该子集中的每台计算机都应该接收分组的副本\n\n\n\n#13.11 广播, 多播和多点传递\n在 LAN 中, 广播和多播极大地提高了效率\n#14.6 随机接入协议\n接入技术: 多台相互独立的计算机如何进行协调接入一个共享介质\nMAC 层: Media Access Control 介质接入控制\n方法: 采用复用技术的改进形式, 采用分布式算法\n多址接入协议\n\n受控接入协议\n\n预约\n轮询\n令牌传递\n\n\n随机接入协议\n\nALOHA\nCSMA/CD\nCSMA/CA\n\n\n信道分配协议\n\nFDMA\nTDMA\nCDMA\n\n\n\n在 LAN 中, 用户接入共享介质的时机是随机的, 因此可能造成冲突, 需要用合适的方法解决\n\n\n\n类型\n描述\n\n\n\n\nALOHA\n教科书中流行的协议, 但没有实际应用, 划分上行/下行两个频段用来传输信息, 上行采用随机延迟来解决冲突问题, 下行采用广播机制\n\n\nCSMA/CD\n带冲突检测的载波侦听多址接入, 原始以太网使用, 但交换式以太网不再使用, 使用载波侦听+冲突检测+指数退避算法解决冲突问题\n\n\nCSMA/CA\n带冲突避免的载波侦听多址接入, 无线 LAN 使用, 与 CSMA/CD 类似, 不同在于分组传输前会发送一个控制报文, 接收方广播控制消息\n\n\n\nCSMA/CD:\n\nCarrier Sense 载波侦听\nMultiple Access 多址接入\nCollision Detect 冲突检测\n\nCSMA/CA:\n\nCarrier Sense 载波侦听\nMultiple Access 多址接入\nCollision Avoidance 冲突避免\n\n#15.3 以太网帧格式\n(传统)以太网: 是一种 LAN 技术, 最早在施乐公司的 PARC 中发明, 后来被标准化\n帧: 数据链路层的协议数据单元\n(传统)以太网帧格式:\n\n\n\n以太网帧(64-1518) := 头部(14) + 载荷(46-1500) + CRC(4)\n\n\n头部 := 目的地址(MAC 地址, 6) + 源地址(MAC 地址, 6) + 类型(2)\n\n\n#15.4 以太网帧的类型域\n以太网帧类型域包含了载荷中的报文类型, 如 IPv4/IPv6/ARP 等, 接收方检查该值来进行复用与解复用\n#15.5 802.3 以太网\n802.3 以太网: IEEE 802.3 重新修订后的以太网标准, (但不是很成功)\n\n\n\n802.3 以太网帧(64-1518) := 头部(14) + SNAP 头部(8) + 新载荷(46(存疑)-1492) + CRC(4)\n\n\n头部 := 目的地址(MAC 地址, 6) + 源地址(MAC 地址, 6) + 帧长度(2)\n\n\nSNAP 头部 := LLC(3) + OUI(3) + 类型(2)\n\n\n兼容性处理: 类型域&lt;=1500 是 802.3 帧, 否则是传统以太网帧\n\n\n#15.7 粗缆布线的以太网\n粗缆以太网(Thick wire Ethernet)/粗网(Thicknet, 10Base5): 最初版本的以太网布线方案, 因为其使用笨重的同轴电缆+收发器+AUI 电缆方式而得名.\n\n#15.8 细缆布线的以太网\n细缆以太网(Thin wire Ethernet)/细网(Thinnet, 10Base2): 第二代以太网布线方案, 使用细同轴电缆依次串联计算机, 不再需要 AUI 线, 收发器集成到 NIC 中, 缺点是容易单点故障\n\n#15.9 双绞线布线的以太网\n双绞线以太网(10BaseT): 第三代以太网布线系统, 使用了双绞线取代同轴电缆, 引入中心集线器(hub)\n\n集线器是物理层设备, 具有一定的功能, 比如 CSMA/CD 等, 现在大部分已经被交换机取代\n#16.2 无线网络的分类\n和有线网络类似: 个域网/局域网/城域网/广域网\n#16.3 个域网\n蓝牙/红外/ISM 无线\n#16.5 WLAN 和 Wi-Fi\nWLAN 技术的种类很多, 归类于 IEEE802.11 标准\nWi-Fi 联盟: 一群无线设备供应商组成的非营利性组织, 使用 802.11 标准对无线设备进行测试与认证\n#16.8 WLAN 基础结构\nWLAN 的三个构件: 接入点(Access Point, AP, 非正式也称基站), 互联机构(交换机或路由器等), 无线主机(Station)\n\n\nWLAN 的两种类型: Adhoc 无接入点结构(很少), Infrastructure 有中心接入点\n互联设备到 AP 的有线连接通常使用双绞线以太网\n基本服务组(BSS): 在某一个给定 AP 范围内的计算机集合\n#16.9 重叠, 关联, 802.11 帧格式\n重叠: 两个 AP 太近, 一台 STA 可以同时触及到两个 AP\n802.11 要求每个主机只能与一个 AP 相关联(associate), 而且在 802.11 帧中携带了 AP 的 MAC 地址\n\n接入点 MAC\n本机地址\n路由器 MAC\n专用地址\n\n\n#16.11 竞争与无竞争接入\n802.11 定义了两种接入方法\n\n点协调功能 PCF\n分布式协调功能 DCF\n\n隐蔽站问题: WLAN 中所用的发射机有一个受限的发射范围, 离发射机的距离超过发射范围的接收方将无法收到信号, 因而无法检测载波\n802.11 使用 CSMA/CA 协议来解决接入中的隐蔽站问题\n\n传输分组前交换一对 RTS,CTS 控制报文\n没有冲突检测, 而是采用确认 ACK+超时重传机制\n设定 SIFS(发 ACK 前延时)/DIFS(传输前延时)/Slot Time 等时间间隔参数\n\n\n#16.13 PAN 技术与标准\n\n802.15.1a 蓝牙\n802.15.3a UWB 超宽带\n802.15.4 Zigbee 工业无线控制标准\n\n#17.3 光纤猫扩展\n最简单的 LAN 扩展机制: 光纤+一对光纤猫, 连接一台计算机和一个远程以太网\n#17.4 中继器\n中继器: 信号放大, 信号转发, 早期广泛应用\n#17.5 网桥与桥接\n网桥: 往往用来连接两个 LAN 网段(比如 hub), 保证两边的计算机都能互相连通\n计算机无法分辨 LAN 中有无网桥\n#17.6 自学习网桥与帧过滤\n网桥与一根网线直连的区别: 网桥分隔冲突域, 但不分隔广播域 网桥仅在有必要的时候才进行转发\n\n源地址和目标地址位于两个网段\n广播帧和多播帧\n\n网桥能够自动学习设备位于网桥的哪一侧(基于分组中的源 MAC 地址)\n#17.7 桥接的好处\n桥接的各个网段可以同时进行传输, 使得不同建筑物之间的通讯成为了可能\n#17.8 分布式生成树\n如果网桥连接导致网络中存在环路, 那么广播和多播帧会被无限循环转发下去\n因此网桥会执行生成树算法来生成一个树\n#18.7 下一条转发\n分组交换机收到一个分组后, 如果分组的目的地不是直连在交换机上, 就需要查表转发\n交换机的转发表: 目的地址:输出接口\n#18.9 广域网动态路由更新\n转发表的要求\n\n全局通信 - 必须覆盖所有可能目的地的\n最优路径 - 下一条必须是距目的地的最短路径\n\n路由软件: 自动重新配置转发表的软件\n#18.12 分布式路径计算\n分布式路径计算的两种常用形式:\n\n链路状态路由 LSR, 采用 Dijkstra 算法\n距离向量路由 DVR\n\nLSR: 每台交换机定期广播一条边的状态, 相邻交换机据此更新自己的转发表\nDVR: 每台交换机定期告诉相邻交换机自己的距离向量表(包含一对(目的,距离)), 相邻交换机据此更新自己的转发表\n#18.13 最短路径的计算\nDijkstra 算法\n#20.6 用路由器连接物理网络\n路由器是一台专门完成网络互联任务的专用硬件系统, 可以将多个使用不同技术（包括不同的传输介质、物理编址方案或帧格式）的网络互相连接（互联）起来 (比如连接以太网和 WiFi 网)\n#21.7 IP 有类编址方案\nIP 地址有类编址方案\n\n\n\n类别\n标识位\n前缀长度\n最大网络数量\n后缀长度\n最大主机数量\n\n\n\n\nA\n0\n7\n128\n24\n2^24\n\n\nB\n10\n14\n16384\n16\n65536\n\n\nC\n110\n21\n2^21\n8\n256\n\n\nD(组播)\n1110\n-\n-\n-\n-\n\n\nE(保留)\n1111\n-\n-\n-\n-\n\n\n\n#21.10 IP 无类编址方案\nIP 地址无类/子网编址方案: 引入附加的地址掩码(子网掩码)来指定网络地址和主机地址的分界线\n无类地址 &amp; 子网掩码 == 网络地址\n#21.21 IPv6 地址\n和 IPv4 类似, IPv6 为每个接入网络的计算机分配了一个独一无二的地址\nIPv6 的特点\n\n\n将网络地址进一步细分为全球单播地址和子网地址两部分, 主机地址仍保留, 两者共 128 位, 用/xx来表示网络地址的长度, 如不指定默认是 64 位\n\n\n\n取消广播, 支持多播, 增加任播(anycast)\n\n\n\n#22.4 IP 数据报\nIP 数据报(-65535) := 头部 + 数据区(载荷区)\n#22.5 IP 数据报头部格式\n\n#22.7 网络前缀提取与数据报转发\n根据网络地址查询转发表发给下一站\n#22.8 最长前缀匹配\n查询转发表时如果有多项匹配, 采用最长前缀匹配规则\n#22.11 IP 封装\nIP 数据报要被封装到一个帧里才能发送出去, 帧的目的地址是下一站的 IP 地址对应的 MAC 地址(通过 ARP 协议获得)\n#22.14 IP 分片和重装机制\n#22.15 分片数据报的收集\n最大传输单元 MTU: 每种物理网络规定的一帧能够携带的最大数据量\nIP 数据报分片机制: 当一个数据报长度大于前方网络的 MTU 时, 路由器会将数据报分成若干较小的片(fragment), 然后将每一片独立地封装发送出去\n分片: 每个片与数据报的格式一样, 头部部分参数不同\n\n标识: 每个数据报的唯一标识, 同一个原始数据报的所有片段的标识是一样的\n标志: 标识了该数据报是片还是完整的数据报, 以及是否是最后一片\n片偏移: 该片在原数据报中的位置\n\n重装: 在所有片的基础上重新产生原数据报的过程, IP 规定只有最终目的主机才能对片进行重装\n#23.9 ICMP 报文格式与封装\n因特网控制报文协议(ICMP): 利用 IP 来传输 IP 传输中发生的错误\nICMP 报文放置在 IP 数据报的数据区传输\n#23.10 协议软件, 参数和配置\n协议软件采用参数化设计\n主机配置采用自举过程, 运行时获取到 IP 地址, 掩码, DNS 服务器地址等参数填入程序中运行\n#24.3 UDP\nUDP 的特征:\n\n端到端: UDP 是一个传输协议, 它能区分运行在给定计算机上的多个应用程序\n无连接: UDP 提供给应用的接口遵从无连接模式\n面向报文: 使用 UDP 的应用进程所发送和接收的数据是单个报文\n尽力而为: UDP 提供给应用的是与 IP 一样的尽力传递机制\n任意交互: UDP 允许应用进程给很多其他应用进程发送数据, 也允许从很多其他应用进程那里接收数据, 或者只跟一个其他应用进程相互通信\n操作系统无关: UDP 所提供的标识应用程序的方法, 不取决于本地操作系统所使用的标识符\n\n\n#24.4 无连接的通信模式\nUDP 采用无连接通信模式, 可以在任何时候发送数据, 不需要维护通信状态, 也不使用控制报文, 因此传输开销极低\n#24.7 交互模式和广播传递\nUDP 支持四种交互通信方式:\n\n一对一\n一对多\n多对一\n多对多\n\n底层是利用 IP 组播和广播机制实现的\n#24.8 用协议端口号标识端点\n协议端口号: 用来标识应用程序的标识符抽象集, 独立于底层操作系统\n#25.3 TCP\nTCP 提供的服务有 7 个主要特点:\n\n面向连接: TCP 提供面向连接的服务, 应用程序必须首先请求建立一个到目的地的连接, 然后使用这个连接来传输数据\n点对点通信: 每个 TCP 连接上只有两个端点\n完全的可靠性: TCP 能保证在一个连接上发送的数据被正确地传递, 且保证数据的完整和按序到达\n全双工通信: TCP 连接允许数据在任何一个方向上流动, 并允许任何应用程序在任何时刻发送数据\n流接口: TCP 提供一个流接口, 利用它应用进程可以在一个连接上发送连续的字节流. TCP 不必将数据组合成记录或是报文, 也不要求传递给接收应用进程的数据段大小一定要与发送端所送出的数据段大小相同\n可靠的连接建立: TCP 允许两个应用进程可靠地开始通信\n友好的连接关闭: 在关闭一个连接之前, TCP 必须保证所有数据已经传递完毕, 并且通信双方都要同意关闭这个连接\n\n\n#25.4 端对端服务与虚拟连接\nTCP 和 UDP 都是端到端协议: 提供在一台计算机上的应用进程与另一台计算机上的应用进程之间的通信能力\nTCP 是面向连接的协议, TCP 提供的连接是一种虚拟连接, 因为它是软件实现的连接\nTCP 将底层因特网视为一个通信系统\n#25.5 一般可靠传输协议所采用的技术\n\n\n分组乱序\n\n排序技术: 每个分组都附加一个序号\n\n\n\n分组丢失\n\n带重传的正向确认: 接收方发送 ACK 报文表示成功接收; 若超时仍未收到 ACK, 则重传\n\n\n\n分组重复(重放错误)\n\n每一次会话用一个唯一的 ID 标识\n\n\n\n数据过载\n\n停-走系统: 不好, 效率很低\n滑动窗口: 定义窗口大小为接收方一次可以承载的最大数据量/发送方收到 ACK 前可以发送的最大数据量, 发送方收到 ACK 之后窗口向后移动\n\n\n\n#25.11 缓冲, 流控制, 窗口\nTCP 使用了以字节计量的窗口, 接收方将自己剩余的缓冲区大小放在 ACK 报文中, 发送方根据这个窗口大小来决定是继续发送还是停止\n#26.4 动态路由和路由器\n每个路由器都运行路由软件，该软件了解其他路由器可以到达的目的地，并向其他路由器通知其可以到达的目的地。路由软件使用传入的信息来连续更新本地转发表。\n#26.7 两类因特网路由协议\n因特网路由协议的分类:\n\n内部网关协议 IGP: 自治系统内部使用的协议, 可以自由选择, 例如 RIP, OSPF, is-is 等\n外部网关协议 EGP: 自治系统之间使用的协议, 例如 BGP\n\n路由度量指标: 评价路由路径好坏的指标\n\n\n尽管大多数 Internet 路由协议都设计为使用跃点作为路由指标，网络管理员也可以覆盖该指标以实施策略。\n\n\n在自治系统中，IGP 软件使用路由度量来选择到达每个目的地的最佳路径。 EGP 软件可以找到到达每个目的地的路径，但是找不到最佳路径，因为它无法比较来自多个自治系统的路由度量。\n\n\n#26.12 OSPF 协议\nRIP 是内部网关协议, 使用距离矢量算法来传递路由信息。RIP 是在 UDP 协议之上的一种路由协议，应用于 OSI 网络七层模型的网络层。\nOSPF 是内部网关协议，它使用链路状态算法来传播路由信息。路由器使用Dijkstra 算法计算最短路径。\n#26.16 组播路由技术\nIP 组播群组的成员关系构成是动态的，即计算机可以随时加入或离开一个群组。群组成员关系只是定义了一组接收者；任意一个应用进程（即使它不是一个群组成员）都能发送数据报给组播群组。\n因特网组播的动态特性使它的组播路径传播问题变得很困难。虽然已经提出了很多协议，但是目前因特网还没有全网范围内的组播路由设施。\n#29.9 私有密钥(对称)加密\n#29.10 公开密钥(非对称)加密\n#29.11 使用数字签名的鉴别\n签名: 发送方使用自己的私钥对报文进行加密\n验证: 接收方使用发送方的公钥进行解密\n认证+加密: 发送者先对报文进行签名, 再用接收者的公钥进行加密\n#29.13 防火墙\n防火墙: 防止因特网上的问题扩散到本地内部的计算机上的技术, 一般放置在本单位的网络与外部因特网之间\n#29.14 包过滤防火墙的实现\n防火墙利用包过滤技术来防止不希望有的通信交互。每种过滤器规则都要给出数据包头部参数的组合，包括 IP 源地址、目的地址、协议端口号和传输协议类型等。\n#30.7 SNMP\n简单网络管理协议 SNMP: 基于 IP 的网络管理的标准协议\n#按作业内容\n#TCP/IP 的分层模型\n物理层/网络接口层/网络层/传输层/应用层\n#导线类型\n\n无屏蔽双绞线(UTP)\n屏蔽双绞线(STP)\n同轴电缆(CC)\n\n#通信卫星类型\n\n\n\n类型\n优点\n缺点\n\n\n\n\n低地球轨道(LEO)\n低时延\n相对地球移动\n\n\n中地球轨道(MEO)\n主要提供南北级通信\n-\n\n\n地球静止轨道(GEO)\n相对地球静止\n距离远\n\n\n\n#为什么要区分上下行通信\n大多数因特网用户都是按非对称（asymmetric）模式使用网络的，即典型的居民用户从因特网接收的数据要多于他们发送出去的数据。例如，为了浏览一个网页，浏览器发送一个只包含几个字节的 URL，而 Web 服务器响应回来的内容可能是包含几千字节的文本或者是一个包含好几万字节的图片。运行 Web 服务器的商业用户可能具有相反的流量模式——商业用户发送的数据要多于接收的数据。\n#宽带与窄带\n窄带接入技术:\n\n拨号电话连接\n使用调制解调器的租用电路\n部分 T1 数据电路\nISDN 及其他电信数据服务\n\n宽带接入技术:\n\nDSL 技术\n电缆调制解调器技术\n无线接入技术\nT1 速率或更高速率的数据线路\n\n#\n#背背背\n#TCP 的特征\n\n面向连接: TCP 提供面向连接的服务, 应用程序必须首先请求建立一个到目的地的连接, 然后使用这个连接来传输数据\n点对点通信: 每个 TCP 连接上只有两个端点\n完全的可靠性: TCP 能保证在一个连接上发送的数据被正确地传递, 且保证数据的完整和按序到达\n全双工通信: TCP 连接允许数据在任何一个方向上流动, 并允许任何应用程序在任何时刻发送数据\n流接口: TCP 提供一个流接口, 利用它应用进程可以在一个连接上发送连续的字节流. TCP 不必将数据组合成记录或是报文, 也不要求传递给接收应用进程的数据段大小一定要与发送端所送出的数据段大小相同\n可靠的连接建立: TCP 允许两个应用进程可靠地开始通信\n友好的连接关闭: 在关闭一个连接之前, TCP 必须保证所有数据已经传递完毕, 并且通信双方都要同意关闭这个连接\n\n#UDP 的特征\n\n端到端: UDP 是一个传输协议，它能区分运行在给定计算机上的多个应用程序。\n无连接: UDP 提供给应用的接口遵从无连接模式。\n面向报文: 使用 UDP 的应用进程所发送和接收的数据是单个报文。\n尽力而为: UDP 提供给应用的是与 IP 一样的尽力传递机制。\n任意交互: UDP 允许应用进程给很多其他应用进程发送数据，也允许从很多其他应用进程那里接收数据，或者只跟一个其他应用进程相互通信。\n\n#服务端-客户端特征\n虽然存在少量的变种，但大多数客户——服务器交互模式都具有相同的一般特征。一般情况下，客户软件具有如下特征：\n\n它是一个任意的应用程序，仅在需要进行远程访问时才暂时成为客户，同时还要完成其他的计算任务。\n直接受用户介入操作，并且只执行一个会话过程。\n在用户的个人计算机上进行本地运行。\n主动地发起与服务器的连接请求。\n能访问所需的多种服务，但通常一次只与一个远地服务器请求连接。\n不会特别地要求功能强大的计算机硬件。\n\n而服务器软件的特征如下：\n\n它是一个专门提供某种服务的专用特权程序，但同时可以处理多个远程客户的请求。\n在系统启动时自动被调入执行，进行多次会话并持续不断地运行。\n运行在大型、高性能计算机上。\n被动地等待来自任意的远端客户的通信请求。\n接收来自任何客户的通信请求，但只提供单一的服务。\n要求功能强大的硬件和高级复杂的操作系统支持。\n\n#\n#端口号\nFTP: 20/数据流, 21/控制流\nTELNET: 23\nSMTP: 25\nDNS: 53\nSNMP: 161/162\n","categories":["computer-networking"],"tags":["计算机网络"]},{"title":"CMU 15-445 数据库系统 实验记录","url":"/2024/05/03/CMU-15-445-database-systems-fall-2023-projects/","content":"\n以下 Project 内容均基于 Fall 2023 的版本.\n\n#Project 0 - C++ Primer\n简单的 C++ 语法练习, 略过.\n#Project 1 - Buffer Pool Manager\nProject 1 要求实现 DBMS 最底层的组件之一 —— Buffer Pool Manager (BPM), BPM 是对数据库所在的物理存储的抽象, 负责在内存和外存中换入换出页面, 其作用类似与操作系统的 page cache.\n在 DBMS 中, 底层物理是一个包含海量空间的持久化存储介质, DMBS 的上层抽象, 例如 B 树、哈希表结构 和数据库中实际保存的数据就持久化在这样的物理存储上.\n然而, 按照冯诺依曼架构, CPU 想要操作物理存储并不是一件轻松的事情, 以 POSIX 文件接口为例, 需要先调用 open 打开文件, 然后 seek 移动到正确的位置, 才能 read/write 需要的数据.\n更坏的是, 我们想要读或者写的数据往往非常复杂, 需要先加载到内存中, 进行一定的计算和分析, 才能真正执行读写操作.\nBPM 组件就是为了简化这些细节, 它提供了换入换出某个物理页的接口, 能够自动管理内存和不需要的物理页, 方便上层使用.\nBPM 的主要组件包括：\n\nFrames: 存储换入页数据的内存 Buffer;\nHouseKeeper: 记录 Frame 和 Page 之间的对应关系, 每个 Frame 的 Ref count 等;\nReplacer: 统计每个换入页的访问次数和频率, 当 Frame 不够用时, 决策应该换出哪个 Page;\n\nBPM 提供读和写页面的接口, 返回的 Page/PageGuard 对象中会保存对 frame 的引用.\n\n#基本实现\n#Task 1 - LRU-K\nLRU-K 是 LRU 算法的一种变体, 与原版 LRU 算法相比, 在淘汰时, LRU-K 算法会衡量每个数据条目的倒数第 k 次被访问的时间, 按照这个时间去淘汰最久的元素.\nBustub 的 BPM 使用了 LRU-K 算法来管理物理页, 需要注意的是, BPM 还需要置换算法允许某个页面被 pin 住, 不被换出.\nLRU-K 最优的算法实现之一是使用两条链表, 一条链表保存访问未达到 k 次的元素, 另一条保存访问 k 次的元素.\n记录时, 正常记录元素访问历史, 如果是元素是首次被访问, 则插入到第一条链表尾; 如果满 k 次访问, 则移动到第二条链表尾.\n淘汰时, 先从第一条链表中淘汰, 若第一条链表没有元素, 或者所有元素都被锁定, 则再从第二条链表中选择最近 k 次访问的时间最小的元素淘汰.\n#Task 2 - Disk Scheduler\n由于磁盘的一些奇特性质（比如需要实际的寻道过程, 顺序读写比随机读写快等）, 合理调度磁盘请求往往可以获得更高的性能.\nDisk Scheduler 是 BPM 和实际物理存储的中间层, 负责直接和磁盘打交道.\nBPM 的读写硬盘请求会被发送给 Disk Scheduler 异步执行, 两者通过 std::promise 交互.\n\n关于std::future, std::promise的使用参见 C++并发查漏补缺.\n\n这部分调度算法和操作系统的文件管理类似, 常用的算法例如电梯算法等.\n#Task 3 - Buffer Pool Manager\nTask 3 基本上是把 Task 1 和 Task 2 合理地组装起来, 加上一些 RAII 和锁机制, 形成 BPM 的接口.\nBPM 的核心接口是 FetchPage(page_id), 实现：\n\n如果 page 已经在内存中, 增加 Refcount, 然后返回;\n找一个空的 slot, 如果没有空的, 则调用 Replacer 置换出一个没有正在被引用的, 可能需要写回脏页;\n从 Disk 上读取特定 page, 放在空 frame 中;\n将 frame 包装成 PageGuard 返回;\n\n最后, 如果 FetchPage 是为了写, 需要把页面设置为 dirty.\n#优化：BPM &amp; Replacer 减少一道锁\nReplacer 组件本身是线程安全的, 这是通过内部加锁实现的.\nBPM 组件也是线程安全的, 也有锁.\n当 Replacer 组件被集成进 BPM 内部时, BPM 本身的锁就足以保证 Replacer 的安全, 所以 Replacer 再加锁就冗余了.\n改进：Replacer 额外提供一套不加锁的接口, BPM 内部调用这套不加锁的接口.\n#优化：BPM 减小锁范围\n在拿到一个 frame 之后, 我们可以立即切换到 frame 级别的细粒度锁, 释放 BPM 的大锁, 只要保证不会有其他线程操作同样的 frame 即可.\n需要注意的是, Page 本身的锁（rwlatch_）是是保护数据内容用的（而不是 Page 的元数据）, 在 Project 1, 由于其他部分的代码没有使用它, 因此可以直接用 Page 内部的锁, 但是到 Project 2 就会出问题.\n因此最好在 BPM 里面维护单独的 Per-Frame 锁.\n需要小心处理 evict 的情况, slot 被置换的时候, 旧的 page 的刷脏请求需要在大锁保护下提交.\n否则有可能出现某个 page 刚被另一个线程 evict 掉, 还没来得及刷脏; 另一个线程立即来读这个 page, 又 evict 另外一个 page 的空间, 先读后刷脏的情况. 需要用大锁保证刷脏页一定先执行.\n#优化：BPM workload 区分\nBufferPoolManager::UnpinPage 接口传入了当前页访问的模式, 可以是 Lookup、Scan、Index 三种. BPM 可以根据这个信息做未来页面的预测.\n在 LRU-K Replacer 中, 当元素被首次访问, 而且访问模式是 Scan, 可以把元素插入到第一条链表头而非链表尾, 使之更快被淘汰.\n#优化：DiskScheduler 并发 I/O\n\n在 Disk Scheduler 中开一个线程池, 每个线程开一个单独的任务队列. 收到的 IO 任务按 page_id % n_worker 分派到任务队列中.\nBustub 中提供了 Channel 类可以直接使用.\n#优化：DiskScheduler 缓存\n\n每个 Worker 带一小块页缓存, 读写的时候如果缓存中的内容是有效的, 就进行 memcpy, 尽可能减少对 OS 的调用.\n缓存可以同样使用 LRU-K 策略替换.\n#优化：Sharded Buffer Pool Manager\nBPM 使用了一把大锁保证线程安全, 然而大锁限制了性能. 我们可以按照 page_id 对 BPM 分片.\n每 N 个 frame 划分一个分片, 按照 page_id % shard_num 决定由哪个分片处理.\n需要注意所有分片总的 pool_size 要和不分片的情况保持一致. 还有 NewPage() 的时候要保证 page_id 全局单调递增.\n\n#优化：BPM 缓存\nSpring 2024 的 bustub 添加了官方的 BPM 缓存实现, 使用的是 WriteBackCache 这个数据结构.\n不过笔者没使用这个.\n#坑点\n\nFall 2023 的测试中没有良好处理 bpm 和 disk_manager 的退出顺序, 如下面伪代码所示. 这会导致 bpm 缓存中的数据还没有完全写回磁盘上, 负责磁盘 IO 的 disk_manager 就已经关闭.\n\ndisk_manager-&gt;ShutDown();remove(&quot;test.db&quot;);delete bpm;delete disk_manager;\n\n如果 BufferPoolManagerTest.SchedulerTest 没过, 考虑是不是 is_dirty_ 的设置不对, 在创建 WritePageGuard 的时候就应该标记页面为 dirty.\n\n#Project 2 - Hash Index\nProject 2 的目标是在前面实现的换页基础上, 实现一个三级可拓展哈希表.\n#可拓展哈希表结构\n\n每个 Bucket 右侧的数字是它的 Local Depth, 表示该桶内的元素最多有多少个 bit 是一致的.\n简单推论：\n\n如果GD = LD, 则一定只有一个 entry 指向当前 bucket;\n如果GD &lt; LD, 则当前 bucket 被 2(GD−LD)2^{\\left(\\text{GD}-\\text{LD}\\right)}2(GD−LD) 个 entry 指向.\n\n#Header\nHeader是 Project 相对于课本增加的内容, Header按照 Most Significant Bits 预先对Directory进行一次分区, 目的是增大整个系统的并发度, 对主要逻辑影响不大.\n#初始状态\n初始状态下, Directory和Bucket均不存在：\n\n#插入算法\n#算法逻辑\n\n\n计算插入元素的 hash 值;\n\n\n根据Header定位到Directory, 如不存在则创建;\n\n\n根据Directory定位到Bucket, 如不存在则创建;\n\n\n遍历Bucket, 如果元素已存在则直接结束;\n\n\n检查Bucket是否装满, 如果不满则直接插入到当前Bucket中, 结束;\n\n\n\n桶装满, 需要进行分裂操作：检查 Bucket 的 Local Depth (LD) 和 Global Depth (GD);\n\n\n如果LD == GD, 需要先扩容 Directory：\n\n如果 Directory 已经达到最大深度, 插入失败, 结束.\n翻倍 Directory, GD++, 填充好新的 Entry;\n由于区分度多了一个 bit, 需要重新计算 HashToBucketIndex;\n\n\n\n\n此时一定有 LD &lt; GD, 该桶被多个 Entry 指向.\n进行分裂操作, 将这个 Bucket 拆分成两个, 两个桶的 LD_next 均设置为原 LD+1;\n桶元素按照第 1 &lt;&lt; LD_next 位分散到两个桶中, 这一步预期是平均分配;\n然后更新 Directory 相关的 Entry 指针到新的 Bucket 上;\n\n\n\n如果当前 Bucket 仍是满的, 回到第 6 步;\n\n\n在当前 Bucket 中插入元素, 结束;\n\n\n\n#示例 1：目录翻倍两次\n原状态：\n\n插入…1001, 拓展目录：\n\n\n再次拓展目录：\n\n\n#示例 2：指针更新\n原状态：\n\n插入…010：\n原来的桶(...010)从...0变成...10, LD改变1-&gt;2;\n新创建的桶(...000)保存...00, LD=2;\nEntry ...000和...100被驱逐到新的桶中：\n\n#删除算法\n删除算法基本上是插入算法的逆序操作.\n#优化：乐观读写锁\n对 Directory Page 和 Bucket Page 的并发保护可以换成读写锁.\n乐观读写锁指在不确定是否需要修改 Directory Page 以创建新的 Bucket, 以及不确定是否需要修改 Bucket Page 以插入新的条目的时候, 先假设不需要修改, 拿读锁进行检查. 如果检查发现不符合之前的假设, 再释放读锁, 换成写锁.\n需要注意释放读锁, 换成写锁的时候, 可能有其他线程拿到锁, 需要重新检查条件是否满足.\n#优化：BPM 负优化\nProject 2 的 Leaderboard 中使用的 DiskManager 是纯内存模拟的 DiskManagerUnlimitedMemory 结构, 且没有开启读延迟, 所以某些 P1 的优化在 P2 刷榜的时候可能会减分.\n#坑点\n计算哈希值要调用Hash()而不是hash_fn_.GetHash().\n#Project 2 - B+ Tree\n#Project 3 - Query Execution\nProject 3 的目标是实现几个数据库的 Executor.\n#Project 4 - Concurrency Control\nProject 4 要求实现基于 MVCC 的事务并发控制.\n","categories":["database-system"],"tags":["Database"]},{"title":"CMU 15-445 数据库系统 课程笔记","url":"/2024/04/15/CMU-15-445-database-systems-fall-2023/","content":"教师：Andy Pavlo\n#Lecture 1: 背景\n\nDB vs DBMS\n平面文件数据库 vs 关系型数据库\nDML: 过程式 vs 声明式(SQL)\n关系代数: Select Projection Union Intersection Difference Product Join Observation\n\n#Lecture 2: SQL\n\n\n关系型语言\n\n\nSQL 标准 历史\n\n\nJoin\n\n\n聚合函数\n\n\n字符串操作: 大小写敏感 单引号 模式匹配\n\n\n日期和时间\n\n\n输出重定向\n\n\n输出控制: ASC DESC ORDER BY LIMIT\n\n\n嵌套子查询:\nSELECT in SELECT\nSELECT in FROM\nSELECT in WHERE\n通常难以优化\n输出处理: ALL ANY IN EXISTS\n\n\n窗口函数: OVER ORDER BY\n\n\n公用表表达式(CTE): WITH 临时表, WITH RECURSIVE\n\n\n#Lecture 3: 存储 Part 1\n#易失性设备\n一般支持高速随机按字节访问\n通常叫 memory\n#非易失性设备\n一般支持按块/页存取\n通常顺序存取更快\n通常叫 disk\n#持久化内存 / 非易失性内存\n用的少 不讲\n#NVMe SSDs\n仍然属于 disk\n#面向硬盘的 DBMS\nBuffer Pool: 把需要用到的数据页从硬盘搬到内存上\n能否利用 OS 的mmap()？不好，难以控制 OS 什么时候刷盘，可以用的：madvise(), mlock(), msync()\nStorage Manager: 管理硬盘上存储的文件，记录文件包含的页，文件剩余的存储空间等\n#页\n\n硬件页: 4K 可以保证原子写\nOS 页：4K\n数据库页：1-16K\n\n#页堆\n堆文件组织是一种组织页的方式。包括一个堆文件，其中包含无序的页集合。\n具体有两种组织方式：\n\n链表：每个页指向下一个页，需要 O(n)遍历，效率低\n页目录：维护特殊的页记录每个数据页的信息\n\n#页结构\n\n页大小\n校验码\n版本\n事务可见性\n自包含\n\n两种页内数据组织方式：\n\n\nSlotted-Page\n现在大多数 DBMS 使用的\n每个 slot 存一个元组\n头部保存 slot 索引，元组从后往前增长\n\n\nLog-Structured\n\n\n#元组结构\n元组是细粒度的数据，字节序列\n\n元组头\n元组数据\nUID\n逆正规化元组数据\n\n#Lecture 4: 存储 Part 2\n#Log-Structured\nSlotted-Page 的问题：\n\n内碎片：删除元组产生一个空洞\n写放大：需要读写整个块\n\nLog-Structured 的好处：\n\n日志保存的是对数据库的操作 (put, delete)\n读记录的时候，DMBS 从新到旧扫描日志文件，重建元组\n写很快，读可能会慢，已经写过的页是不可变的，减小随机 IO\n…\n\n#数据表示\n\n整数，浮点数：INTEGER，BIGINT，SMALLINT，TINYINT，FLOAT，REAL 按照 IEEE-754 标准，与 C++类型一致\n定点数：NUMERIC，DECIMAL，存储方式类似字符串\n变长数据：VARCHAR，VARBINARY，TEXT，BLOB\n日期和时间：TIME，DATE，TIMESTAMP\n\n#Lecture 5: 存储模型 &amp; 压缩\n#数据库的 Workload\n\nOLTP\nOLAP\nHTAP\n\n#存储模型\n\nNSM n 元存储模型\n\n…\n\n\nDSM 分解存储模型\n\n…\n\n\nPAX 分区字段\n\n…\n\n\n\n#数据库压缩\n\n真实世界的数据都是高可压缩的\n元组的字段之间有很强的关联性\n\n压缩的要求\n\n压缩完最好是固定长度\nlazy 解压\n必须是无损压缩\n\n压缩粒度\n\n块级别\n元组级别\n字段级别\n列级别\n\n#简易压缩\n#列式压缩\n\n游程编码\n位编码\n…\n\n#Lecture 6: 缓冲池\nProject 1 内容，略\n#OS 页缓存\n大多数 DBMS 绕过 OS 的页缓存（通过直接 IO），Postgres 除外\n#磁盘 IO 调度\n\nSequencial vs. Random I/O\nCritical Path Task vs. Background Task\nTable vs. Index vs. Log vs. Ephemeral Data\nTransaction Information\nUser-based SLAs\n\n#Lecture 7: 哈希表\n数据结构课几乎都学过，略\nSOTA 哈希函数：XXHash3\n哈希表 = 哈希函数 + 哈希策略\n哈希策略：解决哈希冲突的方法\n静态哈希策略\n\n线性扫描哈希\nCuckoo 哈希\n\n动态哈希策略\n\n链式哈希\n可拓展哈希\n线性哈希\n\n#Lecture 8: 树索引\n数据结构课几乎都学过，略\n#Lecture 9: 索引并发控制\n锁，OS 课几乎都学过，略\n#Lecture 10: 排序 &amp; 聚合算法\n#查询计划\nDBMS 会首先把 SQL 编译成查询计划，编译的目标是最小化 I/O。\n#排序\n关系模型没有定义数据元组保存的顺序，当执行 ORDER BY, GROUP BY, JOIN, DISTINCT 运算符的时候，可能需要先对数据排序。对于小数据直接在内存中排序，大量数据就需要用到外排序。\n外排序的标准算法是使用归并排序。\n\n两路归并\nK 路归并\n\n优化思路：\n\n双倍缓冲优化\n比较优化\n用 B+树索引\n\n另外，当查询同时包含了 ORDER BY 和 LIMIT 时，就等价于查询数据的 Top-N 元素，可以用另外的算法。\n#聚合\n聚合运算将一个或多个元组合并成单个标量值。运算符包括 GROUP BY，\n#Lecture 11: 连接算法\n\nThe goal of a good database design is to minimize the amount of information repetition.\n\nTODO\n#Lecture 12 &amp; 13: 查询执行\n#查询计划\nDBMS 首先把 SQL 命令转换成查询计划 (query plan)，然后再执行，为了方便优化和 debug；\n查询计划按照数的结构组织，数据从叶子节点计算到跟节点。\n#处理模型\n处理模型决定如何执行一个查询计划，主要有三种：\n\n迭代器模型 (aka, 火山模型, Pipeline 模型)\n物化模型\n向量/批处理模型\n\n处理顺序可以是自顶向下，也可以是自底向上；\n\n\n迭代器模型\n\n\n\n物化模型\n是迭代器模型的一种特例，每个 operator 在一次调用中处理完所有需要的数据；\n为了防止数据太多，DBMS 可以传递 limit 信息给 operator\n\n\n向量模型\n和迭代器模型类似，但是每次 Next 函数返回一批数据，而不是单个 tuple；\n这样允许使用 SIMD 等指令优化速度；\n\n\n#数据访问方法\nDBMS 读数据有两种最基本的操作：Sequential Scan 和 Index Scan；\n针对 Sequential Scan 的优化方法：\n\nPrefetch\nBuffer Pool Bypass\nParallelization\nLate Materialization\nHeap Clustering\nApproximate Queries\nZone Map\n\n#修改数据\n会修改数据的 Operator 需要额外维护索引信息。\nUPDATE/DELETE Operator 需要记录 RID，否则会有万圣节问题\n万圣节问题：Update 操作改变了 tuple 的位置，使得一条数据被 scan 了两遍；\n#表达式求值\n每个 tuple 都执行一遍表达式很慢，成熟的 DBMS 会根据 cost 模型决定要不要把表达式 JIT 编译；\n#分布式数据库\n现在大部分的 DBMS 都采用了 thread per worker 的方式；\n#IO 并行\n多硬盘并行\n数据库分区\n#Lecture 14: 查询计划 &amp; 查询优化\nSQL 是一种声明式语言，只说了要计算什么，但并没有说明如何去计算；\nDBMS 需要先把 SQL 转换成实际的查询计划以便执行；\nIBM System R 在 1970s 实现了第一个查询优化器，在此之前，人们普遍认为 DBMS 生成的查询计划不会比人写的更好；\n两种优化的策略：\n\n基于静态规则\n基于 cost\n\n逻辑查询计划 vs 物理查询计划\n#逻辑查询优化\n#Cost 估计\n直方图\n采样\n#Lecture 15: 并发控制理论\n两个并发的问题：\n\n并发控制：两个事务同时更新一条记录怎么处理；\n持久性：断电怎么保证状态正确；\n\n#事务\n最简单的实现是每次执行事务时，复制所有文件。\n事务的特性：\n\n原子性 A：原子性确保事务中的所有操作都发生，或者不发生。\n一致性 C：如果每个事务是一致的，并且数据库在事务开始时是一致的，那么在事务完成时保证数据库是一致的。如果数据满足所有验证规则（如约束、级联和触发器），则数据是一致的。\n隔离性 I：隔离是指当一个事务执行时，它应该有一种错觉，即它与其他事务是隔离的。隔离可确保事务的并发执行应具有与事务的顺序执行相同的生成数据库状态。\n持久性 D：如果事务提交，则其对数据库的影响应持续存在。\n\n#原子性\n实现原子性的两种方案：\n\n用日志记录所有操作：通过日志来实现事务回滚，在内存和硬盘都保存一份，常用；\nCoW 创建 Page 的副本：回滚性能较好，运行时性能较差，不常用；\n\n#一致性\n\n数据库一致性：数据库准确地表示它正在建模的真实实体，并遵循完整性约束。（例如，一个人的年龄不能是负数）。此外，将来的事务应在数据库中看到过去提交的事务的影响。\n事务一致性：如果数据库在事务开始前是一致的，那么在事务开始之后也会保持一致。确保事务一致性是应用程序的责任。\n\n#隔离性\n事务仿佛在系统中单独运行，看不到并发事务的影响。\n并发控制：\n\n悲观\n乐观\n\n#持久性\n在崩溃或重新启动后，已提交事务的所有更改都必须是持久的（即持久的）。\n#Lecture 16: 两阶段锁\n\nShared (S)：读锁\nExclusive (X)：写锁\n\n两阶段锁是乐观并发控制方法的一种，特点是第一阶段只获取锁，第二阶段只释放锁；\n2PL 能够保证冲突序列化；但是不能处理 cascading aborts（回滚事务 A 导致事务 B 读到坏数据）；\n2PL 会有脏读的问题，依然可能导致死锁；\n严格 2PL：在 2PL 的基础上，还要求必须 commit 之后才能释放锁；\n严格 2PL 能处理 cascading aborts，但是会降低并发度；\n死锁检测和死锁避免：略\n锁粒度：数据库级别，表级别，页级别，tuple 级别\n意向锁：保证不同粒度的锁之间的互斥（例如表锁和表中的行锁）的锁，具体分为：\n\nIntention-Shared (IS)：整块区域的读锁；\nIntention-Exclusive (IX)：整块区域的写锁；\nShared+Intention-Exclusive (SIX)：S+IX，表示事务要读全部行+更新部分行；\n\n\n#Lecture 17: 时间戳顺序\n时间戳顺序 (Timestamp ordering, T/O) 是一种不需要锁的并发控制手段；\n每个事务都会分配一个单调递增的唯一的时间戳，\n#基本时间戳顺序 (BASIC T/O)\n为每个 tuple 记录上一次读和写的时间戳，\n读操作：检查时间戳，如果是过去的；\n写操作：\nThomas Write Rule：\n#乐观并发控制 (OCC)\n乐观并发控制 (OCC) 是另一种并发控制方案，为每个事务保存要操作的数据的副本；\n读操作：读数据复制一份\n校验操作\n写操作：\n#幻读问题\n因为一个事务可能会插入新数据，新的数据没有上锁；\n\n重新执行一遍 scan：Commit 的时候重新执行一遍 WHERE 字句；\n条件加锁：根据条件判断的结果加锁；\n索引加锁：利用索引键保护数据；\n\nK-V 锁：对单独一条数据的锁\n间隙锁：对两条数据之间空隙的锁，防止插入新数据；\n范围锁：对一段数据的锁；\n层次锁：\n\n\n\n#隔离级别\n异常现象：\n\n脏读\n不可重复读\n幻读\n【NEW】丢失更新\n【NEW】Write skew\n\nSQL-92 标准隔离级别\n\n序列化：Index Lock+严格 2PL\n【NEW】快照隔离；\n可重复读：严格 2PL\n【NEW】游标稳定性：IBM DB2 默认级别，避免了丢失更新\n读已提交：写锁执行严格 2PL，读锁在读后立即释放（Short duration）\n读未提交：写锁执行严格 2PL，读不加锁\n\n额外的隔离级别：\n#Lecture 18: MVCC\n多版本并发控制 (MVCC) 是最广泛使用的数据库并发控制方法。\n#Lecture 19: 数据库日志\n","categories":["database-system"],"tags":["Database"]},{"title":"设计数据密集型应用","url":"/2024/09/18/DDIA/","content":"#第三章：存储与检索\n\nKV 数据\n\n哈希表\n\n\nBitCask 模型\n\n磁盘保存 Value, 内存保存&#123; Key -&gt; Value Offset &#125;的索引\n\n\n\n","categories":["database-system"],"tags":["Database"]},{"title":"[VLDB'21] Constructing and Analyzing the LSM Compaction Design Space 论文阅读","url":"/2026/01/13/constructing-and-analyzing-the-lsm-compaction/","content":"#1. 引言\n基于日志布局合并 (LSM) 的键值存储 LSM 树如今被广泛用作现代 NoSQL 键值存储的存储层.\nLSM 树采用 out-of-place 范式来实现快速数据写入. 传入的键值对被缓冲在主存中, 并定期刷新到持久存储中, 形成 sorted immutable runs.\n随着磁盘上 run 数量的增加, 它们会被排序合并, 从而构建更少但更长的 sorted runs. 这个过程被称为 Compaction.\n为了便于快速查询, LSM 树使用辅助的内存数据结构 (Bloom Filter 和 Fence Pointer) 辅助减少每次查询执行的平均磁盘 I/O 次数.\n由于这些优势, LSM 树被多个生产级键值存储系统采用, 包括 Google 的 LevelDB 和 BigTable、Facebook 的 RocksDB、阿里巴巴的 X-Engine、MongoDB 的 WiredTiger、Cockroach Labs 的 CockroachDB、LinkedIn 的 Voldemort、Amazon 的 DynamoDB、Apache 的 AsterixDB、Cassandra、HBase、Accumulo 以及 Yahoo 的 bLSM 和 cLSM.\n基于 LSM 树的学术系统包括 Monkey、SlimDB、Dostoevsky、LSM-Bush、Lethe、Silk、LSbM-tree、SifrDB 和 Leaper.\nLSM 树中的 Compaction LSM 树中的 Compaction 操作被定期执行以 通过写放大来减少读放大和空间放大 , 同时确保数据一致性和查询正确性.\nCompaction 操作会将一个或多个层之间的两个或多个 sorted run 合并, 以确保 LSM 树的层大小呈指数级增长.\nCompaction 操作通常在某个层达到其容量上限时触发, 此时 Compaction 例程会将数据从已饱和层移动到容量呈指数级增长的下一层.\n在 Compaction 过程中, 任何重复条目 (由更新导致) 和失效条目 (由删除导致) 都会被移除, 仅保留逻辑上正确的 (最新的有效) 版本.\nCompaction 操作决定了磁盘驻留数据的重整方式和时间, 从而影响磁盘上的物理数据布局.\n图 1(a) 定性地展示了当前 SOTA 的 LSM 引擎中采用的各种 Compaction 策略的性能影响.\n\n图 1: (a) SOTA 的 LSM-Engine 采用的不同压缩策略导致了性能的差异；(b) 根据设计基本要素对 LSM 压缩进行分类。\n挑战: 如何选择合适的 Compaction 策略 尽管 Compaction 对于 LSM 引擎的性能至关重要, 但选择合适的 Compaction 策略需要人为干预.\n实际上, 在基于 LSM 的生产数据存储中, “如何 (重新) 组织磁盘上的数据”, 以及&quot;实施或使用哪些 Compaction 策略&quot;, 通常取决于工程师或数据库管理员 (DBA) 的专业知识.\n这主要是由于两个原因. 首先, LSM 树中的 Compaction 过程通常是黑盒而且不会被暴露为一个可调旋钮.\n尽管 LSM Compaction 设计空间巨大, 但由于缺乏正式的 Compaction 模板, 导致严重依赖个人经验, 使得很大一部分设计空间未被探索.\n其次, 缺乏关于 Compaction 如何影响 LSM 引擎性能的分析和实验数据, 尤其是在存储引擎底层设计和工作负载特性的背景下.\n因此, 即使是专家也很难回答以下设计问题:\n\n\n我的 LSM 引擎提供的写入性能低于预期: 改变 Compaction 策略是否会有帮助? 如果有帮助, 应该采用哪些策略?\n\n\n我们过去处理的 Workload 发生了变化: 这如何影响我们系统的读取吞吐量吗? 是否存在可以提高读取吞吐量的 Compaction 策略?\n\n\n我们计划设计一种新的 LSM 引擎来处理特定工作负载: 为了获得最佳整体性能, 我应该如何 Compaction 数据? 是否存在我必须避免的 Compaction 策略?\n\n\n依靠人工经验为每个应用程序手动选择合适的 Compaction 策略是不可扩展的, 尤其是在大规模系统部署中.\n为此, 本文形式化了基于 LSM 的存储引擎中 Compaction 的设计空间.\n此外, 我们通过实验探索了该空间, 并在此基础上提出了 7 个高级要点和 12 个观察结果, 这些内容构成了一套全面的 LSM Compaction 指南, 并为 Compaction 调优和自动化奠定了基础.\n#2. 背景\nLSM 基础知识 为了支持快速数据摄取, LSM 树会将传入的 inserts、updates 和 deletes 操作 (即一般意义上的数据摄取) 缓冲在主存中.\n一旦内存缓冲区满, 其中的条目将按键进行排序, 然后将缓冲区作为一个 sorted run 刷到树的磁盘部分.\nsorted run 是指一个或多个大小通常相同的 immutable 文件的集合.\n对于具有 L 层的 LSM 树, 我们假设其第一层 L(0) 是内存缓冲区, 其余层 (L(1) 到 L(L-1)) 驻留在磁盘上.\n在磁盘上, L(i) (i &gt; 1) 层的容量比第 L(i-1) 层大 T 倍, 其中 T 是树的大小比例.\nLSM Compaction 为了限制磁盘上 sorted run 的数量 (从而加快查询速度并提高空间利用率), LSM 树会定期将 L(i) 的 run (或 run 的一部分) 与 L(i+1) 中重叠的 run 进行排序合并.\n这种数据重整过程, 即在磁盘上创建更少、更长的 sorted run, 被称为 Compaction.\n然而, 数据排序合并过程需要在磁盘和主存之间来回移动数据. 这会导致写放大, 在 SOTA 的基于 LSM 的数据存储中, 写放大倍数可能高达 40 倍.\nPartial Compaction 为了分摊文件选择成本, 从而避免延迟峰值, SOTA 的 LSM 引擎会将数据组织成更小的文件, 并以文件为单位而非以层为单位执行 Compaction.\n如果 L(i) 的数据增长超过阈值, 则会触发 Compaction, 并从 L(i) 中选择一个文件 (或文件子集) 与 L(i+1) 中具有重叠键范围的文件进行 Compaction.\n此过程称为 Partial Compaction.\n图 2 展示了 LSM 树中完全 Compaction 和 Partial Compaction 例程的对比图.\n\n图 2: (a) 调用时, 经典的 Full Compaction 过程一次压缩整个 Level, 而 (b) Partial Compaction 以文件为粒度执行压缩。\n查询 LSM 树 由于 LSM 树以 out-of-place 方式进行更新和删除, 因此树中可能存在多个具有相同键的条目, 但只有最新版本有效.\n点查询 点查询从内存缓冲区开始, 从最小层到最大层遍历树, 并在同一层内从最新 run 到最旧 run 遍历.\n找到匹配的键后, 查询立即终止.\nSOTA 的 LSM 引擎使用内存数据结构, 例如 Bloom Filter 和 Fence Pointer 限制查询探测的 run 次数.\n范围扫描 范围扫描需要对树所有层中符合范围查询条件的 run 进行排序合并.\n这些 run 在内存中进行排序合并, 并返回每个符合条件的条目的最新版本, 同时丢弃所有较旧的、逻辑上无效的版本.\nLSM 树中的删除操作 点删除操作通过插入一种特殊的键值对条目 (称为 Tombstone) 来实现, 该条目在逻辑上使目标条目失效, 但并不实际影响它们.\n在 Compaction 过程中, Tombstone 会清除所有具有匹配键的旧条目.\n当相应的 Tombstone 到达树的最后一个层时, 删除操作最终被视为持久删除, 此时可以安全地删除该 Tombstone.\n从基于 LSM 的数据存储中持久删除数据对象所需的时间取决于数据重整过程.\n因此, Compaction 在及时持久地删除条目方面也发挥着至关重要的作用, 尤其是在新的数据隐私法规出台的背景下.\n#3. Compaction 设计空间\n#3.1. Compaction 基本要素\n我们将 Compaction 策略定义为设计基本要素的集合.\n它代表了关于物理数据布局和数据 (重) 组织策略的基本决策. 每个基本要素都回答了一个基本的设计问题.\n\nCompaction 触发条件: 何时需要进行数据重整?\n数据布局: 如何在存储设备上进行数据物理布局?\nCompaction 粒度: 在布局重整期间一次需要移动多少数据?\n数据迁移策略: 重整过程中需要迁移哪些数据块?\n\n这些设计基本要素共同定义了 LSM 引擎何时以及如何重整持久介质上的数据布局.\n所提出的基本要素涵盖了所有 SOTA 的 LSM Compaction 策略, 并且能够合成新的或未探索过的 Compaction 策略.\n\n图 3: 定义 LSM 压缩的基本要素: 触发器、数据布局、粒度和文件选择策略。\n#3.1.1 Compaction 触发条件\nCompaction 触发条件是指可以启动 Compaction 作业的一系列事件.\n最常见的 Compaction 触发条件是基于 LSM 树中某一层的 Saturation.\nL(i) 的 Saturation 通常以 L(i) 存储的数据字节数与该层理论容量 (以字节为单位) 的比值来衡量.\n一旦 Saturation 超过预定义的阈值, L(i) 中的一个或多个 immutable 文件将被标记为待 Compaction.\n一些 LSM 引擎使用层中的文件数量来计算 Saturation.\n需要注意的是, 基于文件数量的 Saturation 计算方法仅在所有 immutable 文件大小相等, 或者系统文件大小可调的情况下才有效.\n基于 “sorted runs” 的触发条件如果一个层中的 sorted runs (或 “tiers”) 的数量超过预定义的阈值, 则触发 Compaction, 而不管层的大小如何.\n其他 Compaction 触发因素包括 文件 staleness、基于 Tombstone 的 TTL 以及 空间和读放大.\n例如, 为了确保更新和删除操作能够传播到树的更深层, 一些 LSM 引擎会在创建每个文件时为其分配一个 TTL.\n每个文件可以在一个层中存活一段有限的时间, 一旦 TTL 到期, 该文件就会被标记为需要 Compaction.\n另一个由删除操作驱动的 Compaction 触发条件确保一些 LSM 引擎会在 LSM 树中创建文件时, 通过不同的基于时间戳的方案, 为每个文件分配一个 TTL, 以限制删除操作的持久性延迟.\n每个包含至少一个 Tombstone 标记的文件, 在每个层中都会被分配一个特殊的 TTL, 一旦该计时器到期, 文件就会被标记为待 Compaction.\n下面, 我们列出最常见的 Compaction 触发条件:\n\nLevel Saturation(LS): 层大小超过标称阈值\nSorted Runs 数量(SR): 某一层的 sorted runs 次数达到阈值\nFile Staleness(FS): 文件在某一层停留时间过长\n空间放大(SA): 总空间放大超过阈值\nTombstone-TTL(TTL): 存在 Tombstone TTL 过期\n\n#3.1.2 数据布局\n数据布局由 Compaction 急迫程度决定, 并通过控制每个层的 sorted runs 个数来决定磁盘上的数据组织方式.\nCompaction 操作会在存储和内存之间移动数据, 消耗大量的设备带宽.\n因此, 摄入数据 (外部) 和 Compaction (内部) 之间存在着对设备带宽的固有竞争 – 这种权衡取决于 Compaction 优先级.\n数据布局通常分为 Leveling 布局和 Tiering 布局.\n\nLeveling 布局中, 一旦 L(i) 触发 Compaction, 标记为 Compaction 的文件将与 L(i+1)中重叠的文件合并, 并将结果写回 L(i+1). 因此, L(i+1)最终会得到一个更长的、sorted、immutable 的文件序列.\nTiering 布局中, 每一层可以包含多个具有重叠键域的 sorted run. 一旦 L(i) 触发 Compaction, L(i) 中所有 sorted run 将合并在一起, 并将结果作为一个新的 sorted run 写入 L(i+1), 而不会影响该层中已有的序列.\n\nDostoevsky 提出了一种混合设计, 其中最后一层采用 Leveling 布局, 其余磁盘层均采用 Tiering 布局. 文献 [24, 37] 对此思想进行了推广, 提出了一种连续体设计方案, 允许每一层单独决定采用 Tiering 布局还是 Leveling 布局.\n在生产系统中, RocksDB 的 L1 采用 Tiering 布局, 并允许其持续增长, 以避免在数据摄取密集型工作负载中出现 Write-Stall.\n以下列出了数据布局中最常见的几种选项:\n\nLeveling: 每层一个 sorted run\nTiering: 每层多个 sorted run\nFirst-Leveling: 第一层采用 Tiering, 其余层采用 Leveling\nLast-Leveling: 最后一层采用 Leveling, 其余层采用 Tiering\nHybrid: 一个层既可以是 Tiering, 也可以是 Leveling\n\n#3.1.3 Compaction 粒度\nCompaction 粒度是指单次 Compaction 作业期间移动的数据量.\n一种 Compaction 数据的方法是对数据进行排序合并, 并将所有数据从一个层移动到下一个层 – 称为 Full Compaction.\n这会导致在 Compaction 过程中由于大量文件选择而出现周期性的 I/O Burst, 并且随着树的深度增加, 延迟峰值会加剧, 导致长时间的 Write-Stall.\n为了分摊 Compaction 带来的 I/O 开销, 基于 Tiering 的 LSM 的引擎采用了 Partial Compaction, 其中每次 Compaction 只涉及较小粒度的数据, 而不是移动整个层.\n数据粒度可以是单文件或多文件, 具体取决于系统设计和工作负载.\n需要注意的是, Partial Compaction 并不会从根本上改变 Compaction 导致的文件选择总量, 而是将这些文件选择均匀地分摊到时间上, 从而防止出现不必要的 Latency Spike.\n针对 sorted runs 的 Compaction 粒度主要适用于采用惰性合并策略的 LSM.\n一旦在 L(i) 触发 Compaction, L(i) 中的所有 sorted runs (或层) 将被 Compaction 在一起, 并将生成的条目写入 L(i+1), 作为一个新的 immutable sorted runs.\n下面, 我们列出最常见的 Compaction 粒度:\n\nLevel(L): 两个连续层中的所有数据\nSorted Runs(SR): 某一层中的所有 sorted runs\nSorted File(F): 一次处理一个 Sorted file\n多个 Sorted File(Fs): 一次处理多个 Sorted file.\n\n#3.1.4 文件选择策略\n当 Partial Compaction 启动后, 文件选择策略会选择要 Compaction 的文件.\n一种简单的选择文件的方式是随机选择或使用 Round-robin 策略, 并不专注于优化任何特定的性能指标, 而是有助于减少空间放大.\n为了优化读取吞吐量, 许多生产数据存储会在触发 Compaction 时选择当前层中最冷的文件.\n另一个常见的优化目标是最小化写放大.\n在这种策略中, 与目标层重叠最少的文件会被标记为 Compaction 文件.\n为了减少空间放大, 一些存储引擎会选择 Tombstone 标记和/或更新次数最多的文件.\n另一种考虑删除操作的方法引入了一种基于 Tombstone 标记年龄的文件选择策略, 旨在及时持久化逻辑删除.\n下面, 我们列出了常见的文件选择策略:\n\nRound-Robin(RR): 以 RR 方式选择文件\n重叠最少的父文件(+1): 与父文件重叠最少的文件\n重叠最少的祖父文件(+2): 与父文件重叠最少的文件, 但与祖父文件重叠最少\n最冷文件©: 最近最少访问的文件.\n最旧文件(O): 一层中最旧的文件\nTombstone 密度(TD): Tombstone 数量超过阈值的文件\nTombstone TTL(TTL): 存在 Tombstone TTL 过期的文件\n\n#3.2 Compaction 作为基本要素集合\n每种 Compaction 策略都接受四个基本要素中的一个或多个值. 触发条件、粒度和文件选择策略是多值基本要素, 而数据布局是单值基本要素.\n例如, 一种常见的 LSM 设计采用 Leveled 布局 (数据布局), 一旦某个层达到标称大小 (触发条件) , 就会一次性 Compaction 整个层 (粒度).\n这种设计没有实现许多精细的优化, 例如 Partial Compaction, 因此根据定义, 它不需要文件选择策略.\n一个更复杂的例子是 Leveled LSM 树 (数据布局) 的 Compaction 策略, 其中 Compaction 以文件为粒度执行. 如果满足以下任一条件, 则会触发 Compaction:\n\n某个层达到其容量\n包含 Tombstone 的文件在某个层中保留的时间超过预设的 TTL\n\n一旦触发, 文件选择策略会选择:\n\nTombstone 密度最高的文件\n否则, 与父层重叠最少的文件\n\nCompaction 设计空间大小 如果两种 Compaction 策略在四个基本要素中的至少一个上存在差异, 则认为它们彼此不同.\n即使仅在一个基本要素上存在差异, 在相同的硬件上运行并承受相同的工作负载时, 它们的性能也可能截然不同.\n代入一些典型的基本要素基数值, 我们估计 Compaction 策略的基数大于 10 种, 这是一个庞大但尚未被充分探索的设计空间.\n表 1 展示了该空间的一个代表性部分, 详细列出了二十多个学术和生产系统中使用的 Compaction 策略.\nCompaction 策略分析 为了进行分析和实验, 我们选择了十种在生产和学术界基于 LSM 的系统中普遍存在的具有代表性的 Compaction 策略.\n我们在表 2 中对这些候选 Compaction 策略进行了编码和展示.\n\nFull 表示 Tiering LSM 树的 Compaction 策略, 该策略在调用时 Compaction 整个层.\n+1 和 +2 表示两个 Partial Compaction 例程, 分别选择与父层 (i+1) 和祖父层 (i+2) 中的文件重叠最小的文件进行 Compaction.\nRR 以 Round-robin 方式从每个层中选择文件进行 Compaction.\nCold 和 Old 是读友好策略, 分别标记层中最冷和最旧的文件进行 Compaction.\nTSD 和 TSA 是删除驱动的 Compaction 策略, 其触发条件和文件选择策略分别由 Tombstone 密度和文件中包含的最旧 Tombstone 的年龄决定.\nTier 表示 Tiering 数据布局的一种变体, 其中当 (a) 层中 sorted run 数量或 (b) 树中估计的空间放大达到特定阈值时, 将触发 Compaction. 这种 Tiering 解释在 RocksDB 等系统中也被称为 Universal Compaction.\n1-Lvl 表示一种混合数据布局, 其中第一磁盘层采用 Tiering 布局, 而其他磁盘层采用 Leveling 布局. 这是 RocksDB 的默认数据布局.\n\n表 1: SOTA 系统中的压缩策略 [L 表示采用 Leveling 结构的层级; T 表示采用 Tiering 结构的层级]\n\n\n\nDatabase\n数据布局\n触发条件\n粒度\n文件选择\n\n\n\n\nRocksDB, Monkey\n(1-)Level\nLS, FS\nF, Fs\n+1, C, O, TD\n\n\n\nTiering\nSR, SA, TTL\nSR\nNA\n\n\nLevelDB, Monkey(J.)\nLevel\nLS\nF\nRR, +1, +2\n\n\nSlimDB\nTiering\nLS\nF, Fs\nNA\n\n\nDostoevsky\nL-level\nLS(L), SR(T)\nL(L), SR(T)\n+1(L), NA(T)\n\n\nLSM-Bush\nHybrid\nLS(L), SR(T)\nL(L), SR(T)\n+1(L), NA(T)\n\n\nLethe\nLevel\nLS, TTL\nF, Fs\n+1, TTL\n\n\nSilk, Silk+\nLevel\nLS\nF, Fs\nRR\n\n\nHyperLevelDB\nLevel\nLS\nF\nRR, +1, +2\n\n\nPebblesDB\nHybrid\nLS\nF, Fs\nNA\n\n\nCassandra\nTiering\nSR, FS, TTL\nSR\nNA\n\n\n\nLevel\nLS, TTL\nF, Fs\n+1, TD, TTL\n\n\nWiredTiger\nLevel\nLS\nL\nNA\n\n\nX-Engine, Leaper\nHybrid\nLS\nF, Fs\n+1, TD\n\n\nHBase\nTiering\nSR\nSR\nNA\n\n\nAsterixDB\nLevel\nLS\nL\nNA\n\n\n\nTiering\nSR\nSR\nNA\n\n\nTarantool\nL-level\nLS(L), SR(T)\nL(L), SR(T)\nNA\n\n\nScyllaDB\nTiering\nSR, FS, TTL\nSR\nNA\n\n\n\nLevel\nLS, TTL\nF, Fs\n+1, TD, TTL\n\n\nbLSM, cLSM\nLevel\nLS\nF\nRR\n\n\nAccumulo\nTiering\nLS, SR, TTL\nSR\nNA\n\n\nLsbM-tree\nLevel\nLS\nL\nNA\n\n\nSifrDB\nTiering\nLS\nFs\nNA\n\n\n\n","categories":["database-system"],"tags":["分布式系统","数据库"]},{"title":"[VLDB'10] Dremel: Interactive Analysis of Web-Scale Datasets 论文阅读","url":"/2025/09/02/dremel-interactive-analysis-of-web-scale-datasets/","content":"#0. 摘要\nDremel 是一个可扩展、交互式的针对只读嵌套数据的分析查询系统。通过结合多级执行树和列式存储布局，它能够在秒内对万亿行表进行聚合查询。该系统能够扩展到数千 CPU 和 PB 级别的数据，并且在 Google 内部有数千用户。在这篇论文中，我们描述了 Dremel 的体系结构和实现，并解释了它是如何补充基于 MapReduce 的计算的。我们提出了一个新颖的嵌套记录的列式存储表示，并讨论了在系统的小规模实例上（不到几千个节点）所做的一些实验。\n#1. 引言\n大规模分析数据处理在 Web 公司和跨行业变得越来越普遍，尤其是因为低成本存储使得收集大量商业关键数据成为可能。将这些数据放在分析师和工程师的指尖变得越来越重要；交互式响应时间通常会在数据探索、监控、在线客户支持、快速原型制作、数据管道调试和其他任务中产生质的差异。\n在大规模上执行交互式数据分析需要高程度的并行性。例如，使用当今的商用硬盘驱动器想要在一秒钟内读取 1TB 的压缩数据，可能需要数千个磁盘。同样，CPU 密集型查询可能需要在数千个内核上运行数秒才能完成。在 Google，使用共享的集群化机器进行大规模并行计算。一个集群通常托管许多分布式应用程序，这些程序共享资源、具有广泛的工作负载，并且在具有不同硬件参数的机器上运行。分布式应用程序中的单个工作者可能比其他工作者花费更长的时间来执行给定的任务，或者由于群集管理系统的故障或抢占而永远无法完成该任务。因此，处理拖延者和故障对于实现快速执行和容错至关重要。\n用于网络和科学计算的数据通常是非关系型的。因此，在这些领域中，灵活的数据模型至关重要。编程语言中的数据结构、分布式系统中交换的消息、结构化文档等交换的数据，很自然地适合嵌套表示。在 Web 规模上规范并重新组合这样的数据通常是不可行的。Google 及其他一些主要的网络公司据报道，在其大部分结构化数据处理中都使用了基于层次结构的数据模型。\n本文描述了一个名为 Dremel 的系统，该系统支持在共享的商用机器集群上对超大规模数据集进行交互式分析。与传统数据库不同，它能够就地操作嵌套的数据。这里的“就地”是指访问“就地”存储的数据的能力，例如分布式文件系统（如 GFS [14]）或另一个存储层（如 Bigtable [8]）。\nDremel 可以执行许多查询，这些查询通常需要一系列 MapReduce (MR [12]) 作业来完成，但其执行时间仅为其中的一小部分。\nDremel 不旨在取代 MR，而是经常与 MR 配合使用，用于分析 MR 管道的输出或快速原型化更大的计算任务。\nDremel 自 2006 年开始生产，拥有数千名 Google 用户。公司部署了多个 Dremel 实例，从数十个到数千个节点不等。使用该系统的例子包括：\n\nAnalysis of crawled web documents\nTracking install data for applications on Android Market.\nCrash reporting for Google products\nOCR results from Google Books\nSpam analysis\nDebugging of map tiles on Google Maps\nTablet migrations in managed Bigtable instances\nResults of tests run on Google’s distributed build system\nDisk I/O statistics for hundreds of thousands of disks.\nResource monitoring for jobs run in Google’s data centers.\nSymbols and dependencies in Google’s codebase\n\nDremel 是在网页搜索和并行数据库管理系统的基础上构建的。\n\n它的架构借鉴了分布式搜索引擎中使用的 服务树 概念。就像网络搜索请求一样，查询会沿着树向下传递，并在每个步骤中被重写。通过聚合从树下层接收到的回复来组装查询结果。\nDremel 提供了一种类似于 SQL 的高级语言来表达 ad hoc 查询。与 Pig 和 Hive 等层不同，它原生地执行查询而无需将其转换为 MapReduce 作业。\n也是最重要的，Dremel 使用了一种分层存储表示法，这使得它能够从次级存储中读取更少的数据，并且通过更便宜的压缩降低 CPU 成本。列式存储已被用于分析关系数据[1]，但据我们所知，尚未扩展到嵌套数据模型。我们提出的列式存储格式得到了谷歌许多数据处理工具的支持，包括 MR、Sawzall [20] 和 FlumeJava [7] 。\n\n本文的主要贡献如下：\n\n我们描述了一种用于嵌套数据的新列式存储格式。我们在第 4 节中介绍了拆分嵌套记录并重新组装它们的算法。\n我们概述了 Dremel 的查询语言和执行。两者都是为了高效地操作列状嵌套数据而设计的，不需要重新组织嵌套记录（第 5 节）。\n我们展示了如何在数据库处理中应用 Web 搜索系统中使用的执行树，并解释了它们对于高效回答聚合查询的好处（第 6 部分）。\n我们在第 7 节中报告了在运行在 1000 到 4000 个节点上的系统实例上进行的万亿记录、多 TB 数据集的实验。\n\n#2. 背景\n我们从一个示例场景开始，该示例展示了交互式查询处理如何融入更广泛的 数据管理 生态系统。假设谷歌工程师爱丽丝想出了一种提取网页中新信号的新方法。她运行了一个 MapReduce 作业，通过输入数据并生成包含新信号的数据集，在分布式文件系统中的数十亿条记录中存储这些新信号。为了分析她的实验结果，她启动了 Dremel 并执行了一些交互式命令：\nDEFINE TABLE t AS /path/to/data/*SELECT TOP(signal1, 100), COUNT(*) FROM t\n她的命令几秒钟内就能执行。她运行了一些其他的查询，以确保自己的算法有效。她在 信号 1 中发现了一个不规则性，并通过编写一个更复杂的分析计算程序，深入挖掘了她输出的数据集。问题解决后，她搭建了一个管道来处理不断输入的数据。她制定了一些预先定义好的 SQL 查询语句，对管道中的结果进行汇总，并将其添加到交互式仪表板中。最后，她在目录中注册了新的数据集，这样其他工程师就可以快速找到并查询它。\n上述场景需要查询处理器和其他数据管理工具之间的互操作。为此，首先需要一个通用存储层。Google 文件系统（GFS [14]）是公司内广泛使用的一种分布式存储层。\nGFS 使用复制来保存数据，尽管硬件出现故障，也能实现快速响应时间，并在存在迟到者时提供高吞吐量。高性能存储层对于原位数据管理至关重要。它允许在没有费时加载阶段的情况下访问数据，而数据库使用是分析数据处理中的主要瓶颈[13]，通常可以在数据库管理系统能够加载数据并执行单个查询之前运行数十个 MR 分析。此外，文件系统中的数据可以使用标准工具轻松操作，例如将数据传输到另一个群集、更改访问权限或根据文件名识别要分析的数据子集。\n\n构建可互操作的数据管理组件的第二个要素是共享存储格式。 列存储对于扁平关系型数据很成功，但要使其适用于谷歌，则需要将其适应为嵌套的数据模型。 图 1 描述了这个基本思想：所有具有嵌套字段 (A.B.C) 的值都按连续方式存储。 因此，可以检索 A.B.C 而无需读取 A.E、A.B.D 等等。 我们解决的挑战在于如何保留所有结构信息并能够从任意一组字段重建记录。 接下来讨论我们的数据模型，然后介绍算法和查询处理。\n#3. 数据模型\nDremel 的数据模型起源于分布式系统，广泛应用于谷歌，并以开源实现的形式提供。数据模型基于强类型的嵌套记录。它的抽象语法如下：\nτ=dom∣&lt;A1:τ[∗∣?],⋯ ,An:τ[∗∣?]&gt;\\tau = \\text{dom} | \\left&lt; A_1 : \\tau[*|?], \\cdots, A_n : \\tau[*|?] \\right&gt;\nτ=dom∣⟨A1​:τ[∗∣?],⋯,An​:τ[∗∣?]⟩\n其中 τ 是原子类型或记录类型。原子类型内的索引包括整数、浮点数、字符串等。记录由一个或多个字段组成。记录中的第 i 个字段有一个名称 Ai 和可选的多重性标签。\nRepeated 字段 * 可以在记录中出现多次。它们被解释为值列表，即记录中字段出现的顺序是有意义的。\nOptional 字段 ? 可能不存在于记录中。否则，字段是必需的，即必须恰好出现一次。\n\n例如，考虑图 2。它显示了一个定义文档类型的模式，表示一个 Web 文档。该模式定义使用了来自 [21] 的具体语法。文档有一个必需的整数型 DocId 和一个可选的链接 Links，其中包含一组向前和向后条目，每个条目都包含其他网页的 DocId。文档可以有多个名称，这些名称是以不同 URL 引用文档的方式。名称由一对一对的 Code 和（可选）Country 组成。\n图 2 还展示了两个符合该模式的样本记录 r1 和 r2。记录结构使用缩进进行说明。在接下来的几节中，我们将使用这些示例记录来解释算法。模式中定义的字段形成树形层次结构。嵌套字段的完整路径使用常规点标记法表示，如 Name.Language.Code。\n谷歌使用嵌套的数据模型作为其结构化数据序列化的中立且可扩展的基础。 代码生成工具为诸如 C++ 或 Java 之类的编程语言生成绑定。通过在记录之间以按顺序排列的方式布置字段值的标准二进制的 wire 表示来实现跨语言互操作性。这样，用 Java 编写的 MR 程序就可以从通过 C++ 库暴露的数据源中消费记录。因此，如果记录存储在列式格式中，则能够快速地组装它们对于与 MR 和其他数据处理工具进行交互很重要。\n#4. 嵌套列式存储\n如图 1 所示，我们的目标是按顺序存储给定字段的所有值以提高检索效率。在本节中，我们将解决以下挑战：\n\n无损表示记录结构的列格式（第 4.1 节）\n快速编码（第 4.2 节）\n高效的记录组装（第 4.3 节）\n\n#4.1 重复与定义级别\n数据值本身不能传达记录结构。给定一个重复字段的两个值，我们不知道该值在哪个“级别”上被重复（例如，这些值是否来自两个不同的记录，还是同一记录中的两个重复值）。同样地，对于缺失的可选字段，我们不知道哪些包含它的记录是明确定义的。因此，我们引入了重复级别和定义级别的概念，如下面所定义的。为了参考，请参阅图 3，它总结了我们样本记录中所有原子字段的重复级别和定义级别。\n\n重复级别。考虑图 2 中的字段 Code。它在 r1 中出现了三次。‘en-us’ 和 ‘en’ 出现在第一个 Name 中，而 ‘en-gb’ 则出现在第三个 Name 中。为了消除这些出现的歧义，我们在每个值上附带了一个重复级别。它告诉我们该值在字段路径中的哪个重复字段中重复了。\nName.Language.Code 字段路径包含两个重复字段，即 Name 和 Language。因此，Code 的重复级别范围为 0 到 2；级别 0 表示新记录的开始。假设我们正在从上到下扫描记录 r1。当我们遇到 ‘en-us’ 时，我们还没有看到任何重复字段，即重复级别为 0。当看到 ‘en’ 时，Language 字段已重复，所以重复级别为 2。最后，当我们遇到 ‘en-gb’ 时，Name 最近发生了重复（Language 只在 Name 后面出现了一次），因此重复级别为 1。因此，r1 中 Code 值的重复级别分别为 0、2 和 1。\n注意，r1 中的第二个 Name 中不包含任何 Code 值。为了确定“en-gb”出现在第三个名称而不是第二个名称中，我们在 “en” 和 “en-gb” 之间添加了一个空值（参见图 3）。Code 是 Language 中的一个必填字段，因此缺少它意味着 Language 没有定义。然而，在一般情况下，要确定嵌套记录存在的级别需要额外的信息。\n定义级别。路径 p 的每个字段值，特别是每个 NULL 值，都有一个定义级别，用于指定在 p 中定义成*或者?，但是实际上有值的字段的数量。例如，观察到 r1 没有向后的链接。然而，字段 Links 定义（在第一级）。为了保留此信息，我们在 Links 向后列中添加了一个具有第一级定义级别的 NULL 值。同样地，r2 中缺少 Name.Language.Country 的发生情况带有第一级定义级别，而它在 r1 中的缺失分别发生在第二级（在 Name.Language 内）和第一级（在 Name 内）。\n上述编码方法 无损地保留了记录结构。为了节省篇幅，我们省略了证明。\n编码。每一列都存储为一组块。每个块包含重复级别和定义级别（从现在开始，简单地称为级别）以及压缩字段值。NULL 不会明确存储，因为它们由定义级别决定：如果字段路径中可选字段的数量小于定义级别，则该定义级别表示一个空值。对于始终已定义的值不会存储定义级别。同样，只有在需要时才存储重复级别；例如，定义级别 0 暗示重复级别 0，因此后者可以省略。实际上，在图 3 中，并未为 DocId 存储任何级别。级别以位串的形式进行打包。我们只使用必要的 bit 数；例如，如果最大定义级别为 3，则每个定义级别使用 2 个 bit。\n#4.2 把记录分割成列\n我们已经在上文中展示了如何以表格形式对记录结构进行编码。下一个我们要解决的问题是如何高效地生成重复度高且定义层次分明的列块。\n计算重复和定义级别的基本算法在附录 A 中给出。 算法递归进入记录结构并为每个字段值计算级别。 如前所述，即使字段值缺失，也可能会计算重复和定义级别。 Google 使用的许多数据集都是稀疏的；比较常见的是具有数千个字段的 Schema，但是在一条记录中只有一百个被用到。因此，我们试图以尽可能低的成本处理缺失字段。为了产生列块，我们创建了一个 field writer 的树，其结构与模式中的字段层次结构匹配。基本思想是只在 field writer 拥有自己的数据时更新它们，并且除非绝对必要，否则不尝试将父状态传播到树的下一级。为了做到这一点，子 Writer 从他们的父母那里继承了层次结构。每当添加一个新值时，子 Writer 就会与父级同步。\n#4.3 记录的组装\n高效地从行列数据中组装记录对面向记录的数据处理工具（例如，MapReduce）至关重要。给定一个字段子集，我们的目标是重建原始记录，就像它们只包含所选字段一样，所有其他字段都被删除了。关键思想是这样的：我们创建了一个有限状态机 (FSM)，它读取每个字段的值和级别，并按顺序将值附加到输出记录中。\nFSM 的状态对应于每个选定字段的字段读取器。状态转换带有重复级别标签。一旦读者获取了一个值，我们就查看下一个重复级别以确定要使用哪个下一个读者。对于每条记录，FSM 从起始状态遍历到结束状态一次。\n\n图 4 显示了一个在我们的示例中重建完整记录的状态机。状态机的起始状态为文档 ID。一旦读取了文档 ID 值，状态机会转换到 Links.Backward。当所有重复的 Backward 值都已消耗完后，状态机会跳转到 Links.Forward 等等。有关记录组装算法的详细信息，请参见附录 B。\n为了概述 FSM 状态转移是如何构建的，让 l 是当前字段读取器为字段 f 返回的下一个重复级别。从 schema 树中的 f 开始，在其祖先中找到一个在级别 l 处重复，并选择该祖先内的第一个叶子字段 n。这给我们带来了 FSM 转换(f,l)-&gt;n。例如，假设 l = 1 是通过 Name.Language.Country 字段读取的下一个重复级别。它的具有重复级别的 1 级祖先为 Name，其第一个叶子字段为 n = Name.Url。FSM 构造算法的详细信息请参见附录 C。\n\n如果只需要检索子集字段，那么我们构造一个更简单的有限状态机来执行。图 5 展示了一个读取字段 DocId 和 Name.Language.Country 的有限状态机。图中展示了由自动机生成的输出记录 s1 和 s2。注意我们的编码和组装算法\n#5. 查询语言\n\nDremel 的查询语言基于 SQL，并旨在高效地在 列存储的嵌套表 上实现。本文档的范围不包括对语言的正式定义；相反，我们展示了它的风格。每个 SQL 语句（以及它转换为的代数运算符）都以一个或多个嵌套表及其模式作为输入，并产生一个嵌套表及其输出模式。图 6 显示了一个示例查询，该查询执行投影、选择和记录内的聚合。对该查询进行评估时使用了来自图 2 中的表 t={r1, r2}。字段通过路径表达式进行引用。尽管查询中没有记录构造器，但该查询会产生一个嵌套的结果。\n为了说明查询做了什么，考虑选择操作（WHERE 子句）。把嵌套记录想象成一个带标签的树，每个标签都对应着一个字段名。选择运算符会去掉不满足指定条件的树上的分支。因此，只有那些 Name.Url 被定义且以 http 开头的嵌套记录被保留下来。接下来考虑投影。SELECT 子句中的每个标量表达式都在与该表达式中使用最频繁的输入字段相同的层次上产生值。因此，字符串连接表达式会在输入模式中的 Name.Language.Code 层次上产生 Str 值。COUNT 表达式演示了记录内的聚合。聚合在每个 Name 子记录内进行，并为每个 Name 输出 Name.Language.Code 的出现次数，作为一个无符号 64 位整数（uint64）。\n该语言支持嵌套子查询、跨记录和内记录聚合、top-k、连接、用户定义函数等；实验部分举例说明了其中的一些功能。\n#6. 查询执行\n为了简单起见，我们在只读系统中讨论核心思想。 许多 Dremel 查询是一次性聚合； 因此，我们专注于解释这些，并在下一部分使用它们进行实验。 我们将在未来的工作中推迟连接、索引、更新等的讨论。\n\n树形架构。Dremel 使用多层服务树来执行查询（参见图 7）。\n\n根服务器接收传入的查询，从表中读取元数据，并将查询路由到服务树的下一层。\n叶节点服务器负责和存储层进行交互或访问本地磁盘的数据。\n\n考虑下面一个简单的聚合查询：\nSELECT A, COUNT(B) FROM T GROUP BY A\n当根服务器接收到上述查询时，它会确定由 T 组成的所有 tablet（即表的水平分区），并重写查询如下：\nSELECT A, SUM(c) FROM(R^1_1 UNION ALL... R^1_n) GROUP BY A\nT^1_i 是服务器 i 在第一级处理的 T 中的不相交分区。每个服务级别执行类似的重写。最后，查询到达叶节点，并行扫描 T 中的表。在向上返回的过程中，中间服务器并行聚合部分结果。上面描述的执行模型非常适合于聚合查询，这些查询返回小型或中型的结果集，它们是一类非常常见的交互式查询。大型聚合和其他类型的查询可能需要依赖于并行 DBMS 和 MR 已知的执行机制。\n查询调度程序。Dremel 是一个多用户系统，即通常会同时执行多个查询。查询调度程序根据其优先级对查询进行调度并平衡负载。它的重要作用是在一个服务器比其他服务器慢很多或某个表格外存副本无法访问时提供容错能力。\n每个查询处理的数据量通常都大于可用于执行的处理单元数，即我们所称的 slot。一个 slot 对应于叶服务器上的一个执行线程。例如，一个由 3,000 台叶服务器组成的系统，每台服务器使用 8 个线程，则有 24,000 个 slot。因此，对于一个包含 100,000 个 tablet 的表，每个 slot 需要处理 5 个 tablet。在查询运行期间，查询分发器计算 tablet 处理时间直方图。如果某个 tablet 处理时间过长，就会将其调度到另一个服务器上。有些 tablet 可能需要被重新调度多次。\n叶节点服务器以分块形式读取列式存储的嵌套数据。每个条带中的块异步预取；通常情况下，read-ahead 的缓存命中率为 95%。\ntablet 通常是三个副本。当一个叶节点无法访问一个平板副本时，它会 fallback 到另一个副本。\n查询调度程序遵守一个参数，该参数指定了在返回结果之前必须扫描的最小百分比。将此参数设置为较低值（例如 98％而不是 100％）通常可以显著加快执行速度，尤其是复制因子较小时。（译者注：因为 straggler 的存在）\n每个服务器都有一个内部执行树，如图 7 所示的右侧所示。内部树对应于物理查询执行计划，包括标量表达式的评估。为大多数标量函数生成了优化的、类型特定的代码。项目选择聚合查询的执行计划由一组迭代器组成，它们以步调扫描输入列，并发出带有正确重复级别和定义级别的聚合和标量函数的结果，在查询执行过程中完全跳过记录组装。有关详细信息，请参阅附录 D。\nDremel 的一些查询，如 top-k 和 count-distinct，使用已知的 one-pass 算法（例如[4]）返回近似结果。\n#7. 实验\n略\n#参考资料\n\nzhjwpku 论文阅读笔记\nApache Parquet 的实现参考了 Dremel 的数据模型，Apache Drill 的实现参考了 Dremel 的查询模型。\n\n","categories":["database-system"],"tags":["分布式系统","数据库","Dremel"]},{"title":"[SOSP'07] Dynamo: Amazon’s Highly Available Key-value Store 论文阅读","url":"/2025/04/13/dynamo-amazons-highly-available-key-value-store/","content":"\n发表于 SOSP 2007\n2017 年获得 SOSP Hall of Fame Award\nNoSQL 领域的里程碑之作, 启发了后续众多系统的设计催生了 AWS DynamoDB 服务\n\nDynamo 是 Amazon 在 2007 年发表的大规模分布式 K-V 存储系统.\n作为电商领域的巨头, Amazon 较早面临巨大业务规模带来的技术挑战.\nDynamo 的技术方案在当时的分布式系统中是非常前沿的, 其设计思想和实现细节对后来的 NoSQL 系统产生了深远的影响.\n#1. 引言\nAmazon 运行一个全球范围的电子商务平台, 在高峰时段为数百万客户提供服务, 使用世界各地多个数据中心的数千台服务器.\n在这种环境下, Amazon 对性能、可靠性和效率有着严格的要求.\n可靠性是最重要的要求之一, 因为即使是最细微的停机, 也会产生巨大的经济损失, 并影响客户信任.\n此外, 为了支持平台的持续增长, 平台需要具备高度的可扩展性.\n在运营 Amazon 平台的过程中, 我们学到的一个教训是, 一个系统的可靠性和可扩展性取决于应用状态的管理方式.\nAmazon 使用了一个高度去中心化、松耦合、面向服务的架构来管理数百个服务.\n在这种环境下, 需要一种始终可用的数据存储技术.\n例如, 即使发生磁盘故障、网络路由抖动或者甚至数据中心被龙卷风摧毁, 客户也应该能够查看并添加商品到他们的购物车中.\n因此, 负责管理购物车的服务必须确保它总是可以写入和读取数据存储, 并且其数据需要再多个数据中心之间可用.\n处理由数百万个组件组成的基础设施中的故障是我们的标准操作模式；在任意给定的时刻, 总有一些服务器和网络组件处于故障状态.\n因此, 亚马逊的软件系统需要以这样的方式构建：将处理故障视为正常情况, 而不会影响可用性或性能.\n\nThe major difference between a thing that might go wrong\nand a thing that cannot possibly go wrong is that when a thing\nthat cannot possibly go wrong goes wrong it usually turns out to\nbe impossible to get at or repair.\n可能出错的事物 与 不可能出错的事物 之间的主要区别是, 当一个不可能出错的事物出错时, 通常不可能修复.\nDouglas Adams, Mostly Harmless（1992）\n\n为了满足这些要求, Amazon 开发了一系列存储技术.\n其中最著名的可能是 Amazon S3, AWS 对外也提供该服务.\n本文介绍了 Dynamo – 另一个高可用且可扩展的分布式数据存储的设计和实现, 用于构建 Amazon 平台.\nDynamo 用于管理对可靠性(reliability)要求非常高且需要严格控制可用性(availability)、一致性(consistency)、成本效益(cost-effectiveness)和性能之间权衡的服务状态.\nAmazon 平台上有很多不同的应用, 它们对存储的要求各不相同.\n一组选定的应用需要一种足够灵活的存储技术, 以便应用开发人员可以根据这些权衡, 适当地配置他们的数据存储方式, 以最经济的方式实现高可用和有保证的性能.\nAmazon 平台上有许多服务只需要对数据存储进行主键访问, 例如畅销书列表、购物车、客户偏好、会话管理、销售排名和产品目录的服务.\n对于很多服务来说, 使用关系数据库的常见模式效率很低, 并且限制了规模和可用性.\nDynamo 提供了一个简单的仅使用主键的接口来满足这些应用的需求.\nDynamo 综合运用了一系列众所周知的技术来实现可扩展性和可用性:\n\n数据通过一致性哈希进行分片和复制, 通过对象版本控制来保持一致性.\n更新过程中, 副本之间的一致性通过类似 quorum 的方式和去中心化的副本同步协议来维护.\nDynamo 采用基于 Gossip 的分布式故障检测和成员协议.\n\nDynamo 是完全去中心化的, 对手动运维的需求极低. 存储节点可以在无需任何手动分片或再分配的情况下添加或删除.\n在过去一年中, Dynamo 已经是 Amazon 电商平台中多个核心服务的底层存储技术.\n它在繁忙的假日购物季期间能够高效应对极端峰值负载, 并且从未停机.\n例如, 购物车服务处理了数百万个请求, 一天内处理了超过 300 万次结账；会话管理服务处理数十万个并发的会话请求.\n本文对学术界的主要贡献在于：评估了如何将不同的技术组合在一起以提供一个高可用的系统.\n它证明了最终一致性的存储系统可以用于生产系统, 服务严格的应用程序.\n此外, 它还提供了深入的insight, 关于如何调优这些技术以满足生产系统的严格性能需求.\n#2. 背景\nAmazon 的电商平台由数百个协同工作的服务组成, 提供从推荐、订单履行到欺诈检测等的各种功能.\n每个服务都通过定义明确的 API 公开, 并可以通过网络进行访问.\n这些服务托管在遍布世界各地的数据中心中的数万台服务器组成的基础设施上.\n其中一些服务是无状态的 (即聚合其他服务响应的服务), 另一些服务则是有状态的 (对持久存储上的状态执行业务逻辑).\n传统生产系统将状态存储在关系数据库中.\n然而, 对于许多常见的使用模式而言, 关系数据库并不理想.\n这些服务大多数仅通过主键存储和检索数据, 并不需要 RDBMS 提供的复杂查询和管理功能.\n这种额外的功能需要昂贵的硬件和高技能的人员来操作, 使之成为一个非常低效的解决方案.\n此外, 可用的复制技术有限, 并且通常更看重一致性(Consistency)而不是可用性(Availability).\n尽管近年来有一些进步, 但拓展数据库或者使用智能分片方案进行 load balancing 仍然很困难.\n本文介绍了 Dynamo, 这是一种高可用的数据存储技术, 可以满足这些重要业务的要求.\nDynamo 具有简单的 K/V 接口, 并且具有清晰定义的一致性窗口, 高资源利用率, 简单的横向扩展方案.\n每个使用 Dynamo 的服务都运行自己的 Dynamo 实例. (即服务化部署)\n#2.1 系统假设和需求\n这类服务的存储系统有如下的需求:\n\n查询模型: 对有唯一键标识的数据项进行简单的读和写操作. 状态表示为二进制对象.\n操作不会跨多个数据项, 且不需要关系型模式. 对象相对较小(通常 &lt; 1MB).\nACID 属性: Amazon 的经验表明, 提供 ACID 保证的数据存储往往可用性较差, 这一点已经被业界和学术界广泛认可.\nDynamo 的目标是那些 A&gt;C 的应用. Dynamo 不提供任何隔离性(I)保证, 并且仅允许单值更新.\n效率: 系统需要在商用硬件上运行.\n在 Amazon 平台上, 服务对延迟有着严格的要求, 通常是以 p999 为准.\n鉴于状态访问在服务操作中起着至关重要的作用, 存储系统必须能够满足这些严格的 SLA.\n服务必须能够配置 Dynamo 的行为, 以便始终实现其延迟和吞吐量要求.\n在性能、性价比、可用性和持久性之间进行权衡.\n其他: Dynamo 仅供 Amazon 内部服务使用.\n其运行环境是可信的, 没有身份验证和授权相关的安全要求.\nDynamo 实例的初始设计目标是达到数百台服务器的规模.\n\n#2.2 SLA\n为了确保应用能够在限定时间内完成其功能, 平台中的每个依赖关系都需要提供严格的时间边界.\n客户端和服务端之间会约定一个SLA, 对几个系统相关指标达成共识, 其中最突出的是客户对特定 API 的预期请求速率分布以及在这些条件下预期的服务延迟.\n一个简单的 SLA 的示例是, 一个服务承诺: 能够在峰值客户端负载为 500 RPS 的情况下, 保证 99.9% 的请求响应时间不超过 300 ms.\n在 Amazon 去中心化的面向服务基础设施中, SLA 起着重要的作用.\n例如, 对一个电商网站的页面请求通常需要渲染引擎向超过 150 个服务发送请求.\n这些服务往往有多个依赖关系, 这些依赖项通常是其他服务, 因此应用的调用图中经常具有多个层级.\n为了保证页面渲染引擎能够清晰地控制界限页面交付, 调用链中的每个服务都必须遵循其 SLA.\n图 1 展示了 Amazon 平台架构的抽象视图, 其中动态 Web 内容由页面渲染组件生成, 而这些组件又会查询许多其他服务. 服务可以使用不同的数据存储来管理其状态, 并且这些数据存储只能在其服务边界内访问. 一些服务充当聚合器, 通过使用多个其他服务来生成复合响应. 通常, 聚合器服务是无状态的, 但它们会使用大量缓存.\n在行业中, 以性能为导向的 SLA 通常使用平均值、中位数和预期方差来描述.\n在 Amazon 中我们发现, 如果最终目标是要建立一个让所有（而不是大部分）客户都有良好体验的系统, 则这些指标还不够好.\n例如, 如果使用大量的个性化技术, 则具有较长历史记录的客户则需要更多的处理, 这会影响分布高端的性能.\n用平均值或者中位数的 SLA 无法解决这个问题.\n为了解决这个问题, Amazon 的 SLA 使用 p999 分位数.\n选择 p999 而不是更高的百分位数是基于成本效益分析得出的, 要继续提高性能, 成本会显著增加.\n亚马逊的经验证明, 这种 SLA 方案提供了更好的整体体验.\n本文多次提及 99.9 百分位分布, 这体现了亚马逊工程师从客户体验角度对性能的不懈追求. 许多论文报告的是平均值, 因此本文也仅将其纳入, 以便进行比较. 然而, 亚马逊的工程和优化工作并非专注于平均值. 一些技术, 例如负载均衡的写入 Coordinator 选择, 纯粹是为了将性能控制在 99.9 百分位.\n存储系统通常在确定服务的 SLA 中起着重要的作用, 尤其是在业务逻辑相对轻量级的情况下, 许多 Amazon 的服务就是这样.\n在这种情况下, 状态管理就成为服务 SLA 的主要组成部分之一.\nDynamo 的主要设计考虑之一是为服务提供对其系统属性(如持久性和一致性)的控制能力, 并让服务自行权衡功能、性能和成本效益之间的取舍.\n#2.3 设计考虑因素\n商业系统中使用的数据复制算法通常执行同步的副本调和, 以提供强一致的数据访问接口.\n为了实现这种级别的一致性, 在某些故障场景下, 不得不牺牲数据的可用性.\n例如, 相对于处理一个不确定是否正确(correct)的结果, 不如先将数据设置为不可用, 直到能够确定它是正确的为止.\n从早期的复制数据库工作开始, 就已知当处理网络故障的可能性时, 强一致性与高可用性不能同时实现.\n因此, 此类系统和应用需要了解在哪些条件下可以实现哪些属性.\n对于容易发生服务器和网络故障的系统, 可以通过使用乐观复制技术来提高可用性, 这种技术允许在后台将更改传播到副本, 并且容忍并发、断开连接的工作.\n这种方法的挑战在于它可能导致修改冲突, 必须检测并解决这些更改.\n这种冲突解决过程引入了两个问题: 何时解决冲突(when)以及谁来解决冲突(who).\nDynamo 设计为最终一致的数据存储：所有更新最终都会到达所有副本.\n第一个重要的设计决策是决定何时解决冲突, 即是否要在读或写操作时解决冲突.\n许多传统数据存储选择在写入期间执行冲突解决, 并保持读取操作简单, 因此在这种系统中, 如果写入操作在给定时间内无法到达多数副本, 则会被拒绝.\n\n即, 当发生网络分区时放弃了写可用性\n\n另一方面, Dynamo 的设计目标为持续可写的数据存储 (即数据存储对写入具有高度可用性).\n对于亚马逊的某些服务而言, 拒绝客户更新可能会导致较差的客户体验. 例如像购物车服务必须允许客户在其购物车中添加和删除商品, 即使在网络故障和服务器故障期间也是如此.\n这一要求迫使我们把冲突解决的复杂性推到读取过程中, 以确保写入永远不会被拒绝.\n下一个设计决策是谁来解决冲突. 这可以由数据存储或应用完成.\n如果由数据存储进行冲突解决, 其选择就比较有限. 在这种情况下, 数据存储只能使用简单的策略来解决冲突更新, 例如最后写入者获胜 (LWW).\n另一方面, 由于应用程序感知数据模式, 它可以决定最合适的冲突解决方法.\n例如, 维护客户购物车的应用可以选择将冲突版本合并, 并返回一个统一的购物车.\n尽管具有这种灵活性, 一些应用开发人员可能不想编写自己的冲突解决机制, 并将其推送到数据存储中, 而数据存储则会选择简单的方法, 如 LWW.\n设计中包含的其他关键原则是:\n\n增量式可扩展性: Dynamo 应该能够一次扩展一台存储主机, 对系统运维和系统本身的影响最小.\n对称性: Dynamo 中的每个节点都应该具有与其对等节点相同的职责; 不应该有特殊的节点或节点承担特殊角色或额外的职责. 在我们的经验中, 对称性简化了系统的配置和维护.\n去中心化: 作为对称性的延伸, 设计应优先考虑去中心化的点对点技术, 而非集中式控制. 过去, 集中式控制曾导致过宕机, 而我们的目标是尽可能避免这种情况. 这导致了一个更简单、更具可扩展性和可用性更高的系统.\n异质性: 系统需要能够利用其运行的基础设施的异质性. 例如, 工作负载分配必须与单个服务器的能力成比例. 这在添加具有更高容量的新节点而无需一次升级所有主机时至关重要.\n\n#3. 相关工作\n#3.1 P2P 系统\n有几种对等 (P2P) 系统研究过数据存储和分发问题.\n第一代 P2P 系统, 如 Freenet 和 Gnutella, 主要用作文件共享系统.\n这些是非结构化 P2P 网络的例子, 在这些网络中, 对等节点之间的覆盖链接任意建立.\n在这些网络中, 通常通过网络发送搜索查询以找到尽可能多的共享数据的对等节点.\n第二代 P2P 系统即广为人知的结构化P2P网络.\n这些网络采用全局一致的协议来确保任何节点都可以高效地将搜索查询路由到具有所需数据的某个对等节点.\n像 Pastry 和 Chord 这样的系统使用路由机制来保证可以在有限的跳数内回答查询.\n为了减少多跳路由引入的额外延迟, 一些 P2P 系统采用了 O(1) 路由, 每个对等节点都维护足够的路由信息, 以便它可以在常数个跳数内将请求(数据项访问)路由到适当的对等节点.\n各种存储系统, 例如 Oceanstore 和 PAST, 都构建在这些路由覆盖层上.\nOceanstore 提供了一个全球、事务性、持久化存储服务, 支持对广泛复制数据进行串行更新.\n为了允许并发更新并避免广域锁定固有的许多问题, 它使用一种基于冲突解决的更新模型.\n[21]引入了冲突解决, 以减少事务回滚的次数.\nOceanstore 通过处理一系列更新, 在它们之间确定一个全序, 并然后按此顺序原子地应用它们, 来解决冲突.\n它专为在不可信基础设施上复制数据的环境而构建.\n相比之下, PAST 在 Pastry 之上提供了持久且不可变对象的简单抽象层.\n它假设应用可以在其之上构建必要的存储语义(如可变文件).\n#3.2 分布式文件系统和数据库\n文件系统和数据库系统社区已广泛研究如何通过分散数据来提高性能、可用​​性和持久性.\n与仅支持扁平命名空间的 P2P 存储系统相比, 分布式文件系统通常支持分层命名空间.\n\nFicus 和 Coda 等系统通过复制文件来实现高可用性, 但牺牲了一致性. 更新冲突通常使用专门的冲突解决程序进行管理.\nFarsite 系统 是一个分布式文件系统. 它不使用像 NFS 这样的中央服务器, 而是使用复制来实现高可用性和可扩展性.\nGoogle 文件系统 (GFS) 是另一个用于托管谷歌内部应用状态的分布式文件系统. GFS 采用简单的设计, 只有一个主服务器来托管整个元数据, 并将数据分成块并存储在块服务器上.\nBayou 是一个分布式关系数据库系统, 允许离线操作并提供最终的数据一致性.\n\n在这几个系统中, Bayou、Coda 和 Ficus 允许脱机操作, 并且对网络分区和故障等问题具有鲁棒性.\n这些系统的冲突解决程序各不相同:\n\nCoda 和 Ficus 执行系统级别的冲突解决\nBayou 则允许应用级别的解决\n\n然而, 它们都保证了最终一致性.\n与这些系统类似, Dynamo 即使在网络分区期间也允许读写操作继续进行, 并使用不同的冲突解决机制来解决更新后的冲突.\n像 FAB 这样的分布式块存储系统将大对象拆分成较小的块, 并以高可用的方式存储每个块.\n与其他系统相比, 在这种情况下, 键值存储更合适, 因为:\n\n键值存储旨在存储相对较小的对象 (&lt;1M),\n键值存储更容易根据应用进行配置.\n\nAntiquity 是一个广域分布式 (wide-area) 的存储系统, 旨在处理多个服务器故障. 它使用安全日志 (secure log) 来保存数据完整性, 将每个日志复制到多台服务器以确保持久性, 并使用拜占庭容错协议来确保数据的一致性.\n相比之下, Dynamo 不关注数据完整性和安全性问题, 并且是为可信环境构建的.\nBigtable 是一种用于管理结构化数据的分布式存储系统. 它维护一个稀疏的多维有序映射, 并允许应用通过多个属性访问其数据.\n相比之下, Dynamo 的目标是仅需要键/值访问的应用, 并且主要关注高可用性, 即使在网络分区或服务器故障的情况下也不会拒绝更新.\n传统的复制关系型数据库系统专注于保证复制数据的强一致性问题. 尽管强一致性为应用编写者提供了方便的编程模型, 但这些系统在可扩展性和可用性方面受到限制. 由于它们通常提供强一致性的保证, 因此这些系统无法处理网络分区问题.\n#3.3 讨论\nDynamo 与上述去中心化存储系统在目标要求方面有所不同.\n\nDynamo 主要针对需要持续可写的数据存储应用, 这些应用不允许由于故障或并发写入而拒绝更新. 这对于许多 Amazon 应用至关重要.\nDynamo 是为单个管理域内的基础设施构建的, 在该基础设施中所有节点都被假设为可信的.\n使用 Dynamo 的应用不需要支持层次化的命名空间 (许多文件系统的常态) 或复杂的关系模式 (传统数据库支持).\nDynamo 专为延迟敏感型应用程序设计, 要求读取和写入操作的延迟 p999 必须在几百毫秒内.\n为了满足这些严格的延迟要求, 我们避免通过多个节点路由请求 (这是几个分布式哈希表系统, 例如 Chord 和 Pastry 所采用的典型设计), 这是因为多跳路由会增加响应时间的变异性, 并且在更高百分位数时增加延迟.\nDynamo 可以被描述为零跳 DHT, 其中每个节点在其本地维护足够的路由信息以直接将请求路由到适当的节点.\n\n#4. 系统架构\n需要在生产环境中运行的存储系统架构非常复杂.\n除了实际的数据持久化组件外, 该系统还需要具有可扩展且稳健的解决方案, 解决这些问题:\n\n负载均衡\n成员发现和故障检测\n故障恢复\n复制同步\n过载处理\n状态转移\n并发性和作业调度\n请求编组\n请求路由\n系统监控和报警\n配置管理\n\n本文重点介绍了 Dynamo 中使用的分布式系统技术的核心: 分片、复制、版本控制、成员资格、故障处理和扩缩容.\n\n\n\n问题\n采用的解决方案技术\n优势\nApache Cassandra\nRiak\n\n\n\n\n数据分片 (partition)\n一致性哈希\n增量式可扩展性\n√\n√\n\n\n持续可写 (写高可用)\n向量时钟\n版本大小与更新速率解耦\nLWW\n√\n\n\n临时故障处理\n宽松 Quorum 和提示 handoff\n当某些副本不可用时提供高可用性和持久性保证\n√\n√\n\n\n从永久故障中恢复\n使用 Merkle 树进行反熵\n在后台同步分歧的副本\n√\n√\n\n\n成员发现和故障检测\n基于 Gossip 的成员协议和故障检测\n保持对称性并避免集中式注册中心来存储成员和节点存活信息\n√\n√\n\n\n\n#4.1 接口\nDynamo 通过简单的接口存储与键关联的对象；它暴露了两个操作: get 和 put.\n\nget(key) 操作在存储系统中定位与键相关的对象副本, 并返回单个对象, 或一个冲突版本的对象列表以及 context .\nput(key, context, object) 操作根据关联的键确定对象的副本应放置的位置, 并将副本写入磁盘.\n\ncontext 对对象的系统元数据进行编码, 该元数据对调用者是不透明的, 并包括诸如对象版本的信息.\ncontext 信息与对象一起存储, 以便系统可以验证提供的 context 对象的有效性.\nDynamo 将由调用者提供的键和对象都视为不透明的字节数组. 它对键应用 MD5 散列以生成 128 位标识符, 该标识符用于确定负责服务 Key 的存储节点.\n#4.2 数据分片\n\nDynamo 的关键设计要求之一是它必须按增量式方式扩展. 这需要一种机制来动态地将数据分布在系统中的节点上.\nDynamo 的分片方案依赖于一致性哈希来在多个存储主机之间分配负载.\n在一致性哈希中, 哈希函数输出范围被处理为固定圆环空间或“环”(即最大哈希值会循环到最小哈希值).\n每个系统中的节点都被分配一个随机值在这个空间内, 代表其“位置”在环上.\n每个由键标识的数据项通过将数据项的键进行哈希以获得其在环上的位置, 并然后顺时针沿着环找到第一个位置大于该数据项的位置的第一个节点而分配给一个节点.\n因此, 每个节点都负责环上其前一个节点和它之间的区域.\n一致性哈希的优点是, 节点的离开或到达仅影响其直接邻居, 而其他节点不受影响.\n\n传统的做法：使用 mod 函数将数据项映射到节点上.\n这种方法的问题在于, 当节点数量发生变化时, 大量数据项需要重新映射到不同的节点上.\n这会导致大量的数据迁移, 并且在节点频繁变化的环境中效率低下.\n\n基本的一致性哈希算法存在一些挑战:\n\n环上的每个节点的随机位置分配导致数据和负载分布不均匀.\n基本算法忽略了节点的异质性.\n\n为了应对这些问题, Dynamo 使用了一种一致性哈希变体: 相对于将一个节点映射到圆圈中的单个点上, 每个节点被分配到多个环上的点上.\n为此, Dynamo 使用了“虚拟节点”的概念. 一个虚拟节点看起来像系统中的一个节点, 但每个节点可以负责多个虚拟节点. 简而言之, 在系统中添加一个新的节点时, 它会被分配到环上的多个位置(以后称为“令牌”).\nDynamo 分区方案的微调过程将在第 6 节讨论.\n使用虚拟节点具有以下优点:\n\n如果一个节点变得不可用 (由于故障或例行维护), 由该节点处理的负载将均匀地分散到剩余可用节点上.\n当一个节点再次可用, 或系统中添加了一个新节点时, 新的可用节点将从其他可用节点接收大致相等的负载量.\n节点负责的虚拟节点数量可以根据其容量来决定, 以考虑物理基础设施的异质性.\n\n\n一致性哈希的计算开销: 一次哈希计算和一次环上查找 (使用跳表或类似的数据结构实现), O(log N).\n\n#4.3 数据复制\n为了实现高可用性和持久性, Dynamo 在多个主机上复制其数据.\n每个数据项都会存储在 N 个不同的主机上, N 是一个实例级别的配置参数.\n每个键都对应一个 Coordinator 节点.\nCoordinator 节点负责复制数据项到它应该属于的范围. 除了本地存储每个范围内的键外, Coordinator 节点还会将这些键复制到环形中的顺时针方向上的 N-1 个节点.\n最终系统中, 每个节点对其与第 N 个前驱之间的环形区域负责.\n例如在图 2 中, 位于 (A, B] 之间的键 k 将被存储在节点 B、C 和 D 上. 节点 D 将存储属于范围 (A, B]、(B, C] 和 (C, D] 内的键.\n\nCoordinator 节点: 哈希环上顺时针方向上第一个位置大于 Key 的节点.\n\n负责存储特定 Key 的节点列表称为该 Key 的 偏好列表 (Preference list).\n系统被设计为每个节点都可以确定任何特定 Key 的 Preference list.\n为了应对节点故障, Preference list 包含超过 N 个节点.\n注: 在使用了虚拟节点的情况下, Preference list 可能包含重复的物理节点. 在这种情况下, 会跳过环上的位置以确保列表仅包含不同的物理节点.\n\nPreference list: 哈希环上后继 &gt;N 个不同的物理节点.\n\n#4.4 多版本控制\nDynamo 提供最终一致性, 更新操作会异步地传播到所有副本.\n一个 put 调用可能在所有副本应用更新之前就返回给调用者, 这可能导致后续的 get 操作可能返回一个旧的对象.\n如果没有发生故障, 更新在有限时间内即可完成. 然而, 在某些故障场景下(例如服务器停机或网络分区), 更新可能需要较长时间才能到达所有副本.\nAmazon 平台中有些应用程序可以容忍这种不一致, 并且可以在这些条件下运行.\n例如, 购物车应用要求“添加到购物车”的操作永远不会被遗忘或拒绝.\n如果最新的购物车状态不可用, 用户对老版本的购物车状态进行了更改, 则该更改仍然是有意义的并且应该保留.\n但同时, 它不应该取代当前不可用的购物车状态, 而该状态本身可能包含应保留的变化.\n注: “添加到购物车”和“从购物车删除商品”操作都被翻译成 Dynamo 中的 put 请求. 当客户想要将增加/删除购物车中的商品, 而且最新版本不可用时, 该变更会被施加到较旧版本中, 并且不同的版本稍后会进行合并.\n为了提供这种保证, Dynamo 将每个修改的结果视为一个新的、不可变的数据版本.\n它允许系统中同时存在一个对象的多个版本.\n大多数情况下, 新版本会覆盖之前的版本(语法调和 syntactic reconciliation), 并且系统本身可以确定权威版本(语义调和 semantic reconciliation).\n然而, 在存在故障和并发更新的情况下可能会发生版本分叉, 从而导致对象出现冲突版本.\n\n参考共享文档编辑系统中的冲突情况.\n\n在这种情况下, 存储系统无法对同一对象的多个版本进行一致化处理, 客户端必须执行一致化操作以将数据演进的多个分叉合并回一个 (语义调和 semantic reconciliation).\n典型的合并操作示例是“合并”不同版本的客户购物车.\n使用这种调和机制, “添加到购物车”的操作永远不会丢失. 但是, 被删除的商品可能会重新出现.\n\n对于购物车场景, 这通常是可以接受的, 因为客户可以在结账时删除不需要的商品.\n也符合电商网站的商业目标, 即尽可能多地销售商品.\n\n重要的是要理解某些故障模式可能会导致系统不仅有两份, 而是多份相同的数据.\n在存在网络分区和节点故障的情况下进行更新可能导致对象具有不同的版本子历史记录, 而该系统将来需要解决这个问题.\n这要求我们设计应用来明确承认同一数据的多个版本的可能性 (以确保不会丢失任何更新).\nDynamo 使用向量时钟来捕获同一对象的不同版本之间的因果关系.\n一个向量时钟实际上是一个 (节点, 计数器) 对的列表. 每个对象的所有版本都关联一个向量时钟.\n通过检查向量时钟, 可以确定两个对象的版本是否在并行分支上还是具有因果顺序.\n如果第一个对象的时钟上的计数器小于等于第二个时钟中的所有节点, 则第一个是第二个的祖先, 并且可以被遗忘.\n否则, 这两个更改被认为是冲突并且需要进行调和.\n在 Dynamo 中, 当客户端希望更新对象时, 它必须指定要更新的版本.\n这是通过传递先前读取操作获得的 context 来完成的, context 中包含向量时钟信息.\n处理读请求时, 如果 Dynamo 访问了多个无法语法上调和的分支, 它将返回所有叶子对象, 并在 context 中包含相应的版本信息.\n使用此 context 进行更新被认为是已调和了不同的版本, 并且分支会合并为一个新版本.\n\n为了说明向量时钟的使用, 让我们考虑上图所示的例子. 客户端写入新对象.\n处理此 Key 的节点(假设是 Sx)增加其序列号, 并将其用于创建数据的向量时钟.\n现在系统拥有对象 D1 及其关联的时钟[(Sx, 1)]. 客户端更新该对象.\n假设同一节点也处理了这个请求. 现在系统还拥有对象 D2 及其关联的时钟[(Sx, 2)].\nD2 从 D1 继承而来, 因此会覆盖 D1, 但是可能在尚未看到 D2 的节点上仍然存在 D1 的副本.\n让我们假设相同的客户端再次更新该对象并由不同的服务器(假设是 Sy)处理请求.\n现在系统拥有数据 D3 及其关联的时钟[(Sx, 2), (Sy, 1)].\n接下来假设一个不同的客户端读取 D2, 然后尝试更新它, 并且另一个节点(假设为 Sz)进行写入.\n现在系统中有 D4(D2 的后代), 其版本时钟为[(Sx, 2), (Sz, 1)].\n一个知道 D1 或 D2 的节点, 在收到 D4 及其时钟后, 可以确定 D1 和 D2 被新数据覆盖并可以进行垃圾回收.\n一个知道 D3 并且接收 D4 的节点会发现它们之间没有因果关系.\n换句话说, D3 和 D4 中的变化在彼此之间没有反映出来.\n这两个版本的数据必须保持并呈现给客户端(在读取时)以实现语义上的调和.\n现在假设一些客户端读取了 D3 和 D4 (context 将反映这两个值都是通过读取找到的).\n读取的 context 是 D3 和 D4 时钟的摘要, 即[(Sx, 2), (Sy, 1), (Sz, 1)].\n如果客户端执行调和并节点 Sx 调和写入, 则 Sx 会更新其时钟中的序列号.\n新数据 D5 将具有以下时钟: [(Sx,3), (Sy, 1), (Sz, 1)].\n向量时钟的一个可能的问题是, 如果许多服务器调和对同一个对象的写入, 则向量时钟的大小可能会增长.\n在实践中, 这不太可能, 因为写操作通常由 Preference list 中前 N 个节点中的一个处理.\n在网络分区或多个服务器故障的情况下, 写请求可能会由不在 Preference list 中前 N 个节点的节点处理, 导致向量时钟的大小增长.\n在这种情况下, 限制向量时钟的大小是可取的.\n为此, Dynamo 采用了以下截断方案: 与每个 (node, counter) 对一起存储一个时间戳, 指示节点最后一次更新数据项的时间.\n当向量时钟中的 (node, counter) 对的数量达到阈值(例如 10)时, 最旧的一对将从时钟中删除.\n显然, 这种截断方案会导致合并过程中的效率低下, 因为后代关系无法准确推导出来.\n然而, 在生产环境中尚未出现过这个问题, 因此还没有得到彻底调查.\n#4.5 执行 get 和 put 操作 (无故障)\nDynamo 中的任何存储节点都可接收客户端的 get 和 put 操作.\n在本节中, 为了简单起见, 我们先描述这些操作在无故障的情况下如何执行, 在下一节中, 我们将描述在发生故障期间如何执行读写操作.\nget 和 put 都是通过使用 Amazon 的 HTTP RPC 框架进行调用.\n客户端可以使用两种策略来访问 Dynamo:\n\nProxy: 将其请求路由到一个通用 Load Balancer, 该 Load Balancer 将根据负载信息选择节点\n\n优点: 客户端不需要在应用中链接任何与 Dynamo 相关的代码\n\n\nSmart Client: 使用感知分区的客户端库直接将请求路由到适当的 Coordinator 节点.\n\n优点: 可以实现更低的延迟, 因为它跳过了可能的转发步骤\n\n\n\n处理读写操作的节点被称为 Coordinator, 通常是 Preference list 中的第一个成员.\n在 Proxy 模式下, Proxy 会将请求随机转发给集群中的任意一个节点, 该节点再将请求转发给真正的 Coordinator 节点.\n读写操作只会涉及 Preference list 中的前 N 个健康节点, 跳过那些已下线或不可访问的节点.\n当所有节点都处于健康状态时, 会访问一个 Key 的 Preference list 中排名最高的 N 个节点.\n在出现节点故障或网络分区的情况下, 会访问 Preference list 中排名较低的节点.\n为了保持其副本的一致性, Dynamo 使用类似于 Quorum 系统的一致性协议.\n该协议有两个关键可配置值: R 和 W.\n\nR 是成功读取操作必须参与的最小节点数\nW 是成功写入操作必须参与的最小节点数\n\n将 R 和 W 设置为 R + W &gt; N 就得到一个类似 Quorum 的系统.\n在这个模型中, get/put 操作的延迟将由 R/W 中的最慢副本决定.\n因此, 通常会选择小于 N 的 R + W, 以提供更好的延迟.\n在收到对 Key 的 put 请求时, Coordinator 生成新版本的向量时钟, 并将新版本写入本地.\n然后, Coordinator 将新版本(包括新向量时钟)发送到排名最高的 N 个可达节点.\n如果至少有 W-1 个节点响应, 则这次写入操作被认为是成功的.\n同样, 对于一个 get 请求, Coordinator 从 Preference list 中排名最高的 N 个可达节点请求该 Key 的所有现有版本的数据, 并且在等待 R 个响应后返回结果给客户端.\n如果 Coordinator 最终收集了多个数据版本, 则它会返回所有认为与当前版本无关的版本.\n然后将这些不同版本进行调和, 并将调和后的版本写回.\n#4.6 处理临时故障: 提示式移交\n如果 Dynamo 使用传统的 Quorum 方法, 那么在服务器故障和网络分区期间将完全不可用, 并且即使在最简单的故障条件下也会降低持久性.\n为了解决这个问题, 它不强制执行严格的 Quorum 检查, 而是使用一种宽松的 Quorum:\n所有读写操作都在 Preference list 中的前 N 个健康节点上进行, 这些节点不一定是哈希环中顺时针的前 N 个节点.\n考虑图 2 中 N = 3 的 Dynamo 配置示例.\n在此示例中, 如果在写操作期间节点 A 暂时不可用或无法访问, 则原本在 A 上的数据的副本将被发送到节点 D.\n这是为了保持所需的可用性和持久性保证而进行的操作.\n发送给 D 的副本将在其元数据中包含一个提示, 表明哪个节点才是该副本的预期接收者 (在这种情况下为 A).\n收到提示副本的节点会将其保存在一个单独的本地数据库中, 并定期扫描该数据库. 检测到 A 已恢复后, D 会将尝试向 A 交付副本.\n一旦传输成功, D 可以从其本地存储中删除对象, 而不必减少系统中的总副本数量.\nDynamo 使用提示式移交机制确保读写操作不会因临时节点或网络故障而失败.\n需要最高可用性的应用可以将 W 设置为 1, 这保证只要系统中有一个节点 Key 写入其本地存储, 则写请求就会被接受. 只有所有节点都不可用, 才会拒绝写入请求.\n然而, 在实践中, 大多数亚马逊服务在生产环境中设置了更高的 W 以满足所需的持久性级别.\n有关配置 N、R 和 W 的更详细讨论请参见第 6 节.\n高可用存储系统必须能够处理整个数据中心的故障.\n数据中心故障可能由于停电、冷却失败、网络故障或自然灾害引起.\nDynamo 通过配置, 使得每个对象在多个数据中心进行复制.\n本质上, Key 的 Preference list 被构造成以使存储节点分布在多个数据中心中.\n这些数据中心通过高速网络连接.\n这种跨多个数据中心复制的方法允许我们处理整个数据中心的故障而不会出现数据中断.\n#4.7 处理永久性故障: 副本同步\n提示式移交在系统成员流失率低且节点故障是暂时的情况下效果最好.\n有些场景下, 提示副本可能在移交给原始节点之前也不可用了.\n为了处理这种情况对持久性的威胁, Dynamo 实现了一个反熵 (副本同步) 协议来保持副本同步.\n为了更快地检测副本之间的不一致性和最小化传输的数据量, Dynamo 使用 Merkle 树.\nMerkle 树是一种哈希树, 其中叶子是单个键值的哈希. 树中更高层次的父节点是其子节点的哈希.\nMerkle 树的主要优点在于每个分支可以独立检查而无需节点下载整个树或整个数据集.\n此外, Merkle 树有助于减少在检查副本之间的一致性时需要传输的数据量. 例如, 如果两个树根的哈希值相等, 则树中的叶节点的值也相等, 并且节点不需要同步. 否则, 这意味着某些副本的值不同.\n在这种情况下, 节点可能交换孩子的哈希值并继续进行直到达到树的叶子, 在这一点上主机可以识别“脱节”的 Key.\nMerkle 树减少了用于同步所需传输的数据量, 并降低了反熵过程中读取磁盘的数量.\nDynamo 使用 Merkle 树来反熵, 如下所示: 每个节点维护一个单独的 Merkle 树, 以覆盖它所托管的关键范围(虚拟节点所涵盖的一组 Key). 这允许节点比较它们是否在关键范围内具有最新的 Key.\n在这个方案中, 两个节点交换它们共同拥有的 Key 范围对应的 Merkle 树根. 随后, 通过上面描述的树遍历方案, 节点确定是否有任何差异并执行适当的同步操作.\n这种方案的一个缺点是, 在节点加入或离开系统时, 许多 Key 范围会发生变化, 因此需要重新计算树.\n然而, 这个问题可以通过第 6.2 节中描述的细化分区方案得到解决.\n#4.8 成员和故障检测\n#4.8.1 环形成员检测\n在 Amazon 的环境中, 由于故障和维护任务而导致的节点掉线通常是短暂的, 但也可能会持续更长的时间.\n节点掉线很少意味着永久离开, 因此不应该分区分配重新平衡或无法访问副本的修复.\n同样, 人为错误可能导致无意中启动新的 Dynamo 节点.\n因此, 使用明确机制来增加和删除 Dynamo 环中的节点是合适的.\n管理员通过命令行工具或浏览器连接到 Dynamo 节点, 并向该节点发出成员资格更改指令.\n请求服务的节点将成员资格更改及其发布时间写入持久存储.\n因为节点可以多次被添加和删除, 成员资格更改形成历史记录.\n基于 Gossip 的协议会传播成员资格更改并保持最终一致的成员资格视图.\n每个节点每秒会随机选择一个 Peer 进行联系, 两个节点高效地调和它们的持久成员资格更改历史记录.\n当 Dynamo 节点首次启动时, 它先选择其令牌集(一致哈希空间中的虚拟节点)并映射节点到各自的令牌集中.\n该映射会持久化在磁盘上, 最初仅包含本地节点和令牌集.\n在相同通信交换中, 不同 Dynamo 节点存储的映射会进行合并以解决成员变更历史记录.\n因此, 分片和放置信息也会通过基于 Gossip 协议传播, 并且每个存储节点都了解其 Peer 处理的令牌范围.\n这使每个节点能够直接将 Key 的读写操作转发到正确的节点集合.\n#4.8.2 外部服务发现\n上述机制可能会暂时导致逻辑上分区的 Dynamo 环.\n例如, 管理员可以联系节点 A 以将 A 加入到环中, 然后联系节点 B 以将 B 加入到环中.\n在这种情况下, 节点 A 和 B 都会认为自己是环的一部分, 但它们都不会立即意识到对方的存在.\n为了防止逻辑分区, 一些 Dynamo 节点扮演种子的角色.\n种子是通过外部机制发现的节点, 并且所有节点都知道这些种子.\n因为最终所有节点都将与种子进行成员身份匹配, 因此逻辑分区的可能性极低.\n种子可以从静态配置或配置服务中获取.\n通常, 种子节点是 Dynamo 环中的完全正常工作的节点.\n#4.8.3 故障检测\n在 Dynamo 中故障检测用于避免在 get 、put、分片迁移和提示副本传输期间尝试与不可达的对等方进行通信.\n为了防止通信失败, 使用纯粹本地的故障检测概念完全足够: 节点 A 可能认为节点 B 已失败, 即使 B 对节点 C 的消息响应.\n当 Dynamo 环中有稳定的客户端请求速率时, 在节点 B 失败且不响应消息的情况下, 节点 A 很快就会发现节点 B 是无响应的；然后节点 A 使用备用节点来服务映射到 B 的分片的请求；定期重试 B 以检查其恢复情况.\n在没有客户端请求驱动两个节点之间的流量的情况下, 任何节点都不需要知道另一个是否可达和响应.\n去中心化故障检测协议使用一种简单的 Gossip 式协议, 使系统中的每个节点都能了解到其他节点的到达(或离开).\n有关去中心化故障检测器及其影响准确性的参数的详细信息, 请参阅[8].\nDynamo 的早期设计使用去中心化故障检测器来维护全局一致的失败状态视图.\n后来确定了明确的节点加入和离开方法消除了对全局失败状态视图的需求.\n这是因为通过明确的节点加入和离开方法通知节点永久性节点添加和删除, 并且当节点无法与其他节点通信时(在转发请求时), 由单个节点检测临时节点故障.\n#4.9 添加/删除存储节点\n当一个新的节点(例如 X)被添加到系统中时, 它会分配一些随机散落在环上的令牌. 对于分配给节点 X 的每个键范围, 可能有多个节点(小于或等于 N)目前负责处理其令牌范围内的键. 由于将键范围分配给 X, 一些现有节点不再需要某些键, 并且这些节点将这些键转移到 X.\n让我们考虑一个简单的 Bootstrap 场景, 其中节点 X 添加到图 2 中所示的环之间 A 和 B.\n当 X 添加到系统时, 它负责存储范围 (F, G], (G, A] 和 (A, X] 的键.\n因此, 节点 B、C 和 D 不再需要存储这些相应的范围中的键.\n因此, 节点 B、C 和 D 将向 X 提供相应的键值, 并在 X 确认后转移相应的键值集.\n当一个节点从系统中移除时, 键值的重新分配过程将以相反的过程进行.\n运维经验表明, 这种方法可以将 Key 分发的负载均匀地分布在存储节点上, 这对于满足延迟要求和确保快速启动至关重要. 最后, 在源和目的地之间添加确认轮次, 以确保目的地节点不会接收给定 Key 范围内的任何重复传输.\n#5. 实现\n在 Dynamo 中, 每个存储节点都有三个主要的软件组件: 请求协调、成员资格和故障检测、以及本地持久性引擎. 所有这些组件都是用 Java 实现的.\nDynamo 的本地持久性组件允许使用不同的存储引擎. 正在使用的引擎包括:\n\nBerkeley 数据库 (BDB) 事务数据存储\nBDB Java 版\nMySQL\n具有持久存储后端的内存缓冲区\n\n设计可插拔持久性组件的主要原因是选择最适合应用访问模式的存储引擎.\n例如, BDB 可以处理通常在数十 KB 范围内的对象, 而 MySQL 则可以处理更大的对象.\n应用根据其对象大小分布选择 Dynamo 的本地持久性引擎.\n大多数 Dynamo 生产实例都使用 BDB 事务数据存储.\n请求协调组件构建于事件驱动的消息传递基础之上, 该消息处理管道被划分为多个阶段类似于 SEDA 架构.\n所有通信都使用 Java NIO 通道实现.\nCoordinator 通过从一个或多个节点 (读取情况下) 收集数据, 或存储数据在一个或多个节点上 (写入情况下), 代表客户执行读取和写入请求.\n每个客户端请求都会导致接收客户端请求的节点创建状态机.\n状态机包含所有用于识别负责 Key 的节点、发送请求、等待响应、可能进行重试、处理答复并包装响应给客户的逻辑.\n每个状态机实例仅处理一个客户端请求. 例如, 读操作实现以下状态机:\n\n向节点发送读取请求\n等待所需响应的最小数量 R\n如果在给定时间内收到太少的回复, 则失败请求\n否则收集所有数据版本, 并确定要返回的版本\n如果启用版本控制, 则执行语法调和, 并生成包含所有剩余版本的不可见写 context .\n\n为了简洁起见, 错误处理和重试状态被省略了.\n在读响应已返回给调用者后, 状态机等待一小段时间以接收任何未完成的响应. 如果在任何响应中返回了过期版本, Coordinator 会更新这些节点的最新版本. 此过程称为读取修复, 因为它会在机会时间修复错过最近更新的副本, 并且可以减轻反熵协议不必执行该操作的压力.\n如前所述, 写请求由 Preference list 中前 N 个节点中的一个协调.\n尽管第一个节点协调写入操作从而在单个位置上串行所有写入操作是理想的, 但这种方法导致负载不均匀分布, 进而违反了 SLA.\n这是因为请求负载不是在整个对象之间均匀分布的.\n为了应对这种情况, 在 Preference list 的前 N 个节点中任何一个都可以协调写入操作.\n特别是, 由于每个写通常跟随读取操作, 因此选择写入 Coordinator 为回复最快到先前读取操作的节点, 该读取操作存储在请求 context 中.\n这种优化使我们能够选择读取了先前读取操作的数据所在的节点, 从而增加了获得 “read-your-writes” 的一致性机会. 它还减少了请求处理性能的变异性, 这提高了 p999 的性能.\n#6. 经验和教训\nDynamo 被几个具有不同配置的服务使用. 这些实例通过版本对齐逻辑和读写 Quorum 特征各不相同. 以下是在 Dynamo 中使用的主模式:\n\n业务逻辑特定的对齐: 这是 Dynamo 的一个流行用例. 每个数据对象在多个节点上进行复制. 如果版本不同, 客户端应用会执行自己的对齐逻辑. 前面讨论过的购物车服务就是一个此类别中的典型例子. 其业务逻辑通过合并客户购物车的不同版本来对齐对象.\n基于时间戳的对齐: 此案例与前面的一个案例不同之处仅在于对齐机制. 在存在分歧版本的情况下, Dynamo 会执行简单的基于时间戳的“最后写入胜出”逻辑；即选择物理时间戳值最大的对象作为正确版本. 维护客户会话信息的服务是一个使用这种模式的好例子.\n高性能读取引擎: 虽然 Dynamo 是为“始终可写入”的数据存储而构建的, 但一些服务正在调整其共识特性并将其用作高性能读取引擎. 通常, 这些服务具有较高的读请求率和少量更新. 在这种配置中, 通常 R 设置为 1, W 设置为 N. 对于这些服务, Dynamo 提供将数据在多个节点之间分区和复制的能力, 从而实现增量式扩展. 其中一些实例充当了存储在更重负载后备存储中的数据的权威持久缓存. 维护产品目录和服务项目的服务属于此类别.\n\nDynamo 的主要优势在于, 其客户端应用可以调整 N、R 和 W 的值以达到所需的性能、可用性和持久性水平. 例如, N 的值决定了每个对象的持久性. Dynamo 用户通常使用的 N 值为 3.\nW 和 R 的值会影响对象可用性、持久性和一致性. 例如, 如果将 W 设置为 1, 则只要系统中至少有一个节点可以成功处理写入请求, 系统就不会拒绝任何写入请求. 然而, 低值的 W 和 R 可能会增加不一致的风险, 因为即使大多数副本未处理写入请求, 也会认为写入请求成功并将其返回给客户端. 这也引入了一个持久性的脆弱窗口, 当写入请求成功返回给客户端时, 尽管它仅在少数节点上被持久化.\n传统观点认为, 持久性和可用性密不可分. 然而, 这在这里并不一定成立. 例如, 可以通过增加 W 来减少持久性的脆弱性窗口. 这可能会增加拒绝请求的概率（从而降低可用性）, 因为需要更多存储主机保持活动状态才能处理写入请求.\nDynamo 的多个实例使用的常见 (N,R,W) 配置为 (3,2,2). 选择这些值是为了满足必要的性能、持久性、一致性和可用性 SLA 要求.\n本节中介绍的所有测量均在 (3,2,2) 配置的实时系统上进行, 该系统运行着数百个具有相同硬件配置的节点. 如前所述, 每个 Dynamo 实例都包含位于多个数据中心的节点. 这些数据中心通常通过高速网络链路连接. 回想一下, 为了生成成功的 get（或 put）响应, R（或 W）个节点需要响应 Coordinator. 显然, 数据中心之间的网络延迟会影响响应时间, 并且节点（及其数据中心位置）的选择应满足应用程序的目标 SLA.\n#6.1 平衡性能和持久性\n虽然 Dynamo 的主要设计目标是构建一个高可用的数据存储, 但性能也是亚马逊平台同样重要的考量标准. 如前所述, 为了提供一致的客户体验, 亚马逊的服务将其性能目标设定在更高的百分位（例如 99.9 或 99.99 百分位）. 对于使用 Dynamo 的服务, 一个典型的 SLA 要求是 99.9% 的读写请求在 300 毫秒内执行.\n由于 Dynamo 运行在标准商用硬件组件上, 其 I/O 吞吐量远低于高端企业服务器, 因此提供持续的高性能读写操作并非易事. 读写操作涉及多个存储节点, 这使得它更具挑战性, 因为这些操作的性能受到最慢的读或写副本的限制. 图 4 显示了 Dynamo 在 30 天内读写操作的平均延迟和 99.9 百分位延迟. 从图中可以看出, 延迟呈现出明显的昼夜变化模式, 这是由于传入请求率的昼夜变化模式造成的（即白天和晚上的请求率有显著差异）. 此外, 写入延迟明显高于读取延迟, 因为写入操作总是会导致磁盘访问. 此外, 99.9 百分位延迟约为 200 毫秒, 比平均值高出一个数量级. 这是因为第 99.9 个百分位延迟受到请求负载、对象大小和局部模式的变化等多种因素的影响.\n虽然这种性能水平对于许多服务来说是可以接受的, 但一些面向客户的服务需要更高的性能. 对于这些服务, Dynamo 提供了在性能和持久性之间进行权衡的能力. 在优化中, 每个存储节点在其主内存中维护一个对象缓冲区. 每个写操作都存储在缓冲区中, 并由写入线程定期写入存储. 在此方案中, 读操作首先检查请求的键是否存在于缓冲区中. 如果存在, 则从缓冲区而不是存储引擎读取对象.\n这种优化使得在高峰流量期间即使对于只有一千个对象的非常小的缓冲区, 99.9 百分位延迟也降低了 5 倍（见图 5）. 此外, 如图所示, 写入缓冲可以平滑更高百分位延迟. 显然, 这种方案以持久性换取性能. 在此方案中, 服务器崩溃可能导致缓冲区中排队的写入丢失. 为了降低持久性风险, 写入操作被改进为让协调器从 N 个副本中选择一个来执行“持久写入”. 由于协调器仅等待 W 个响应, 因此写入操作的性能不受单个副本执行的持久写入操作的性能影响.\n#6.2 确保均匀负载分布\nDynamo 使用一致性哈希在其副本之间划分键空间, 以确保负载均匀分布. 假设键的访问分布不是高度倾斜, 均匀的键分布可以帮助我们实现均匀的负载分配. 具体而言, Dynamo 的设计假设即使在访问分布中存在显著的倾斜, 在分布的热门端也有足够的键, 因此处理热门键的负载可以通过分区均匀地分布在各个节点上. 本节讨论 Dynamo 中出现的负载不平衡问题, 以及不同分区策略对负载分布的影响.\n为了研究负载不平衡及其与请求负载的相关性, 我们测量了每个节点在 24 小时内收到的请求总数（分为 30 分钟的间隔）. 在给定的时间窗口内, 如果节点的请求负载与平均负载的偏差小于某个阈值（此处为 15%）, 则认为该节点“不平衡”. 否则, 该节点被视为“不平衡”. 图 6 显示了此时间段内“不平衡”节点的比例（以下简称“不平衡率”）. 作为参考, 还绘制了此时间段内整个系统收到的相应请求负载. 如图所示, 不平衡率随着负载的增加而降低. 例如, 在低负载期间, 不平衡率高达 20%, 而在高负载期间接近 10%. 直观地讲, 这可以解释为在高负载下, 大量常用键被访问, 并且由于键的均匀分布, 负载也均匀分布. 然而, 在低负载（负载为测量峰值负载的 1/8）期间, 访问的热门键较少, 导致负载不平衡程度更高.\n本节讨论 Dynamo 的分区方案如何随着时间的推移而演变及其对负载分配的影响.\n策略 1：每个节点分配 T 个随机令牌, 并按令牌值进行分区 ：这是生产环境中部署的初始策略（详见 4.2 节）. 在此方案中, 每个节点分配 T 个令牌（从哈希空间中均匀随机选择）. 所有节点的令牌均根据其在哈希空间中的值进行排序. 每两个连续的令牌定义一个范围. 最后一个令牌和第一个令牌构成一个范围, 该范围从哈希空间中的最高值“环绕”到最低值. 由于令牌是随机选择的, 因此范围的大小会有所不同. 随着节点加入和离开系统, 令牌集会发生变化, 范围也会随之变化. 需要注意的是, 维护每个节点成员资格所需的空间会随着系统中节点数量的增加而线性增加.\n在使用此策略时, 遇到了以下问题. 首先, 当新节点加入系统时, 它需要从其他节点“窃取”其键范围. 然而, 将键范围交给新节点的节点必须扫描其本地持久化存储以检索相应的数据项集. 需要注意的是, 在生产节点上执行此类扫描操作非常棘手, 因为扫描是高度资源密集型操作, 并且需要在后台执行而不影响客户性能. 这要求我们以最低优先级运行引导任务. 然而, 这显著减慢了引导过程, 在繁忙的购物季节, 当节点每天处理数百万个请求时, 引导过程几乎需要一天才能完成. 其次, 当节点加入/离开系统时, 许多节点处理的键范围会发生变化, 需要重新计算新范围的默克尔树, 这在生产系统上执行起来并非易事. 最后, 由于键值范围的随机性, 很难对整个键值空间进行快照, 这使得归档过程变得复杂. 在该方案中, 归档整个键值空间需要我们从每个节点分别检索键值, 效率极低.\n该策略的根本问题在于数据分区和数据放置方案相互交织. 例如, 在某些情况下, 为了应对不断增长的请求负载, 我们倾向于向系统添加更多节点. 然而, 在这种情况下, 添加节点不可能不影响数据分区. 理想情况下, 最好使用独立的分区和放置方案. 为此, 我们评估了以下策略：\n策略 2：每个节点分配 T 个随机令牌, 并设置大小相等的分区： 在此策略中, 哈希空间被划分为 Q 个大小相等的分区/范围, 每个节点分配 T 个随机令牌. Q 通常设置为 Q = N 和 Q = S*T, 其中 S 是系统中的节点数. 在此策略中, 令牌仅用于构建将哈希空间中的值映射到有序节点列表的函数, 而不用于决定分区. 从分区末端顺时针遍历一致性哈希环时遇到的前 N ​​个唯一节点将被放置到该节点上. 图 7 展示了 N=3 时的此策略. 在此示例中, 从包含键 k1 的分区末端遍历环时会遇到节点 A、B、C. 此策略的主要优点是：(i) 分区与分区放置解耦, 以及 (ii) 能够在运行时更改放置方案.\n策略 3：每个节点 Q/S 个令牌, 大小相等的分区： 与策略 2 类似, 此策略将哈希空间划分为 Q 个大小相等的分区, 并且分区的放置与分区方案无关. 此外, 每个节点分配有 Q/S 个令牌, 其中 S 是系统中的节点数. 当一个节点离开系统时, 其令牌会随机分配给剩余节点, 以保留这些属性. 同样, 当一个节点加入系统时, 它会以保留这些属性的方式从系统中的节点“窃取”令牌.\n针对 S=30 和 N=3 的系统, 评估了这三种策略的效率. 然而, 公平地比较这些不同的策略并非易事, 因为不同的策略具有不同的配置来调整其效率. 例如, 策略 1 的负载分布属性取决于令牌数量（即 T）, 而策略 3 则取决于分区数量（即 Q）. 比较这些策略的一种公平方法是评估其负载分布的偏差, 同时所有策略都使用相同的空间来维护其成员信息. 例如, 在策略 1 中, 每个节点需要维护环中所有节点的令牌位置, 而在策略 3 中, 每个节点需要维护分配给每个节点的分区信息.\n在我们的下一个实验中, 我们通过改变相关参数（T 和 Q）来评估这些策略. 我们针对每个节点需要维护的不同规模的成员信息, 测量了每种策略的负载均衡效率, 其中负载均衡效率定义为每个节点服务的平均请求数与最热节点服务的最大请求数之比.\n结果如图 8 所示. 从图中可以看出, 策略 3 的负载均衡效率最高, 策略 2 的负载均衡效率最差. 在 Dynamo 实例从策略 1 迁移到策略 3 的过程中, 策略 2 曾短暂地充当过一个过渡阶段. 与策略 1 相比, 策略 3 的效率更高, 并且将每个节点维护的成员信息大小减少了三个数量级. 虽然存储不是主要问题, 但节点会定期交换成员信息, 因此最好尽可能保持这些信息的紧凑. 除此之外, 策略 3 还具有以下优势, 部署更简单：(i) 更快的引导/恢复： 由于分区范围是固定的, 它们可以存储在单独的文件中, 这意味着只需传输文件即可将分区作为一个单元进行重新定位（避免了定位特定项目所需的随机访问）. 这简化了引导和恢复的过程.  （二） 易于归档 ：定期归档数据集是大多数亚马逊存储服务的强制性要求. 在策略 3 中, 归档 Dynamo 存储的整个数据集更为简单, 因为分区文件可以单独归档. 相比之下, 在策略 1 中, 令牌是随机选择的, 归档 Dynamo 中存储的数据需要分别从各个节点检索密钥, 这通常效率低下且速度缓慢. 策略 3 的缺点是, 更改节点成员身份需要协调, 以保留分配所需的属性.\n#6.3 偏离的版本: 何时以及有多少\n#6.4 客户端协调还是服务器协调\n如第 5 节所述, Dynamo 有一个请求协调组件, 它使用状态机来处理传入的请求.\n客户端请求由负载均衡器统一分配给环中的节点.\n任何 Dynamo 节点都可以充当 get 请求的 Coordinator.\n另一方面, put 请求将由键当前 Preference list 中的节点协调.\n这种限制是由于这些优先节点还承担了创建新版本戳的额外责任, 该戳会因果地包含被写请求更新的版本.\n需要注意的是, 如果 Dynamo 的版本控制方案基于物理时间戳, 则任何节点都可以协调写请求.\n请求协调的另一种方法是将状态机移至客户端节点.\n在此方案中, 客户端应用程序使用库在本地执行请求协调.\n客户端会定期随机选择一个 Dynamo 节点, 并下载其当前的 Dynamo 成员状态视图.\n使用此信息, 客户端可以确定哪些节点集合构成了任何给定键的 Preference list.\n读取请求可以在客户端节点进行协调, 从而避免了负载均衡器将请求分配给随机 Dynamo 节点时产生的额外网络跃点.\n写入操作将被转发到键的 Preference list 中的节点, 或者如果 Dynamo 使用基于时间戳的版本控制, 则可以在本地进行协调.\n客户端驱动协调方法的一个重要优势是不再需要负载均衡器来均匀分配客户端负载.\n通过将密钥近乎均匀地分配给存储节点, 可以隐式地保证公平的负载分配.\n显然, 该方案的效率取决于客户端成员信息的新鲜度.\n目前, 客户端每 10 秒轮询一个随机的 Dynamo 节点以获取成员更新.\n选择基于拉取的方法而不是基于推送的方法, 因为前者在客户端数量较多的情况下扩展性更好, 并且只需要在服务器上维护很少的客户端状态.\n然而, 在最坏的情况下, 客户端可能会在 10 秒内面临过时的成员信息.\n如果客户端检测到其成员表已过时（例如, 当某些成员无法访问时）, 它将立即刷新其成员信息.\n表 2 显示了与服务器驱动方法相比, 使用客户端驱动协调方法在 24 小时内观察到的 99.9 百分位和平均值的延迟改进. 从表中可以看出, 客户端驱动协调方法将 99.9 百分位延迟至少降低了 30 毫秒, 并将平均值降低了 3 到 4 毫秒. 延迟的改进是因为客户端驱动方法消除了负载均衡器的开销以及将请求分配给随机节点时可能产生的额外网络跃点. 从表中可以看出, 平均延迟往往明显低于 99.9 百分位的延迟. 这是因为 Dynamo 的存储引擎缓存和写缓冲区具有良好的命中率. 此外, 由于负载均衡器和网络为响应时间引入了额外的可变性, 因此 99.9 百分位的响应时间增益高于平均值.\n#6.5 平衡后台任务与前台任务\n除了正常的前台 put/get 操作外, 每个节点还执行不同类型的后台任务, 用于副本同步和数据交接（由于提示或添加/删除节点）.\n在早期的生产设置中, 这些后台任务引发了资源争用问题, 并影响了常规 put 和 get 操作的性能.\n因此, 有必要确保后台任务仅在常规关键操作不会受到显著影响时运行.\n为此, 后台任务与准入控制机制集成.\n每个后台任务都使用此控制器来预留资源（例如数据库）的运行时片段, 供所有后台任务共享.\n采用基于对前台任务性能的监控的反馈机制来更改可供后台任务使用的片段数量.\n准入控制器在执行前台 put/get 操作时, 会持续监控资源访问行为.\n监控的方面包括磁盘操作的延迟、由于锁争用和事务超时导致的数据库访问失败, 以及请求队列等待时间.\n这些信息用于检查给定尾随时间窗口内的延迟（或失败）百分位数是否接近期望阈值.\n例如, 后台控制器会检查数据库读取的 p99 延迟（过去 60 秒内）与预设阈值（例如 50ms）的接近程度.\n控制器使用此类比较来评估前台操作的资源可用性.\n随后, 它会决定有多少时间片可供后台任务使用, 从而利用反馈回路来限制后台活动的侵入性.\n需要注意的是, [4]中已经研究了类似的后台任务管理问题.\n#6.6 讨论\n本节总结了在实施和维护 Dynamo 过程中获得的一些经验. 过去两年, 许多 Amazon 内部服务都使用了 Dynamo, 它为其应用程序提供了相当高的可用性. 特别是, 应用程序对 99.9995% 的请求都收到了成功响应（没有超时）, 迄今为止没有发生过任何数据丢失事件.\n此外, Dynamo 的主要优势在于它提供了必要的旋钮, 可以使用 (N, R, W) 三个参数来根据需求调整实例. 与流行的商业数据存储不同, Dynamo 将数据一致性和协调逻辑问题暴露给开发人员. 一开始, 人们可能会认为应用程序逻辑会更加复杂. 然而, 从历史上看, 亚马逊的平台是为高可用性而构建的, 许多应用程序的设计都考虑到了处理可能出现的各种故障模式和不一致性. 因此, 将这些应用程序移植到 Dynamo 是一个相对简单的任务. 对于想要使用 Dynamo 的新应用程序, 在开发的初始阶段需要进行一些分析, 以选择合适的冲突解决机制, 从而满足业务案例的需求. 最后, Dynamo 采用完全成员模型, 每个节点都知道其对等节点托管的数据. 为此, 每个节点都会主动与系统中的其他节点传播完整的路由表. 这种模型对于包含数百个节点的系统非常有效. 然而, 将这样的设计扩展到数万个节点并非易事, 因为维护路由表的开销会随着系统规模的增加而增加. 这一限制或许可以通过在 Dynamo 中引入分层扩展来克服. 另外, 值得注意的是, O(1) DHT 系统（例如 [14]）正在积极解决这个问题.\n#7. 结论\n本文介绍了 Dynamo, 一个高可用且可扩展的数据存储系统, 用于存储 Amazon.com 电商平台多项核心服务的状态. Dynamo 提供了所需的可用性和性能, 并成功应对了服务器故障、数据中心故障和网络分区. Dynamo 具有增量式可扩展性, 允许服务所有者根据当前请求负载进行扩展和缩减. Dynamo 允许服务所有者通过调整 N、R 和 W 参数来定制存储系统, 以满足其所需的性能、持久性和一致性 SLA.\nDynamo 在过去一年的生产环境中的应用表明, 分散式技术可以结合起来, 提供单一的高可用性系统. 它在最具挑战性的应用环境之一中的成功表明, 最终一致性存储系统可以成为高可用性应用程序的基石.\n#参考资料\n\nAmazon’s Dynamo (2007.10.2)\n\n\nDynamo 并不直接作为 Web 服务对外公开；但是, Dynamo 和类似的 Amazon 技术用于支持我们的 Amazon Web Services 的部分功能, 例如 S3.\n\n\nA Decade of Dynamo: Powering the next wave of high-performance, internet-scale applications (2017.10.2)\n\n\n一切始于 2004 年, 当时亚马逊正在运行 Oracle 企业版, 并配备集群和复制功能. 我们拥有一支高级 DBA 团队, 并能与 Oracle 内部的顶级专家沟通. 我们当时正在突破当时领先的商业数据库的极限, 无法满足我们日益增长的亚马逊业务对可用性、可扩展性和性能的需求.\n我们基于 Oracle 的数据库基础架构已不堪重负, 这促使我们评估能否开发一个能够长期支持我们业务需求的专用数据库. 我们优先考虑能够支持亚马逊购物车等大规模、关键任务服务的需求, 并质疑关系数据库传统上持有的假设, 例如对强一致性的要求. 我们的目标是构建一个拥有无限可扩展性、一致性能和高可用性的数据库, 以支持我们快速增长的业务需求.\n深入研究我们现有数据库的使用情况后发现, 它们的关系功能经常被忽略. 大约 70% 的操作属于键值类型, 即仅使用主键并返回一行. 大约 20% 的操作会返回一组行, 但仍然只对单个表进行操作.\n考虑到这些需求, 并秉持着勇于挑战现状的理念, 一小群分布式系统专家齐聚一堂, 设计了一个可水平扩展的分布式数据库, 该数据库可以同时扩展读写能力, 以满足我们业务的长期需求. 这就是 Amazon Dynamo 数据库的起源.\n… Dynamo 的白皮书广受好评, 并成为创建分布式数据库技术类别（如今通常称为“NoSQL”）的催化剂.\n\n\n论文笔记：[SOSP 2007] Dynamo: Amazon’s Highly Available Key-value Store\n\n","categories":["database-system"],"tags":["分布式系统","数据库","K-V 存储"]},{"title":"Oracle数据库学习","url":"/2020/12/03/oracledb/","content":"#前言\n参考资料:\nOracle官方文档\n#Schema\nSchema 是表, 索引, 存储过程等等数据库对象(DB Object)的存储容器\n用户与 Schema 之间是一对一的关系\n例如: 访问用户 A 的 xxx 对象就通过 A.xxx\n\n#Oracle 表空间\n表空间是逻辑上, 存放数据表的容器\n每张表一定属于某个表空间\n每个用户(Schema)可以拥有多个表空间, 而且都有一个默认表空间\n用户之间可以通过共享表空间的方式共享数据表\n物理上, 表空间的存储可能不是连续的\n#PL/SQL\n\nA primary benefit of PL/SQL is the ability to store application logic in the database itself.\n\n#Oracle 体系结构 (重要)\n\nOracle Server = Oracle Instance + Oracle Database\n\n#1. Oracle 实例 (Oracle Instance)\nOracle Instance = SGA(system global area) + Background Process\n是一组 OS 进程(线程)和一些内存(SGA) 的总称\n可以用来 mount 和 open 一个数据库\n一个实例在其生存期中最多只能装载和打开一个数据库。要想再打开这个（或其他）数据库，必须先丢弃这个实例，并创建一个新的实例。\n一个数据库实例的状态分为以下几种\n\n\nstarted\n\n\nmounted\n\n\nopen\n\n\nclose\n\n\nmounted\n\n\nopen\n\n\n#started 状态\n在执行startup nomount命令后, Oracle 会执行以下操作, 之后实例会进入started状态, 此时实例还未绑定数据库\n\n\n读取配置文件\n\n\n分配SGA\n\n\n启动后台进程\n\n\n打开一些用于记录的文件\n\n\n#mounted 状态\n在执行startup mount或者alter database mount命令后, 实例会进入mounted状态, 对应数据库的 open_mode 也是mounted。\n数据库打开后, 执行alter database close命令后, 实例也会进入mounted状态, 对应数据库的 open_mode 也是mounted。\n此时实例与数据库建立了联系, 但是只有 DBA 能够访问数据库。\n#open 状态\n在执行startup或者alter database open命令后, 实例会进入open状态。\n此时实例与数据库建立了联系, 数据库完全启动, 普通用户也可以访问数据库。\n#2. Oracle 数据库 (Oracle Database)\nOracle Database = Controlfile + datafile + logfile + spfile + …\n是存储在磁盘上的一组数据文件的集合\n一般来说, 一个数据库上只有一个实例对其进行操作\n但是也有例外: RAC（Real Application Clusters）就允许在集群环境中的多台计算机上操作，这样就可以有多台实例同时装载并打开一个数据库（位于一组共享物理磁盘上）\n例外 2: 容器式数据库\n#3. Oracle 数据库和Oracle 实例的关系\n\n\n一个实例一生只能够装载及打开一个数据库\n\n\n一个数据库能够被多个实例装载并打开(RAC)\n\n\n每个运行着的数据库一定与至少一个实例关联\n\n\n\nOracle 数据库与实例的启动过程\n\nOracle 数据库与实例的关闭过程\n\n#常用命令\n\n\n名称, 版本信息查看\n\n\n查询数据库版本信息\n`SQL&gt; select * from v$version;`\n\n\n\n查看数据库名称\n`SQL&gt; show parameter db_name;`\n\n\n\n查看数据库服务名\n`SQL&gt; show parameter service_names;`\n\n\n\n查询全局数据库名称\n`SQL&gt; select * from global_name;`\n\n\n\n\n\n数据库实例 (Instance)\n\n\n查询当前数据库实例名称\n`SQL&gt; select instance_name from v$instance;`\n\n\n\n查看当前数据库实例状态(open/mount 等)\n`SQL&gt; select status from v$instance;`\n\n\n\n查看数据库实例启动时间\n`SQL&gt; SELECT TO_CHAR(STARTUP_TIME,'MON-DD-RR HH24:MI:SS') AS &quot;Inst Start Time&quot; FROM V$INSTANCE;`\n\n\n\n创建一个新的数据库实例、加载数据库、打开数据库 (需要 DBA 权限)\n`SQL&gt; startup [nomount | mount | open];`\n\n\n\n关闭实例绑定的数据库、卸载数据库、结束当前实例 (需要 DBA 权限)\n`SQL&gt; shutdown [normal | transactional | immediate];`\n\n\n\n关闭实例绑定的数据库、卸载数据库\n`SQL&gt; alter database close;`\n\n\n\n加载数据库/打开数据库\n`SQL&gt; alter database [mount | open];`\n\n\n\n\n\n数据库设置, 统计信息等\n\n\n查询数据库打开模式(mounted/open/read write)\n`SQL&gt; select open_mode from v$database;`\n\n\n\n查看数据库 DBF 文件位置\n`SQL&gt; select name from v$datafile;`\n\n\n\n查询数据文件状态\n`SQL&gt; select file#,name,status,enabled,checkpoint_change## from v$datafile;`\n\n\n\n查询数据文件位置\n`SQL&gt; select name from v$datafile;`\n\n\n\n查询数据文件（表空间）大小\n`SQL&gt; select sum(bytes)/1024/1024/1024 as GB from v$datafile;`\n\n\n\n查询有效数据大小\n`SQL&gt; select sum(bytes)/1024/1024/1024 as GB from dba_segments;`\n\n\n\n查看当前库的所有数据表\n`SQL&gt; select TABLE_NAME from all_tables;`\n\n\n\n\n\n可插拔数据库 (PDB) 管理\n\n\n查看数据库是 CDB 还是传统 DB\n`SQL&gt; select name, cdb, open_mode, con_id from v$database;`\n\n\n\n查看当前容器 (CDB) 名\n`SQL&gt; show con_name;`\n\n\n\n列举当前容器中的所有 PDB 以及状态\n`SQL&gt; select con_id, dbid, guid, name, open_mode from v$pdbs;`\n\n\n\n切换到某个 PDB\n`SQL&gt; alter session set container=&lt;PDB&gt;;`\n\n\n\n启动 PDB 数据库\n`SQL&gt; alter pluggable database &lt;PDB&gt; open;`\n\n或者\n\n`SQL&gt; alter session set container=&lt;PDB&gt;;`\n`SQL&gt; startup`\n\n\n\n关闭 PDB 数据库\n`SQL&gt; alter pluggable database &lt;PDB&gt; close;`\n\n\n\n\n\n用户管理\n\n创建用户\n\n\n\n","categories":["database-system"]},{"title":"Oracle APEX学习","url":"/2020/12/07/oracle-apex/","content":"#前言\n什么垃圾软件, 毁我人生!!\n#安装\n略\n#启动\n启动 apex 安装到的数据库(PDB 默认不会自动 open)\n启动 ords 服务\n$ java -jar ords.war\n#怎么修改密码\nSQL&gt; @apxchpwd.sql\n","categories":["database-system"]},{"title":"[ICDE'13] The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases 论文阅读","url":"/2026/01/11/the-adaptive-radix-tree-artful-indexing-for-main/","content":"\nhttps://db.in.tum.de/~leis/papers/ART.pdf\n\n#0. 摘要\n主存容量的增长使得大多数数据库都能装入内存 (RAM) 中. 对于主存数据库系统而言, 索引结构的性能是一个关键瓶颈.\n传统的内存数据结构, 例如平衡二叉搜索树 (BST), 在现代硬件上效率低下, 因为它们没有充分利用 CPU 缓存.\n哈希表也常用于主存索引, 虽然速度快, 但仅支持点查询.\n为了克服这些不足, 我们提出了 ART, 一种用于高效主存索引的自适应基数树 (trie). 它的查找性能超越了经过高度优化的只读搜索树, 同时还支持非常高效的插入和删除操作.\n此外, ART 空间利用率极高, 通过自适应地为内部节点选择紧凑高效的数据结构, 解决了大多数基数树普遍存在的空间占用过高的问题.\n尽管 ART 的性能与哈希表相当, 但它以排序顺序维护数据, 这使得它能够执行范围扫描和前缀查找等额外操作.\n","categories":["database-system"]},{"title":"[OSDI'10] Large-scale Incremental Processing Using Distributed Transactions and Notifications 论文阅读","url":"/2025/12/27/large-scale-incremental-processing-using/","content":"\nMapReduece 系统解决了海量数据索引创建的问题, 但 MR 并没有解决增量数据的实时更新问题.\n本文介绍了 Percolator 系统, 讨论了如何在不支持跨行事务的 BigTable 上, 实现大规模增量处理系统.\n\n#引言\n考虑构建一个用于回答搜索查询的网络索引系统.\n该索引系统首先抓取网络上的每一个页面并进行处理, 同时在索引中维护一组不变规则.\n例如, 如果相同内容在多个 URL 下被抓取, 则只有 PageRank 值最高的 URL 会被保留并出现在索引中.\n系统会对每个链接执行反向链接处理, 使每个出站链接的锚文本被记录并指向其目标页面.\n反向链接过程必须能够正确处理重复页面.\n当链接指向某个页面的重复副本时, 这些链接应在必要时被重定向到 PageRank 值最高的那个重复页面.\n这是一个典型的批量处理任务.\n该任务可以表示为一系列 MapReduce 操作.\n例如, 一个 MapReduce 作业用于对重复页面进行聚类. 另一个 MapReduce 作业用于执行链接反转等处理步骤.\n由于 MapReduce 限制了计算的并行度, 因此系统中的不变性相对容易维护.\n在进入下一个处理阶段之前, 所有文档都会完整地执行当前处理步骤.\n例如, 当索引系统正在写入指向当前 PageRank 值最高 URL 的反向链接时, 我们无需担心该 URL 的 PageRank 值会在此过程中发生变化.\n这是因为在之前的 MapReduce 步骤中, 其 PageRank 值已经被确定并固定下来.\n现在, 考虑在仅重新抓取了一小部分网页之后, 如何对索引进行更新.\n仅对新抓取的页面运行 MapReduce 是不充分的.\n原因在于, 新抓取的页面可能与网页集合中的其他页面之间存在链接关系.\n因此, 必须对整个存储库重新运行 MapReduce, 包括新页面和已有页面.\n如果计算资源充足, MapReduce 的良好可扩展性使这种全量重计算方法在实践中是可行的.\n事实上, 在本文所述工作之前, 谷歌的网络搜索索引正是通过这种方式构建的.\n然而, 重新处理整个网页集合会丢弃之前计算所积累的结果.\n同时, 这种方法的处理延迟与存储库的整体规模成正比, 而不是与实际更新的数据量成正比.\n另一种思路是将整个存储库存放在数据库管理系统中.\n索引系统可以通过事务方式对单个文档进行更新, 从而维护数据不变性.\n然而, 现有的数据库管理系统无法处理如此庞大的数据规模.\n谷歌的索引系统需要在数千台机器上存储数十 PB 级别的数据.\n诸如 Bigtable 之类的分布式存储系统可以扩展到这种规模.\n但这类系统并未向程序员提供在并发更新场景下维护数据不变性的高层工具或抽象.\n理想的网络搜索索引数据处理系统应当针对增量处理进行优化.\n也就是说, 该系统能够维护一个规模极其庞大的文档库.\n并且在每次抓取少量新文档时, 都能高效地对现有文档库进行更新.\n由于系统会同时处理大量细粒度的更新操作,\n理想的系统还应提供机制, 以在并发更新条件下保持数据不变性.\n同时, 系统还需要能够跟踪哪些更新已经被成功处理.\n本文余下部分将介绍一种具体的增量处理系统, 即 Percolator.\nPercolator 为用户提供对多 PB 级别存储库的随机访问能力.\n随机访问使我们能够处理单独文档, 而无需像 MapReduce 那样处理整个存储库.\n为了实现高吞吐量, 需要在多台机器上同时使用多个线程来转换仓库.\n因此, Percolator 提供了符合 ACID 标准的事务, 使程序员更容易理解仓库的状态.\n我们目前实现了快照隔离语义.\n除了考虑并发性之外, 增量系统的程序员还需要跟踪增量计算的状态.\n为了帮助他们完成这项任务, Percolator 提供了观察者机制: 当用户指定的列发生变化时, 系统会调用一段代码.\nPercolator 应用程序由一系列观察者构成.\n每个观察者完成一项任务, 并通过向表中写入数据, 为“下游”观察者创造更多工作.\n外部进程通过向表中写入初始数据来触发链中的第一个观察者.\nPercolator 专为增量处理而设计, 并非旨在取代现有解决方案来处理大多数数据处理任务.\n对于无法将结果分解为小更新的计算 (例如文件排序), MapReduce 更为合适.\n此外, 此类计算应具有强一致性要求, 否则 Bigtable 就足够了.\n最后, 此类计算在某些维度上 (例如数据总量、转换所需的 CPU 资源等) 应非常庞大.\n不适合 MapReduce 或 Bigtable 的较小规模计算可以使用传统的数据库管理系统 (DBMS) 来处理.\n在谷歌内部, Percolator 的主要应用是准备网页以纳入实时网络搜索索引.\n通过将索引系统转换为增量系统, 我们能够在抓取文档的同时进行处理.\n这使得平均文档处理延迟降低了 100 倍, 文档在搜索结果中的平均出现时间也缩短了近 50% (搜索结果的出现时间还包含索引以外的延迟, 例如文档更改到被抓取之间的时间).\n该系统还被用于将网页渲染成图像.\nPercolator 会跟踪网页与其依赖资源之间的关系, 以便在任何依赖资源发生更改时重新处理网页.\n#设计\n\nPercolator 提供了两种主要的抽象方式, 用于大规模执行增量处理:\n\n在存储库上随机访问的 ACID 事务\n观察者 (observer) 机制, 用于组织增量计算\n\nPercolator 系统由三个二进制文件组成, 它们运行在集群中的每台机器上:\n\nPercolator 工作进程\nBigtable Tablet Server\nGFS Chunk Server\n\n所有观察者都链接到 Percolator 工作进程, 该进程扫描 Bigtable 以查找已更改的列 (“通知”), 并在工作进程中以函数调用的形式调用相应的观察者.\n观察者通过向 Bigtable Tablet Server 发送读/写 RPC 来执行事务, Tablet Server 再向 GFS Chunk Server 发送读/写 RPC.\n该系统还依赖于两个小型服务:\n\n时间戳服务(Timestamp Oracle)\n轻量级锁服务 (基于 Chubby)\n\n时间戳服务提供严格递增的时间戳, 这是快照隔离协议正确运行所必需的属性.\n工作进程使用轻量级锁服务来提高查找脏通知的效率.\n从程序员的角度来看, Percolator 存储库由少量表组成.\n每个表都是一系列“单元格”的集合, 这些单元格按行和列进行索引.\n每个单元格包含一个值: 一个未经解释的字节数组.\n(在内部, 为了支持快照隔离, 每个单元格表示为一系列按时间戳索引的值. )\nPercolator 的设计受到大规模运行需求和对延迟要求不高的影响.\n放宽延迟要求使我们可以采用惰性方法来清理在故障机器上运行的事务遗留的锁.\n这种惰性且易于实现的方法可能会使事务提交延迟数十秒.\n对于运行 OLTP 任务的数据库管理系统 (DBMS) 而言, 这种延迟是不可接受的, 但对于构建 Web 索引的增量处理系统而言, 则是可以容忍的.\nPercolator 没有集中式的事务管理位置.\n特别是, 它缺少全局死锁检测器.\n这增加了冲突事务的延迟, 但也使系统能够扩展到数千台机器.\n#Bigtable 简介\nPercolator 构建于 Bigtable 分布式存储系统之上.\nBigtable 向用户呈现一个多维排序映射: 键是 (行、列、时间戳) 元组.\nBigtable 支持对每一行进行查找和更新操作, 并且行事务支持对单行进行原子 RMW 操作.\nBigtable 可以处理 PB 级数据, 并在大量不可靠的机器上可靠运行.\n一个运行中的 Bigtable 由一组 Tablet 服务器组成, 每个 Tablet 服务器负责服务多个 Tablet (键空间中的连续区域).\n主服务器协调 Tablet 服务器的运行, 例如指示它们加载或卸载 Tablet.\n每个 Tablet 都以只读的 Google SSTables 格式文件集合的形式存储.\nSSTable 存储在 GFS 文件系统中.\nBigtable 依赖 GFS 在磁盘丢失时保存数据.\nBigtable 允许用户通过将一组列分组到一个位置组中来控制表的性能特征.\n每个位置组中的列都存储在各自的 SSTable 集合中, 这样可以降低扫描成本, 因为无需扫描其他列中的数据.\n基于 Bigtable 构建 Percolator 的决定奠定了其整体架构.\nPercolator 保留了 Bigtable 接口的核心: 数据以 Bigtable 的行和列形式组织, Percolator 元数据则存储在旁边的特殊列中.\nPercolator 的 API 与 Bigtable 的 API 非常相似: Percolator 库主要由封装在 Percolator 特有计算中的 Bigtable 操作组成.\n因此, 实现 Percolator 的挑战在于提供 Bigtable 所不具备的功能: 多行事务和观察者框架.\n#事务\n// 图 2: 使用 Percolator 进行文档聚类和去重的代码样例bool UpdateDocument(Document doc) &#123;  Transaction t(&amp;cluster);  t.Set(doc.url(), &quot;contents&quot;, &quot;document&quot;, doc.contents());  int hash = Hash(doc.contents());  // dups table maps hash → canonical URL  string canonical;  if (!t.Get(hash, &quot;canonical-url&quot;, &quot;dups&quot;, &amp;canonical)) &#123;    // No canonical yet; write myself in    t.Set(hash, &quot;canonical-url&quot;, &quot;dups&quot;, doc.url());  &#125; // else this document already exists, ignore new copy  return t.Commit();&#125;\nPercolator 提供跨行、跨表事务, 并具备 ACID 快照隔离语义.\nPercolator 用户使用命令式语言 (目前为 C++) 编写事务代码, 并将对 Percolator API 的调用与代码混合使用.\n图 2 展示了一个简化的文档聚类示例, 该聚类基于文档内容的哈希值.\n在本例中, 如果 Commit() 返回 false, 则表示事务发生冲突 (在本例中是因为同时处理了两个具有相同内容哈希值的 URL), 应在退避后重试.\nGet() 和 Commit() 的调用是阻塞的; 并行性是通过在线程池中同时运行多个事务来实现的.\n虽然可以在没有强事务的情况下增量处理数据, 但事务使用户更容易理解系统状态, 并避免在长期存在的存储库中引入错误.\n例如, 在事务型 Web 索引系统中, 程序员可以做出这样的假设: 文档内容的哈希值始终与索引重复项的表保持一致.\n如果没有事务, 一次不合时宜的崩溃可能会导致永久性错误: 文档表中的某个条目在重复项表中找不到对应的 URL.\n事务还使得构建始终保持最新且一致的索引表变得容易.\n请注意, 这两个示例都需要跨行事务, 而不是 Bigtable 已提供的单行事务.\n\nPercolator 使用 Bigtable 的时间戳维度存储每个数据项的多个版本.\n多个版本是实现快照隔离所必需的, 它使每个事务看起来像是从某个时间戳的稳定快照中读取数据.\n写入操作则出现在不同且较晚的时间戳中.\n快照隔离机制可以防止写-写冲突: 如果并发运行的事务 A 和 B 写入同一个单元格, 则最多只有一个事务会被提交.\n快照隔离不提供可串行化; 特别是, 在快照隔离下运行的事务容易受到 Write Skew 的影响.\n快照隔离相对于可串行化协议的主要优势在于更高效的读取.\n由于任何时间戳都代表一个一致的快照, 因此读取单元格只需要在给定的时间戳执行 Bigtable 查找; 无需获取锁.\n图 3 展示了快照隔离下事务之间的关系.\n由于 Percolator 是作为访问 Bigtable 的客户端库构建的, 而不是直接控制存储访问, 因此它在实现分布式事务方面面临着与传统并行数据库管理系统 (PDBMS) 不同的挑战.\n其他并行数据库将锁定机制集成到管理磁盘访问的系统组件中: 由于每个节点已经负责协调对磁盘数据的访问, 因此它可以对请求授予锁, 并拒绝违反锁定要求的访问.\n相比之下, Percolator 中的任何节点都可以 (并且确实会) 发出请求直接修改 Bigtable 中的状态: 没有方便的地方可以拦截流量并分配锁.\n因此, Percolator 必须显式地维护锁:\n\n容灾: 即使机器发生故障, 锁也必须保持有效; 如果锁在通信的两次提交之间丢失, 那么系统将会错误地提交两个本应冲突的事务.\n锁服务必须提供高吞吐量; 成千上万台机器将同时请求锁.\n锁服务还应具有低延迟; 每个 Get() 操作除了读取数据外, 还需要读取锁, 我们希望尽可能降低这种延迟.\n\n鉴于这些要求, 锁服务器需要进行复制 (以应对故障)、分布式和负载均衡 (以处理负载), 并写入持久数据存储.\nBigtable 本身满足我们所有要求, 因此 Percolator 将其锁存储在与数据相同的 Bigtable 中的特殊内存列里, 并在访问该行数据时, 通过 Bigtable 的行事务读取或修改锁.\nclass Transaction &#123;  struct Write &#123;    Row row;    Column col;    std::string value;  &#125;;  std::vector&lt;Write&gt; writes;  int start_ts;  Transaction(): start_ts(oracle.GetTimestamp()) &#123;&#125;  void Set(Write w) &#123;    writes.push_back(w);  &#125;&#125; // class Transaction\n图 6: Percolator 事务伪代码\n现在我们将更详细地探讨事务协议.\n图 6 显示了 Percolator 事务的伪代码, 图 4 显示了事务执行期间 Percolator 数据和元数据的布局.\n系统使用的各种元数据列在图 5 中进行了描述.\n事务构造函数会向时间戳服务请求起始时间戳 (start_ts), 该时间戳决定了 Get() 函数所看到快照的一致性.\nSet() 函数的调用会被缓冲 (第 7 行), 直到提交时才会执行.\n提交缓冲写入的基本方法是由客户端协调的两阶段提交.\n不同机器上的事务通过 Bigtable Tablet 服务器上的行事务进行交互.\n// Prewrite tries to lock cell w, returning false in case of conflict.bool Prewrite(Write w, Write primary) &#123;  Column c = w.col;  bigtable::Txn T = bigtable::StartRowTransaction(w.row);  // Abort on writes after our start_timestamp . . .  if (T.Read(w.row, c + &quot;write&quot;, [start_ts, +inf])) return false;  // . . . or locks at any timestamp.  if (T.Read(w.row, c + &quot;lock&quot;, [0, +inf])) return false;  T.Write(w.row, c + &quot;data&quot;, start_ts, w.value);  T.Write(w.row, c + &quot;lock&quot;, start_ts, &#123;primary.row, primary.col&#125;); // The primary’s location.  return T.Commit();&#125;\n在提交的第一阶段 (Prewrite), 我们会尝试锁定所有正在写入的单元格.\n为了处理客户端故障, 我们会任意指定一个锁作为主锁; 我们将在下文讨论这一机制.\n事务会读取元数据, 以检查每个正在写入的单元格是否存在冲突.\n元数据冲突有两种情况: 如果事务在其起始时间戳之后发现另一个写入记录, 则会中止 (第 32 行); 这正是快照隔离机制用于防范的写-写冲突.\n如果事务在任意时间戳发现另一个锁, 也会中止 (第 34 行).\n另一个事务可能只是在提交到我们起始时间戳之后延迟释放其锁, 但我们认为这种情况不太可能发生, 因此会选择中止当前事务.\n如果没有冲突, 我们会向每个单元的起始时间戳写入锁和数据.\n如果没有冲突的单元格, 事务可以提交并进入第二阶段.\nbool Commit() &#123;  Write primary = writes[0];  std::vector&lt;Write&gt; secondaries(writes.begin() + 1, writes.end());  if (!Prewrite(primary, primary)) return false;  for (Write w: secondaries)    if (!Prewrite(w, primary)) return false;  int commit_ts = oracle.GetTimestamp();  // Commit primary first.  Write p = primary;  bigtable::Txn T = bigtable::StartRowTransaction(p.row);  if (!T.Read(p.row, p.col + &quot;lock&quot;, [start_ts, start_ts])) return false; // aborted while working  T.Write(p.row, p.col + &quot;write&quot;, commit_ts, start_ts); // Pointer to data written at start_ts .  T.Erase(p.row, p.col + &quot;lock&quot;, commit_ts);  if (!T.Commit()) return false; // commit point  // Second phase: write out write records for secondary cells.  for (Write w: secondaries) &#123;    bigtable::Write(w.row, w.col + &quot;write&quot;, commit_ts, start_ts);    bigtable::Erase(w.row, w.col + &quot;lock&quot;, commit_ts);  &#125;  return true;&#125;\n在第二阶段开始时, 客户端从时间戳服务获取提交时间戳 (第 48 行).\n随后, 客户端在每个单元格上 (从主单元格开始) 释放其锁, 并通过将锁替换为写入记录, 使写入对读者可见.\n写入记录向读者表明该单元格中存在已提交的数据.\n它包含一个指向起始时间戳的指针, 读者可以通过该时间戳找到实际数据.\n一旦主单元格的写入变得可见 (第 58 行), 事务就必须提交, 因为它已经使写入结果对读者可见.\nbool Get(Row row, Column c, std::string *value) &#123;  while (true) &#123;    bigtable::Txn T = bigtable::StartRowTransaction(row);    // Check for locks that signal concurrent writes.    if (T.Read(row, c + &quot;lock&quot;, [0, start_ts])) &#123;      // There is a pending lock; try to clean it and wait      BackoffAndMaybeCleanupLock(row, c);      continue;    &#125;    // Find the latest_write below our start_timestamp.    latest_write = T.Read(row, c + &quot;write&quot;, [0, start_ts]);    if (!latest_write.found()) return false; // no data    int data_ts = latest_write.start_timestamp();    *value = T.Read(row, c + &quot;data&quot;, [data_ts, data_ts]);    return true;  &#125;&#125;\nGet() 操作首先检查时间戳范围 [0, start_ts] 内是否存在锁, 该范围对应事务快照中可见的时间戳区间 (第 12 行).\n如果存在锁, 则表示另一个事务正在并发写入该单元格, 因此读取事务必须等待锁被释放.\n如果未发现冲突的锁, Get() 会读取该时间戳范围内最新的写入记录 (第 19 行), 并返回与该写入记录对应的数据项 (第 22 行).\n由于可能发生客户端故障, 事务处理过程会变得更加复杂.\nTablet 服务器故障不会影响系统, 因为 Bigtable 能保证在 Tablet 服务器故障后, 已写入的锁仍然有效.\n如果客户端在事务提交过程中发生故障, 则会遗留锁.\n如果不清理这些锁, 后续事务可能会无限期地阻塞.\n为此, Percolator 采用惰性清理策略: 当事务 A 遇到事务 B 遗留下来的冲突锁时, A 可以判断事务 B 已经失败, 并负责清除其锁.\n图 5: Percolator 列 c 在 Bigtable 中的布局\n\n\n\n列名\n说明\n\n\n\n\nc:lock\nAn uncommitted transaction is writing this cell; contains the location of primary lock\n\n\nc:write\nCommitted data present; stores the Bigtable timestamp of the data\n\n\nc:data\nStores the data itself\n\n\nc:notify\nHint: observers may need to run\n\n\nc:ack\nObserver “O” has run ; stores start timestamp of successful last run\n\n\n\n事务 A 很难完全确认事务 B 是否已经失败.\n因此, 必须避免 A 清理 B 的事务与实际上并未失败的 B 正在提交同一事务之间发生竞争.\nPercolator 通过在每个事务中指定一个单元格作为所有提交或清理操作的同步点来解决这一问题.\n该单元格上的锁称为主锁.\n事务 A 和事务 B 都会就主锁的位置达成一致, 主锁的位置会被写入所有其他单元格的锁中.\n无论是执行清理操作还是提交操作, 都必须修改主锁.\n由于该修改是在 Bigtable 行事务下完成的, 因此清理和提交操作中最多只有一个能够成功.\n具体而言, 在事务 B 提交之前, 它必须检查自己是否仍然持有主锁, 并将主锁替换为写入记录.\n在事务 A 擦除 B 的锁之前, A 必须检查主锁以确认 B 尚未提交.\n如果主锁仍然存在, 则可以安全地清除对应的锁.\n当客户端在提交的第二阶段发生崩溃时, 事务已经越过提交点 (至少写入了一条写入记录), 但仍然可能存在锁.\n我们必须对这些事务执行前滚操作.\n遇到锁的事务可以通过检查主锁来区分两种情况: 如果主锁已被写入记录替换, 则写入该锁的事务必定已经提交, 并且该锁必须被前滚.\n否则, 应当回滚, 因为我们总是先提交主锁, 因此如果主锁尚未提交, 可以确保回滚是安全的.\n在执行前滚时, 负责清理的事务会像原始事务一样, 将尚未解锁的锁替换为写入记录.\n由于清理操作是在主锁上进行同步的, 因此清理由仍然活跃的客户端所持有的锁在语义上是安全的.\n然而, 这样做会带来性能损失, 因为回滚会强制事务中止.\n因此, 事务只有在怀疑锁属于已经失效或卡死的 Worker 时, 才会执行锁清理操作.\nPercolator 使用一种简单的机制来判断其他事务是否仍然处于活跃状态.\n正在运行的 Worker 会向 Chubby 锁服务写入一个令牌, 用于表明它们仍然属于该系统.\n其他 Worker 可以通过该令牌是否存在来判断对应的 Worker 是否仍然存活, 该令牌会在进程退出时自动删除.\n为了处理仍然存活但未执行任何操作的 Worker, 我们还会将运行时间写入锁中.\n即使 Worker 的活跃令牌仍然有效, 只要锁中记录的运行时间过旧, 该锁也会被清理.\n为了支持长时间运行的提交操作, Worker 会在提交过程中定期更新该运行时间.\n#时间戳\n时间戳服务是一个严格按递增顺序分发时间戳的服务.\n由于每个事务都需要两次联系时间戳服务, 因此该服务必须具备良好的可扩展性.\n时间戳服务会定期分配一个时间戳范围, 并将所分配时间戳的最大值写入稳定存储.\n在给定已分配时间戳范围的情况下,时间戳服务可以完全从内存中满足后续请求.\n如果服务重启, 时间戳会向前跳到已分配的最大值, 但绝不会回退.\n为了节省 RPC 开销 (代价是增加事务延迟), 每个 Percolator 工作进程通过仅维护一个待处理的 RPC 请求来跨事务处理时间戳请求.\n随着时间戳服务负载的增加, 批处理规模会自然增大以进行补偿.\n批处理提升了时间戳服务的可扩展性, 但不会影响时间戳准确性的保证.\n我们的时间戳服务在单台机器上每秒可提供约 200 万个时间戳.\n事务协议使用严格递增的时间戳来保证 Get() 返回事务开始时间戳之前所有已提交的写入操作.\n为了理解该机制如何提供这一保证, 考虑一个在时间戳 TRT_RTR​ 进行读取的事务 R, 以及一个在时间戳 TW&lt;TRT_W &lt; T_RTW​&lt;TR​ 进行写入的事务 W.\n我们将证明事务 R 一定能够看到事务 W 的写入操作.\n由于 TW&lt;TRT_W &lt; T_RTW​&lt;TR​, 我们知道时间戳服务要么在 TRT_RTR​ 之前, 要么与 TRT_RTR​ 位于同一批次中分发了 TWT_WTW​.\n因此, 事务 W 在事务 R 接收到其起始时间戳 TRT_RTR​ 之前请求了时间戳 TWT_WTW​.\n同时, 事务 R 在接收到其起始时间戳 TRT_RTR​ 之前无法执行任何读取操作, 而事务 W 在请求其提交时间戳之前已经写入了锁.\n由此可知, 在 R 执行任何读取操作之前, W 至少已经写入了所有锁.\n因此, R 的 Get() 操作要么看到已完全提交的写入记录, 要么看到锁, 在后一种情况下, R 将阻塞直到锁被释放.\n无论出现哪种情况, 事务 W 的写入操作对于事务 R 的 Get() 操作都是可见的.\n#通知机制\n事务允许用户在保持数据一致性的前提下修改表, 但用户还需要一种机制来触发并运行这些事务.\n在 Percolator 中, 用户编写称为 “观察器” 的代码, 这些代码会在表发生变化时被触发.\n我们将所有观察器链接到一个二进制文件中, 该文件会与系统中的每个 Tablet 服务器一同运行.\n每个观察器都会向 Percolator 注册一个函数以及一组列.\n当任意行中的某一列被写入数据时, Percolator 就会调用对应的函数.\nPercolator 应用由一系列观察器构成.\n每个观察器完成一项特定任务, 并通过写入表为 “下游” 观察器创造更多工作.\n在我们的索引系统中, MapReduce 通过运行加载器事务将抓取到的文档加载到 Percolator 中.\n这些加载器事务会触发文档处理器事务, 用于对文档进行索引处理 (例如解析内容、提取链接等).\n文档处理器事务还会进一步触发其他事务, 例如聚类事务.\n聚类事务随后会触发将更新后的文档聚类结果导出到服务系统的事务.\n通知机制类似于活动数据库中的数据库触发器或事件, 但与数据库触发器不同的是, 通知不能用于维护数据库不变性.\n具体而言, 被触发的观察器是在与触发写入操作不同的事务中运行的.\n因此, 触发写入操作与观察器产生的写入操作并不是原子执行的.\n通知的设计目标是支持增量计算, 而不是维护数据完整性.\n这种在语义和设计目标上的差异, 使得观察器的行为比具有重叠触发器的复杂语义更容易理解.\nPercolator 应用通常只包含极少量的观察器.\n例如, Google 索引系统中大约只有 10 个观察器.\n每个观察器都会在工作进程二进制文件的 main() 函数中被显式构造, 因此可以清楚地知道当前哪些观察器处于活动状态.\n多个观察器可以同时观察同一列, 但我们刻意避免使用这一特性, 以便明确在写入某一特定列时究竟会运行哪个观察器.\n用户确实需要注意避免通知形成无限循环, 但 Percolator 并未提供自动防护机制.\n通常, 用户会通过合理设计观察器之间的调用关系来避免这种问题.\n我们提供一项明确的保证: 对于被观察列的每一次变更, 最多只会有一个观察器事务成功提交.\n但反过来并不成立: 对同一被观察列的多次写入, 可能只会触发观察器运行一次.\n我们将这一特性称为消息折叠.\n消息折叠通过将多个通知的处理成本合并, 从而避免重复计算.\n例如, 只需要定期重新处理 http://google.com, 而不必在每次发现指向它的新链接时都重新处理.\n为了实现上述通知语义, 每个被观察列都对应一个 “确认” 列.\n该列中存储的是观察器最近一次运行时的起始时间戳.\n当被观察列被写入时, Percolator 会启动一个事务来处理该通知.\n该事务会同时读取被观察列及其对应的确认列.\n如果被观察列是在上一次确认之后写入的, 则运行观察器, 并将确认列更新为当前事务的起始时间戳.\n否则, 说明观察器已经针对该变更运行过, 因此不会再次执行.\n需要注意的是, 如果 Percolator 意外地针对同一通知并发启动了两个事务, 它们都会看到该通知并尝试运行观察器.\n但其中一个事务会因为确认列上的冲突而中止.\n因此, 我们可以保证针对每个通知, 最多只有一个观察器事务能够成功提交.\n为了支持通知机制, Percolator 需要高效地定位需要运行观察器的脏单元格.\n由于通知数量相对稀少, 这一搜索过程变得颇具挑战性.\n系统中的表可能包含数万亿个单元格, 但在系统能够跟上负载的情况下, 通知数量通常只有数百万级别.\n此外, 观察器代码运行在分布于多台机器上的大量客户端进程中, 这意味着对脏单元格的搜索也必须以分布式方式进行.\n为此, Percolator 维护了一个特殊的 Bigtable 列 “notify”, 其中为每个脏单元格保存一条记录.\n当事务写入某个被观察的单元格时, 它也会同时设置对应的通知单元格.\n工作进程会对 notify 列执行分布式扫描, 以查找需要处理的脏单元格.\n在观察器被触发且相关事务成功提交后, 对应的通知单元格会被移除.\n由于 notify 列只是一个普通的 Bigtable 列, 而不是 Percolator 管理的事务列, 因此它不具备事务语义.\n它仅作为扫描提示, 用于引导系统去检查确认列, 从而判断是否需要运行观察器.\n为了进一步提高扫描效率, Percolator 将 notify 列存储在一个单独的 Bigtable 局部性组中.\n这样, 在扫描时只需要读取数百万个通知单元格, 而无需扫描数万亿个数据单元格.\n每个 Percolator 工作进程都会分配多个线程专门用于执行扫描操作.\n每个工作进程负责扫描表的一部分.\n它首先随机选择一个 Bigtable Tablet, 然后在该 Tablet 中随机选择一个键, 最后从该键的位置开始扫描表, 以此确定要扫描的区域.\n由于每个工作进程扫描的是表中的随机区域, 我们担心两个工作进程可能同时对同一行运行观察器.\n虽然由于通知的事务特性, 这种行为不会影响正确性, 但会显著降低效率.\n为避免这种情况, 每个工作进程在扫描行之前都会从一个轻量级锁服务获取锁.\n这个锁服务器不需要持久化状态, 因为它仅提供建议, 因此具有很高的可扩展性.\n随机扫描方法需要进行额外调整.\n在最初部署时, 我们注意到扫描线程倾向于聚集在表格的几个区域, 降低了扫描的并行性.\n这种现象类似公共交通系统中所谓的“编队行驶”或“公交车聚集”现象: 当公交车减速 (可能因交通拥堵或乘客上车缓慢) 时, 后续公交车会加速以弥补时间差, 导致多辆公交车同时到达同一站点.\n我们的扫描线程也出现了类似情况: 运行观察器的线程速度减慢, 而后续线程会快速跳过已清理的行, 与领先线程聚集在一起.\n由于线程聚集导致 Tablet 服务器过载, 后续线程无法超越前面线程.\n为解决这一问题, 我们改进了系统: 当扫描线程发现自己正在扫描另一线程的同一行时, 它会在表中随机选择一个新位置继续扫描.\n这种改进类似于公交车在过近时瞬移到随机站点, 以避免拥堵.\n最后, 基于通知的经验, 我们引入了一种更轻量级但语义较弱的通知机制.\n在并发处理同一页面的多个副本时, 每个事务都会尝试触发对同一重复集群的重新处理, 从而引发冲突.\n为避免事务冲突, 我们设计了弱通知机制: 事务仅写入 Bigtable 的 notify 列.\n为了保持 Percolator 其他部分的事务语义, 弱通知仅限于一种特殊类型的列, 该列不能写入, 只能用于通知.\n较弱的语义意味着单个弱通知可能导致多个观察者同时运行并提交, 但系统会尽量减少这种情况.\n这一机制有助于管理冲突; 如果某个观察者在热点上频繁发生冲突, 通常可以将其拆分为两个观察者, 并通过热点上的非事务通知连接起来以缓解冲突.\n#讨论\n相对于基于 MapReduce 的系统, Percolator 的一个效率瓶颈在于每个工作单元发送的 RPC 次数.\nMapReduce 只需对 GFS 执行一次大规模读取, 即可获取数十甚至数百个网页的所有数据.\n而 Percolator 在处理单个文档时, 需要执行约 50 次独立的 Bigtable 操作.\n在提交过程中会产生额外的 RPC 请求.\n写入锁时, 需要执行读-修改-写操作, 这涉及两次 Bigtable RPC 调用: 一次用于读取是否存在冲突的锁或写入操作, 另一次用于写入新锁.\n为降低开销, 我们修改了 Bigtable API, 添加条件变更功能, 从而在单个 RPC 中完成读-修改-写操作.\n发往同一 Tablet 服务器的多个条件变更操作也可以批量合并到单个 RPC 中, 以进一步减少 RPC 总数.\n我们通过将锁操作延迟几秒钟来形成批次, 从而将它们收集在一起.\n由于锁是并行获取的, 这只会增加每个事务几秒钟的延迟, 我们通过更高的并行度来弥补额外的延迟.\n批量处理也会增加潜在冲突窗口, 但在低竞争环境中, 这并未造成问题.\n在从表中读取数据时, 我们也采用相同的批处理策略.\n每次读取操作都会被延迟, 以便有机会与其他读取同一 Tablet 服务器的操作组成批次.\n这可能会增加每次读取的延迟, 从而显著增加事务延迟.\n然而, 最终的优化措施是预取.\n预取利用了这样一个事实: 读取同一行中的多个值与读取一个值的成本基本相同.\nBigtable 必须从文件系统中读取整个 SSTable 块并解压缩, 无论读取多少列.\nPercolator 会在读取每列时尝试预测事务随后将访问该行中的哪些其他列.\n预测基于历史访问行为.\n预取与已读取项缓存结合, 可将系统原本需要执行的 Bigtable 读取次数减少约 10 倍.\n在 Percolator 的早期实现阶段, 我们将所有 API 调用设置为阻塞式, 并依靠每台机器运行数千个线程以提供足够并行度, 从而保持良好的 CPU 利用率.\n我们选择 thread-per-request 模型主要是为了简化应用程序代码的编写.\n相比之下, 事件驱动模型虽然更高效, 但需要用户在每次 (或多次) 从表中获取数据项时打包状态, 这会大幅增加应用开发复杂度.\n总体而言, thread-per-request 模型带来的体验是积极的: 应用程序代码简洁, 在多核机器上实现了良好的资源利用率, 并且完整且有意义的堆栈跟踪简化了崩溃调试.\n我们在应用程序中遇到的竞态条件比预期要少.\n这种方法的主要缺点是 Linux 内核和谷歌基础设施在高线程数下存在可扩展性问题.\n我们的内核开发团队已经部署了修复程序, 以解决这些内核问题.\n#参考资料\n\nPercolator 和 TiDB 事务算法\nTiKV 事务模型概览, Google Spanner 开源实现\n\nPercolator 是 Google 的上一代分布式事务解决方案, 构建在 BigTable 之上, 在 Google 内部用于网页索引更新的业务. 原理比较简单, 总体来说就是一个经过优化的 2PC 的实现, 依赖一个单点的授时服务 TSO 来实现单调递增的事务编号生成, 提供 SI 的隔离级别.\n传统的分布式事务模型中, 一般都会有一个中央节点作为事务管理器, Percolator 的模型通过对于锁的优化, 去掉了单点的事务管理器的概念, 将整个事务模型中的单点局限于授时服务器上, 在生产环境中, 单点授时是可以接受的, 因为 TSO 的逻辑极其简单, 只需要保证对于每一个请求返回单调递增的 id 即可, 通过一些简单的优化手段 (比如 pipeline) 性能可以达到每秒生成百万 id 以上, 同时 TSO 本身的高可用方案也非常好做, 所以整个 Percolator 模型的分布式程度很高.\n\n\nOptimized Percolator in TiKV: 提到了对 Percolator 的几种优化, parallel prewrite, short value in write column 和 point read without timestamp Two-phase Commit\n分布式系统的时间\nTiDB Percolator 事务实现解析\n基于 KV 的分布式事务方案\n数据库内核月报 － 2018 / 11 – Database · 原理介绍 · Google Percolator 分布式事务实现原理解读\nPercolator 论文阅读笔记\nJepsen Test 关于Consistency Models的总结:\n![Consistency Models](Consistency Models.png)\n\n","categories":["distributed-system"],"tags":["分布式系统","Bigtable","Percolator"]},{"title":"[OSDI'06] Bigtable: A Distributed Storage System for Structured Data 论文阅读","url":"/2025/02/27/bigtable-a-distributed-storage-system/","content":"#0. 摘要\nBigtable 是一个分布式存储系统，用于管理结构化数据，能够扩展到巨大的规模：PB 级别的数据，数千台机器。\nGoogle 的很多项目都保存在 Bigtable 中，包括网页索引，Google Earth 和 Google Finance。\n#1. 引言\nBigtable 从很多方面像一个数据库：它与数据库有很多类似的策略。并行数据库，内存数据库已经实现了缩放性和高性能，但是 Bigtable 提供了不同的接口。\nBigtable 不支持完整的关系模型；取而代之，它提供一个简单的数据模型，还支持动态改变数据布局和格式。而且允许客户端对底层存储的数据的局部性进行推理。数据使用行和列名进行索引，行列是任意的字符串。\nBigtable 把数据视为字符串，需要客户端序列化和反序列化后使用。客户端能够通过细致地控制 schema 来控制数据的局部性。最后，Bigtable 的 schema 有参数控制能否在 OOM 的时候继续提供服务。\n#2. 数据模型\nBigtable 是一个稀疏、分布式、持久化的多维排序映射表。这个表是通过行键、列键和时间戳索引的。表中的值是字节串。\n(row:string, column:string, time:int64) -&gt; string\nGoogle 分析了很多潜在的应用场景，最终敲定了这个数据模型。\n#行键\n表格中的行键是任意的字符串，最大可以是 64KB，典型值是 10-100 字节。\nBigtable 会按照字典序排序行键。表中的所有行会被动态划分为更小的段，称为 Tablet，是数据分散和负载均衡的单元。客户端能够控制行键的格式，以便于控制数据的局部性。例如，在 Google 的 Webtable 中，行键是反转的 hostname，这样 maps.google.com/index.html 就会存储在 com.google.maps/index.html 的位置。\n#列族（Column Family, CF）\n表格中的列被分成若干组，称为列族，列族是基本的 ACL 单元。一个列族中的所有数据通常都是同一种类型（同一个列族的会放在一起压缩）。列族必须在表创建时指定，一旦写入数据之后就不能修改。列族数量按预期是很小的，最多几百个，而且很少会改变。相对的，列的数量没有上限。\n列键使用 family:qualifier 的形式命名。其中 family 必须是可打印的字符，qualifier 可以是任意的字节串。例如 Webtable 中的一个 CF 是 language，用于存储网页的语言。language 下只有一个列，存储语言 ID。另一个 CF 是 anchor，其中每一个列名是指向的站点的名字，值是链接文本。例如 Webtable 中的一条数据可能是这样的：\n\n\n\nRow Key\ncontents\nanchor:cnnsi.com\nanchor:my.look.ca\n\n\n\n\n“com.cnn.www”\n“&lt;html&gt;”\n“CNN”\n“CNN.com”\n\n\n\n访问控制和磁盘和内存统计都是在 CF 级别进行的。因此可以通过 CF 管理不同的应用程序：例如一部分只写入新的基本数据，一部分读基本数据然后创建衍生数据，一部分只读取已有的数据。\n#时间戳\nBigtable 中的每个单元都可以包含相同数据的多个版本，这些版本通过时间戳进行索引。\nBigtable 的时间戳是 64 位整数，这个时间戳可以由 Bigtable 自动生成（使用现实世界的毫秒），也可以由客户端提供。如果由客户端提供，需要保证并发写的时候，多个客户端的时间戳是不冲突的。\nBigtable 会按照时间戳的逆序存储数据，这样最新的数据会被最先读取。\n为了更容易地管理多版本数据，Bigtable 提供了两个 CF 级别的设置：允许 Bigtable 自动回收旧版本的数据。客户端可以选择保留多少个版本，或者只保留最新的版本。\n在 Webtable 中，contents 列存储的数据的时间戳是文档的实际爬取时间，而且设置了只保存最新 3 个版本。\n#3. API\n写入 Bigtable 的代码示例：\n// Open the tableTable *T = OpenOrDie(&quot;/bigtable/web/webtable&quot;);// Write a new anchor and delete an old anchorRowMutation r1(T, &quot;com.cnn.www&quot;);r1.Set(&quot;anchor:www.c-span.org&quot;, &quot;CNN&quot;);r1.Delete(&quot;anchor:www.abc.com&quot;);Operation op;Apply(&amp;op, &amp;r1);\n读取 Bigtable 的代码示例：\nScanner scanner(T);ScanStream *stream;stream = scanner.FetchColumnFamily(&quot;anchor&quot;);stream-&gt;SetReturnAllVersions();scanner.Lookup(&quot;com.cnn.www&quot;);for (; !stream-&gt;Done(); stream-&gt;Next()) &#123;    printf(&quot;%s %s %lld %s\\n&quot;,        scanner.RowName(),        stream-&gt;ColumnName(),        stream-&gt;MicroTimestamp(),        stream-&gt;Value());&#125;\nBigtable API 提供了这些方法和特性：\n\n修改集群\n创建和删除表\n创建、修改和删除 CF，修改 ACL\n读、写和扫描数据\n把一组操作打包成一个原子操作\n单行事务：可以对单行数据进行原子的 Read-Modify-Write 操作\n单元格作为计数器\n在服务器的地址空间中执行客户端提供的 Sawzall 脚本（PS. 类似 lua in redis）\nSawzall 的 API 不能写数据，但是可以进行一些复杂的数据变换、过滤和聚合操作。\n\nBigtable 还可以和 MapReduce 结合使用，Google 写了很多包装器，用于在 MapReduce 任务中读写 Bigtable。\n#4. 基本构建块\nBigtable 是建立在其他几个已有的 Google 系统之上的：\n\nGFS（Google File System）：用于存储日志和数据\n集群管理系统：调度任务，管理资源，处理机器故障，和监控集群状态\nChubby：分布式锁服务\n\nBigtable 使用了 Google SSTable 文件格式。SSTable 提供一个持久化的、有序的、不可变的 K-V 映射，其中 K 和 V 都是任意的字节串。SSTable 文件能提供的操作有：\n\n按 key 查找 value\n迭代特定范围内的所有 key-value 对\n\nSSTable 内部包含一个块序列（块大小可变，默认 64KB）。SSTable 文件末尾存储着块索引，用于定位块。当 SSTable 文件被打开时，块索引会被读入内存。每次查询操作只需要一次磁盘 seek 操作：首先在内存索引中二分查找到块的位置，然后读取该块即可。可选地，SSTable 还可以被完全映射到内存中，这样就不需要访问磁盘了。\nBigtable 还依赖于高可用和持久化地分布式锁服务 Chubby。\nChubby 服务包含 5 个副本，其中一个会被选举为 Master，依赖于 Paxos 算法保证副本数据一致性。Chubby 提供了一个由目录和文件组成的命名空间，每个目录或文件都可以用作锁，而且读写文件的操作是原子的。\nChubby 客户端库提供对 Chubby 文件的一致性缓存，每个客户端都维持着一个 session。如果客户端在超时时间内没有成功续约，则会过期，并失去所有的锁。\nChubby 客户端还可以注册 callback，当锁的状态发生变化时，这个 callback 会被调用。\nBigtable 用 Chubby 实现这些任务：\n\n保证任意时刻最多只有一个 Master；\n保存 Bigtable 的 bootstrap 位置；\n发现新 Tablet Server 的位置；处理 Tablet Server 的后事；\n存储 schema 信息：每个表的 CF；\n存储 ACL；\n\nBigtable 强依赖于 Chubby，如果 Chubby 不可用，Bigtable 也不可用。\nGoogle 衡量了由于 Chubby 问题导致的 Bigtable 不可用的平均时间，在 14 个 Bigtable 集群，使用 11 个 Chubby 实例，这个时间只占 0.0047%。受影响最严重的单个集群，这个时间也只占 0.00326%。\n#5. 实现\nBigtable 的实现包含三部分：链接到应用程序中的客户端库、一个 Master 进程和若干个 Tablet Server 进程。Tablet Server 可以动态地增加或删除。\nBigtable 的 Master 进程负责：\n\n分配 Tablet 到 Tablet Server\n检测 Tablet Server 的增删\n均衡 Tablet Server 的负载\n垃圾回收 GFS 中的文件\n处理 Schema 变更（例如表和 CF 的增删）。\n\nTablet Server 负责管理 Tablet，每个 Tablet Server 通常会管理 10 个到 1000 个 Tablet。\nTablet Server 还要负责处理客户端的读写请求，还有分裂过大的 Tablet。\n和许多单主节点的分布式存储系统（比如 GFS）一样，客户端数据是不会流过 Master 节点的，而是直接和 Tablet Server 交互。因为客户端不依赖于 Master 来获取 Tablet 位置信息，大多数客户端从来不会和 Master 通信。因此 Master 的任务是很轻松的。\n一个 Bigtable 集群存储多个表，每个表由一系列的 Tablet 组成，每个 Tablet 包含了一段行范围的数据。初始时，每个表只有一个 Tablet。随着表的增长，Tablet 会被分裂成更多更小的 Tablet，默认 Tablet 的大小是 100-200 MB。\n#5.1 Tablet 位置\nLevel 0  |         Chubby file         |              |         |              vLevel 1  |         Root Tablet (META1)         |        /   |    |   \\         |       v    v    v    vLevel 2  |   META2 META3 META4 ...         |    /|\\   /|\\   /|\\  ...         |    vvv   vvv   vvv  ...Level 3  | USER1..  ...   ...  ...\nBigtable 使用了三级层次结构来存储 Tablet 位置信息。第 0 级是一个保存于 Chubby 中的文件，保存着 Root Tablet 的位置信息。\nRoot Tablet 是 METADATA 表的第一个 Tablet，而且不会被分裂。\nMETADATA 表是整个系统的元数据表，其中保存着所有其他表的 Tablet 位置信息。其行键是表名和末尾行的编码，值是 Tablet 的位置信息。\nMETADATA 中的每一行数据大约占用 1KB。按照保守估计，每个 Tablet 大小 128MB 的话，三级的 METADATA 表可以存储 2^34 个 Tablet 的位置。\n客户端库会缓存 Tablet 的位置信息。如果客户端没有缓存，或者发现缓存的位置信息不正确，那么它就会递归地到上一级查找。按照这种算法，如果当前没有缓存，定位需要 3 个 RT，加上 1 次 Chubby 读。如果缓存不对，那么最多需要 6 个 RT，加上 1 次 Chubby 读。虽然 Tablet 位置是保存在内存中的，即不需要访问 GFS，但是这种定位方法还是很慢。为了加速一般情况，Bigtable 会让客户端预取 Tablet 位置信息，每次读的时候都多读几个 Tablet 的位置信息。\n为方便 debug，METADATA 表中还会存储每个 Tablet 的时间日志。\n#5.2 Tablet 分配\n每个 Tablet 最多只会分配给一个 Tablet Server。\nMaster 负责记录活着的 Tablet Server，以及 Tablet 到 Tablet Server 的映射关系。如果一个 Tablet 还没有分配，而且某个 Tablet Server 有空闲空间，那么 Master 就会向这个 Tablet Server 发送一个 Tablet load 请求，把这个 Tablet 分配给他。\nBigtable 使用 Chubby 追踪 Tablet Server 的状态。当一个 Tablet Server 启动时，它会在 Chubby 的特定目录下创建一个名字唯一的文件（Server file），并且获取文件的排他锁。Master 会监听这个目录来发现新的 Tablet Server。如果一个 Tablet Server 丢失了这个排它锁，那么它就不再对外提供服务。它会先检查 Server file 是否存在，如果存在就尝试重新获取锁，否则就会直接退出。当 Tablet Server 结束时，它会释放锁，就会被 Master 拿到。\nMaster 需要周期性地监测 Tablet Server 是否在正常工作，如果不是，就要尽快把它负责的 Tablet 分配给其他 Tablet Server。为此，Master 会定期询问 Tablet Server 是否还持有 Server file 的锁，如果没有，或者多次尝试都超时，Master 就会尝试拿一下这个 Tablet 的锁。如果拿到了，说明 Chubby 服务正常，Tablet Server 挂了，就会把这个 Server file 删掉，让 Tablet Server 自行退出；然后把它负责的 Tablet 添加到未分配集合中。为了确保 Bigtable 集群不会受 Master 和 Chubby 之间网络的影响，Master 的 Chubby session 过期，就会直接退出。\n当一个新 Master 被集群管理系统拉起的时候，它需要先检查有哪些 Tablet，按照如下步骤：\n\nMaster 获取一个 Master lock；\n扫描 Server file 目录，找到所有活着的 Tablet Server；\n与每一个 Tablet Server 通信，询问它们负责的 Tablet 列表；\n扫描 METADATA 表，找到所有的 Tablet：此时就可以找到哪些 Tablet 还未分配。\n\n一个小问题是如果 METADATA Tablet 还没有分配的话，Master 就没办法找到 METADATA 表的位置，那步骤 4 就无法进行。因此在步骤 4 之前，Master 会额外检查一下 Root Tablet 有没有分配，如果没有分配，就会额外把 Root Tablet 添加到未分配集合中，最终 Root Tablet 会被分配给空闲的 Tablet Server。\n已有 Tablet 的集合只有当以下时候才会变化：\n\n创建或删除表；\nTablet 合并：两个已有的 Tablet 合并成一个大的 Tablet；\nTablet 分裂：一个 Tablet 被分裂成两个更小的 Tablet；\n\n情况 1 和 2 都是由 Master 初始化的，Master 自然能够追踪这些变更；情况 3 是由 Tablet Server 完成的，Tablet Server 会把分裂出来的新 Tablet 信息记录在 METADATA 表中，然后通知 Master。即使这条通知消息丢失了，当 Master 要求 Tablet Server 加载分裂出来的 Tablet 时，Master 也会找到这个新 Tablet。\n#5.3 Tablet 服务\nTablet 的持久化状态是保存在 GFS 中的。更新操作会被记录成操作日志（Commit log），其中保存着 redo 记录。在这些更新中，最近提交的更新会被保存在内存中的一个称为 Memtable 的有序数据结构中。为了恢复一个 Tablet，Tablet Server 需要先从 METADATA 表中找到这个 Tablet 的元数据，包括组成这个 Tablet 的一系列 SSTable 文件，还有一系列 redo 点，即指向操作日志的指针。\nTablet Server 会把这些 SSTable 文件的索引加载到内存中，然后通过应用自 redo 点开始的所有更新来重建 Memtable。\n写操作：\n\nServer 检查是否合法，是否有权限（通过读取 Chubby 中的 ACL）\n有效的变更会被写入操作日志：会分批写入提高吞吐量\n变更会被写入 Memtable\n\n读操作：\n\nServer 检查是否合法，是否有权限（通过读取 Chubby 中的 ACL）\n在由一系列 SSTable 和 Memtable 组成的数据视图中查找数据\n\nTablet 分裂和合并不影响读写操作。\n#5.4 数据回收\nMinor compaction：写入数据的时候，Memtable 会增大，当增加到一定程度时，就会被冻结，然后转换成 SSTable 写入 GFS。与此同时，一个新的 Memtable 会被创建。\nMinor compaction 的目的是：\n\n减小内存占用\n减少重启时的恢复时间\n\nMinor compaction 不影响读写操作。\n每次 Minor compaction 都会创建一个新的 SSTable。如果一直增加下去，读的时候就需要合并很多 SSTable。因此 Bigtable 会定期在后台执行 Merging compaction，也叫 Major compaction。\nMajor compaction 会把多个 SSTable + Memtable 合并成一个 SSTable。合并的时候，会把被删除的数据去掉。\nMajor compaction 的目的是：\n\n回收删除数据占用的空间\n保证被删除的数据能够及时消失（对于敏感数据）\n\n#6 细化\n为了实现高性能，高可用，高可靠性，Bigtable 实现上还做了很多细节优化。\n#局部性群组\n客户端可以把多个 CF 设置成一个局部性群组。在每个 Tablet 中，每个局部性群组都会有一个单独的 SSTable。将不经常一起访问的 CF 划分成多个局部性群组，能够提高读性能。例如，Webtable 中的页面元数据（例如 langauge 和 chunksums）可以放进一个局部性群组，而网页内容可以放进另一个局部性群组：因为一个读元数据的应用程序往往不会读取内容。\n此外，局部性群组有一些有用的调优参数，例如，局部性群组可以被声明为 in-memory。这样的话，这个局部性群组的 SSTable 在用到时就会被加载到内存中，在读取这个局部性群组内的 CF 时就不用访问磁盘了。这个特性对于频繁访问的小块数据很有用：METADATA 表中的 location CF 就是一个 in-memory 的局部性群组。\n#压缩\n客户端可以控制一个局部性群组的 SSTable 是否压缩，以及使用什么算法。压缩的粒度是 SSTable 中的块，而不是整个 SSTable，这样读的时候就不用解压整个 SSTable。很多客户端会使用一个两趟压缩算法，第一趟用 Bentley-McIlroy 模式，用一个长窗口压缩长的重复串，第二趟用一个快速的算法，在 16KB 的小窗口内压缩剩下的数据。这种两趟压缩算法是很快的，在现代机器上，压缩速度可以达到 100-200MB/s，解压速度可以达到 400-1000MB/s。\n虽然我们在选择压缩算法的时候更倾向于压缩速度而不是压缩率。这种两趟压缩算法的压缩率也不错。在 Webtable 中进行测试，压缩 html 文档，每个文档只保存一个版本，两趟压缩算法的压缩率能达到 10:1，比 gzip 的 3:1 或者 4:1 好的多。如果保存多个版本，压缩率会更高。\n#缓存以提升读性能\n为了提升读性能，Tablet Server 使用了两级缓存。\n\nScan 缓存是 high-level 的缓存，缓存 SSTable 返回的 kv 对。\nBlock 缓存是 low-level 的缓存，缓存从 GFS 中读出来的 SSTable 块。\n\n#布隆过滤器\n读操作必须要从所有的 SSTable 中查找数据，如果 SSTable 不在内存中，就会需要多次磁盘访问。\nBigtable 允许客户端指定创建一个布隆过滤器，用于快速判断一个 key 是否在 SSTable 中。\n#提交日志实现\n如果我们把每个 Tablet 的提交日志都放在单独的 log 文件中，就需要往 GFS 里面写大量的文件。再加上 GFS 实现，可能导致大量磁盘访问。另外，用多个 log 文件也不容易做 group commit 优化。因此 Bigtable 设计成每个 Tablet Server 使用一个 log 文件。\n使用单个 log 文件提供了正常路径下的性能优势，但是会让恢复变得麻烦。当 Tablet Server 挂了，它负责的 Tablet 需要被迁移走，而每个接管的 Server 通常只会分到其中一部分。为了进行恢复，每个 Server 都需要读取一遍完整的 log 文件，会导致巨量的读放大。\n为了避免这种读放大，首先，Bigtable 会按照 &lt;table, row name, log sequence number&gt; 顺序排序 log 文件。排序后，对于单个 Tablet 的变更就是连续存储的，因此新 owner 只需要进行一次 seek 加顺序读就可以了。为了并行化这种排序过程，log 文件会按照 64MB 大小分段，然后分散在不同的 Tablet Server 上并行排序。这种排序过程是由 Master 协调的。当任意 Tablet Server 需要从 log 文件恢复 Tablet 的时候，就会开始这种排序过程。\n由于各种各样的原因，有时往 GFS 上写日志的时候，会有性能抖动。例如 GFS 正在故障恢复，或者网络拥塞，或者单纯就是负载太重。为了避免 GFS 延迟尖峰，每个 Tablet Server 实际上会有两个写 log 的线程，每个线程写自己的日志文件，同一时刻只会有一个线程被使用。如果写入的性能太差，就会切换到另一个线程。队列里面后续的变更也会由另一个线程来处理。日志条目中包含了自增序列号以防止线程切换的时候重复应用相同的日志。\n#加速 Tablet 恢复\n如果 Master 要把一个 Tablet 从一个 Server 迁移到另一个，源 Server 会先对这个 Tablet 进行一次 Minor compaction。这次 compaction 能够缩短恢复时间，因为需要恢复的 log 变少了。在 compaction 之后，源 Tablet Server 就会停止服务这个 Tablet。在真正卸载这个 Tablet 之前，Server 会再做一次（通常是很快的）Minor compaction，目的是消除第一次 compaction 期间收到的变更。第二次 compaction 之后，就没有剩余的 log 了，目标 Server 只需加载 Tablet 就可以了。\n#利用不可变性\n除了 SSTable 缓存以外，Bigtable 系统的各种其他部分也利用了 SSTable 文件的不可变性。比如，并发读 SSTable 的时候不需要任何同步机制。因此行之间的并发控制就可以实现得非常高效。唯一的例外是 Memtable。为了减少 Memtable 的读争抢，我们让每个 Memtable 的行都是 CoW 的，这样读和写就可以并行进行。\n因为 SSTable 是不可变的，所以移除已经删除的数据就转换成了垃圾回收过时 SSTable 的问题。每个 Tablet 的 SSTable 都被注册到 METADATA 表中。Master 在 SSTable 文件集合上使用标记-清除算法来回收过时的 SSTable。\nSSTable 的不可变性还允许我们快速地分裂 Tablet。相对于给每个 Tablet 生成一组新的 SSTable 文件，Bigtable 让子 Tablet 共享父 Tablet 的 SSTable 文件。\n#7 性能评估\n评估实验使用了一个 N 个 Tablet Server 的 Bigtable 集群。用来评估性能和可拓展性随 N 的变化。每个 Tablet Server 使用 1GB 内存，写入一个 1,786 机器的 GFS 集群，每个 GFS 节点配置两块 400GB 的 IDE 硬盘。还有 N 个客户端用于生成读写负载。N 个客户端相对于 N 个 Tablet Server 是非常充足的，这样可以保证客户端的负载不会成为瓶颈。每个机器有两颗双核 Opteron 2GHz 的 CPU。\n（略）\n#8 现实世界的应用\n截至 2006 年 8 月，Google 内部有 388 个生产 Bigtable 集群，一共有 24,500 个 Tablet Server。其中大部分都是供开发使用，因此经常是闲置的。大部分集群都有很少的机器数。有 14 个忙的集群，一共由 8069 个 Tablet Server 组成，服务了 1.2M 的 QPS，进的 RPC 流量大约是 741MB/s，出的是 16GB/s。\n#8.1 Google Analytics\nGoogle Analytics 用于帮助网站所有者分析流量模式。包括聚合统计，例如每天的 UV，每个 URL 的 PV，还有支付的用户比例。为了启用这个功能，网站所有者需要在网站上插入一个 JavaScript 代码片段。每次页面被访问的时候，这个代码片段就会执行。这个代码会记录各种各样的信息，例如用户 id 和页面的信息，发送到 Google Analytics。\nGoogle Analytics 总结这些信息，然后提供给网站所有者。\nGoogle Analytics 使用的 Bigtable 表包括：\n\nraw click 表：~200TB，每个用户会话一行，行名字是网站名+Session 创建时间。这样保证了访问相同站点的用户会话会被存储在一起，而且是按照时间排序的。这个表的压缩率是 14%。\nsummary 表：~20TB，包含每个网站的各种聚合统计信息。这个表是被周期调度的 MapReduce 任务从 raw click 表中生成的。每个 MR 任务从 raw click 表中提取最近的会话数据。这个表的压缩率是 29%。\n\n#8.2 Google Earth\nGoogle 运营着一系列服务，提供地球表面高解析度的卫星图像。这个服务既通过基于 web 的 Google Maps 提供，也通过 Google Earth 软件客户端提供。\nGoogle Earth 的预处理流水线使用了一个 Bigtable 表存储原始图像数据。在预处理过程中，图像被清理和合并，形成最终提供服务的图像。这个表包含了约 70TB 的数据，因此是存储在磁盘中的。这些图像已经被有效地压缩过了，因此 Bigtable 的压缩被关闭了。\n图像表的每一行对应一个地理区域，行名字保证了相邻区域的行是相邻的。这个表包含了一个 CF，用于追踪每个区域的数据来源。这个 CF 有很多列：基本上每个原始图像都有一个列。因为每个区域的图像是从很少几张原始图像中合成的，这个 CF 非常稀疏。\n预处理流水线重度依赖 MapReduce over Bigtable 来处理图像数据。\n服务系统使用一个表来索引 GFS 中的数据。这个表相对较小，只有 ~500GB，但是要保证每个数据中心能低延迟地提供上万的 QPS。因此这个表跨越了数百个 Tablet Server，还开启了 in-memory 选项。\n#8.3 个性化搜索\nPersonalized Search 是一个 Google 的实验性服务，记录用户的查询和点击行为，跨越 Google 的多种服务，例如搜索，图片和新闻。用户可以浏览搜索历史来找到他们之前搜索和点击过的内容，然后他们可以寻求个性化的搜索结果，根据他们的历史行为。\n个性化搜索把每个用户的数据保存在 Bigtable 中。每个用户有一个唯一的 userid，和一个对应的行。所有用户的行为都会被记录在这个行中。每种行为都有一个单独的 CF 保存（例如有一个 CF 保存搜索历史）。数据元素是行为发生的时间戳。个性化搜索用 MapReduce over Bigtable 来生成用户的资料。\n个性化搜索数据在多个 Bigtable 集群之间复制，以增加可用性和减少延迟。个性化搜索团队原本在 Bigtable 之上实现了一个自己的复制系统，但是后来改成了使用 Bigtable 的复制功能。\n个性化搜索的存储系统允许其他组在他们的列中添加每个用户的信息，然后这个系统也被很多其他的，需要为每个用户保存数据的 Google 业务使用。在很多组之间共享一个表导致了不寻常的 CF 数量。为了解决这个问题，Bigtable 增加了一个简单的 Quota 机制来限制单个客户端在共享表中的空间消耗，提供了一定的隔离性。\n#FAQ\n#1. GFS 可能出现重复记录或者 padding，Bigtable 如何处理这种情况？\nBigtable 写入 GFS 的数据分为两种：\n\n操作日志，当 Tablet Server 发生故障时，它上面服务的 Tablet 会被迁移到集群中的其他 Tablet Server 上继续提供服务，加载 Tablet 可能需要回放操作日志，每条操作日志唯一的序号，通过它可以去除重复的操作日志。\n每个 Tablet 包含的 SSTable 数据，如果写入 GFS 失败可以重试并产生多条重复记录，但是 Bigtable 只会索引最后一条写入成功的记录。\n\n#2. 如何保证同一个 Tablet 不会被多台机器同时服务？\n利用 Chubby 保证互斥性。\n#参考资料\n\nTiKV 事务模型概览，Google Spanner 开源实现\n\n最近 Google 基础设施的神人 Jeff Dean 在一次采访中回顾自己作为工程师最大的后悔是什么的问题时提到，他最后悔的事情是没有在 BigTable 中加入跨行事务模型，以至于后来各种各样的团队尝试在 BigTable 上不停的造事务的轮子，但其实这个特性应该是由 BigTable 提供。同样的观点也在他后来的论文中反复提到过。\n\n\n\n","categories":["distributed-system"],"tags":["分布式系统","Bigtable"]},{"title":"MIT 6.824/6.5840 分布式系统 Lab 1: MapReduce","url":"/2024/03/25/mit-6.824-distributed-system-lab1/","content":"本次 Lab 1 讲的是分布式计算的 MapReduce。\n#1. 背景\nMapReduce 源自 Google 在 OSDI '04 发表的论文，论文的作者是 Jeff Dean 和 Sanjay。主要提出了一种简单、可拓展的分布式计算编程模型。\n对于用户来说，只需要实现 Map() 和 Reduce() 两个函数，即可享受到分布式计算的好处，这就是 MapReduce 的设计理念。\n#思想\nMapReduce 需要用户将计算过程表达成以下两个函数：\n\nMap(key, value) -&gt; list(ikey, ivalue)\nReduce(ikey, list(ivalue)) -&gt; list(ivalue)\n\n到这里大家可能已经感受到一点并行的思想，其实 MapReduce 是把一个任务分解成一系列子任务，每个子任务由 Map 函数去执行。这些子任务的输出会被重组，然后分门别类送到 Reduce 函数去执行。\nMapReduce 的 Motivation 是：\n\n很多大数据计算任务都是大量输入对应到小量的输出；\n分而治之，一个复杂计算任务往往可以分成多个更简单的部分；\n子任务之间往往可以并行执行； -&gt; Map\n\n怎么区分”多个更简单的部分”？使用ikey标示子任务。\n#2. MapReduce 程序示例\n#Word Count\nWord count任务是统计一批文本中每个词出现的数量\nMap任务：统计一个分块中的词频\nReduce任务：合并所有分块的词频\n#Grep\nMap任务：对一个分块做grep\nReduce任务：合并所有的grep结果\n#计算图的逆\n已知所有的&lt;source, target&gt;边，计算每个target被哪些source引用\nMap任务：逆转一组边，得到&lt;target, source&gt;\nReduce任务：按target合并所有的source\n#分布式排序\nMap任务：原样输出\nReduce任务：原样输出\n#3. 框架实现\nMapReduce 框架负责调度和执行用户编写的 Map 和 Reduce 任务。\n根据环境（机器配置，网络拓扑，故障概率，和存储成本）不同，可以有很多不同的实现方案\n#概览\n框架首先将输入数据分成MMM块，然后对每一块分别执行map过程，这个过程是在多个机器上并行执行的；\n在map全部完成后，将中间结果按照某些策略分成RRR块，然后并行执行reduce过程；\n#主控节点数据结构\n\n任务状态：任务类型（Map or Reduce），执行状态（进行中 or 已完成），所在的机器\n数据状态：中间结果的存储位置和大小\n\n#分布式容错设计\nWorker节点的保障：Master节点会周期性地ping Worker节点；失联Worker节点已完成的任务会重新调度。\nMaster节点的保障：Master节点会周期性做checkpoint方便故障时重新开始。\n出错时的语义保证：需要实现Map/Reduce任务提交是原子性的\n\nMap任务通过提交临时文件名，由master做改名实现。\nReduce任务完成时会把工作临时文件改名为目标文件。\n\n#程序局部性\n分析局部性主要是考虑到带宽资源的限制。\n论文中实现的 MapReduce 框架是建立在 GFS (Google File System) 上的，GFS 会对文件进行分块然后保存三份副本。\n作者在实现时考虑到了底层GFS的影响，会尽量调度任务到有副本或者距离副本近的机器上。\n#任务划分粒度\n首先，输入数据会被划分成 MMM 个分块调用 Map，然后中间结果会被划分成 RRR 个分块调用 Reduce。\n理想情况下，MMM 和 RRR 应该远远大于 Worker 机器数量：利于负载均衡、故障恢复、和任务均匀分发；\n取值上界：受限于主控节点的Memory，空间复杂度=O(M×R)O(M\\times R)O(M×R)\n实践经验：RRR 影响中间结果文件的数量\nGoogle经验：2,000台Worker机器；M=200,000M=200,000M=200,000；R=5,000R=5,000R=5,000\n#备用任务\n一般来说，MapReduce框架计算的整体速度受制于少数“掉队者”，掉队的原因有很多，例如机器故障；\n当整个计算过程将要完成时，主控节点会将剩余的任务多安排一次（备用任务）。\n#4. 一些细节优化\n#分区函数\n提供给用户的分区函数，方便开发用；\n#数据处理顺序\nMapReduce框架保证同一个分区内，数据是按照key有序处理的，方便用户通过key访问结果文件；\n#合并器 Combiner\n在一些任务中，中间结果划分不是均匀的，可能会导致大量中间结果分配到同一个Reduce任务去，打满网络带宽。\n例如Word count任务，会有大量的&lt;the, 1&gt;记录；\n为此MapReduce框架设计了一个可选的 Combiner 函数，在 Map 之后先对结果进行一次合并，再通过网络发出去；\n#输入输出类型\nMapReduce框架是支持多种输入文件格式，也支持用户自己实现一个reader接口。\n#副作用\n有时候用户在处理数据时可能想要一些辅助文件，这些是通过统一的一层Application Writer完成，这一层会处理原子更新文件（先写到临时文件然后再改名）之类的细节；\n对于多文件的情况，没有提供原子的两阶段提交，这一点需要用户自己保证；\n#跳过坏记录\n有时候用户的Map，Reduce可能会由于bug崩溃，MapReduce框架提供了一种可选的执行模式，可以自动检测确定性的崩溃数据，然后跳过这些记录；\n框架每次调度任务时会附带一个Sequence Number。每个Worker进程都会注册Signal Handler，捕获SegFault和BusError。当程序崩溃时，框架会给主控节点发一个&quot;Last gasp&quot;UDP数据包带上这个Seq Number。当框架检测到某一个记录崩溃超过一次，下次就会跳过。\n#本地执行\n为了方便用户Debug，框架提供了一种本地执行模式。\n#状态信息\n主控节点跑了一个HTTP服务，暴露内部的一些状态信息。\n#计数器\n框架提供了一个Counter对象，方便用户实现一些计数需求（可能是为了san check）\n#5. 性能测试\n#6. 实践经验\n#大规模索引任务\n收益巨大：\n\n简化业务逻辑：索引代码更简单、更小、更容易理解，因为处理容错、分布和并行化的代码隐藏在 MapReduce 库中。\n关注点分离：MapReduce 库的性能足够好，我们可以将概念上不相关的计算分开，而不是将它们混合在一起以避免对数据进行额外的传递。这使得更改索引过程变得容易。\n简化横向拓展：机器、网络导致的问题被框架处理了，使得横向拓展更简单，只需要加机器即可。\n\n#回到 Lab 1\nLab 1 是实现 MapReduce 框架的两个组件 Coordinator（Master）和 Worker。\n\nMaster 负责分发任务给 Worker，会启动一个 RPC Server；\nWorker 从 Master 那里获取 Map 或 Reduce 任务并执行。\n\n整个 MapReduce 框架包括：\n\nmain/mrcoordinator.go 是 Master 的入口，会调用 MakeCoordinator() 然后等待；\nmain/mrworker.go 是 Worker 的入口，会加载用户程序（so库）获得 Mapf 和 Reducef，然后调用 Worker；\nmr/coordinator.go 实现 Master 的逻辑；\nmr/worker.go 实现 Worker 的逻辑；\n用户程序会被编译成 go plugin，被框架加载然后调用。\n\n","categories":["distributed-system"],"tags":["分布式系统","MapReduce","大数据"]},{"title":"MIT 6.824/6.5840 分布式系统 Lab 2","url":"/2025/02/20/mit-6.824-distributed-system-lab2/","content":"#Lab 2: Key/Value Server\n新版本 6.824 课程添加了一个 Lab 2，实现一个请求幂等的简单 KV 存储服务，还有对应的 Client 端。\n幂等性：题目依赖的底层 RPC 网络是不可靠的。为此，在请求中携带一个递增的 Sequence Number，在服务端忽略每个 Client 的旧 Request 即可。\n服务端实现：维护一个 map[int]int，记录每一个 Client 发来的最大的 Sequence Number；注意 Append 命令还需要缓存执行的结果。\ntype Last struct &#123;    Seq   int    Value string&#125;type KVServer struct &#123;    last  map[int]Last&#125;\nGet 命令无需考虑幂等性，直接返回即可：\nfunc (kv *KVServer) Get(args *GetArgs, reply *GetReply) &#123;    kv.mu.Lock()    defer kv.mu.Unlock()    reply.Value = kv.store[args.Key]&#125;\nPut 命令需要保证幂等性：\nfunc (kv *KVServer) Put(args *PutAppendArgs, reply *PutAppendReply) &#123;    // log.Println(&quot;Put&quot;, args)    kv.mu.Lock()    defer kv.mu.Unlock()    if last, ok := kv.last[args.ClientId]; ok &#123;        if args.Seq &lt; last.Seq &#123;            panic(&quot;seq &lt; last&quot;)        &#125;        if args.Seq == last.Seq &#123;            return        &#125;    &#125;    kv.last[args.ClientId] = Last&#123;Seq: args.Seq&#125;    kv.store[args.Key] = args.Value&#125;\nAppend 命令不仅需要保证幂等性，还需要缓存执行的结果：\nfunc (kv *KVServer) Append(args *PutAppendArgs, reply *PutAppendReply) &#123;    // log.Println(&quot;Append&quot;, args)    kv.mu.Lock()    defer kv.mu.Unlock()    if last, ok := kv.last[args.ClientId]; ok &#123;        if args.Seq &lt; last.Seq &#123;            panic(&quot;seq &lt; last&quot;)        &#125;        if args.Seq == last.Seq &#123;            reply.Value = last.Value            return        &#125;    &#125;    // Reply with the old value    reply.Value = kv.store[args.Key]    kv.last[args.ClientId] = Last&#123;Seq: args.Seq, Value: kv.store[args.Key]&#125;    kv.store[args.Key] += args.Value&#125;\n","categories":["distributed-system"],"tags":["分布式系统","Raft"]},{"title":"MIT 6.824/6.5840 分布式系统 Lab 3 - Raft 论文阅读","url":"/2025/02/20/mit-6.824-distributed-system-lab3-1/","content":"Raft 是一种经典的分布式共识算法（Consensus algorithm）。\n#复制状态机\n复制状态机是一组服务器上，计算相同的状态，而且可以容忍部分机器下线的状态机。复制状态机是解决分布式系统中一系列容错问题的基础，例如选主。通常通过 Replicated Log 实现，共识算法的职责就是维护 Replicated Log 的一致性。\n实际应用中，对于共识算法的要求：\n\n正确性：在非拜占庭条件下，永远不会返回错误的结果；\n可用性：只要多数机器正常工作，就可以正常工作；\n一致性不依赖于操作的时序，最坏情况下，只会导致可用性的问题；\n快速：一般情况下，一旦大多数机器响应了一轮 RPC，命令即可完成；\n\n#Paxos 的问题\n\n难以理解\n\n#Raft 共识算法\nRaft 将共识问题分解成三个子问题：（1）选主、（2）日志复制、（3）安全性；\n#Raft 基本定义\nRaft 规定的节点状态：\n\nFollower：被动响应 C 和 L 的请求；\nLeader：处理所有客户端的请求；\nCandidate：选主过程中使用；\n\nRaft 将时间划分为任意长度的任期（Term），任期用连续整数编号，起到逻辑时钟的功能。每个任期开始时先进行选举，选举出来的 Leader 会在这个任期内进行服务；否则继续下一个任期和选举。\n#选主\nRaft 利用心跳机制触发选主，Leader 会周期性地给所有 Follower 发送心跳宣布权威性。如果 Follower 一段时间没有收到心跳，则会开始尝试发起选主。\n选主过程中，Follower 会将 Term++，然后请求所有 Peer 投票（自己当然会给自己投票）。结果可能有几种：\n\n收到的选票数量超过半数：自己成为 Leader；\n收到了 Term 更大的 Candidate 或者 Leader 的任意 RPC：自己被强制降级为 Follower，发起的选举失效；\n收到的选票数量不到半数：没有 Leader，继续下一轮选举；\n\n节点投票时遵循简单的规则：\n\n每个 Term 最多投给一个人\n先来后到\n\nRaft 采用随机选举超时，保证多个节点不会在同一时间启动选举。\n#日志复制\n当一个 Leader 被选出来之后，就开始负责这些事情：\n\n给每个 Follower 发送日志：同步 Log；\n决定什么时候可以提交 Log：当 Leader 确信一条日志已经被同步到多数机器上，则认为可以提交它，Raft 机制保证了剩余机器最终将会提交相同的日志，即所谓最终一致性。\n\nRaft 的Log Matching Property：\n\n两个不同 Log 中的条目，如果 term 和 index 相同，则存储的 command 相同；\n两个不同 Log 中的条目，如果 term 和 index 相同，则之前的所有 log 相同；\n\n在 Raft 执行过程中，由于机器崩溃重启，Follower 可能会缺失部分 Log，可能会有多的未提交的 Log，也可能两者兼有；这种情况下，Leader 会覆盖Follower 上不一致的 Log：首先两者会找到第一条不一致的 Log，然后 Leader 发送后续的 Log 给 Follower。nextIndex就是 Leader 用于记录每一个 Follower 同步位置的变量，初始值为 Leader 的 nextLogIndex。然后会根据AppendEntries的结果减小nextIndex。\n一个可能的优化点是：Follower 反馈更加精确的冲突日志数量。作者怀疑这种策略在现实情况中的必要性。\nRaft 的Leader Append-Only Property：Raft Leader 从来不需要更改或者删除自己的日志；\n#安全性\n上面描述的 Raft 不足以保证命令的顺序性，原因是多个日志冲突的 Follower 可能会交替成为 Leader，导致已经提交的 Log 被覆盖。为此还需要增加一个新的限制：\n\nLeader Completeness Property：Candidate 必须拥有前一个 Term 中所有已经提交的Log，才能被选为 Leader；\n\n#选主限制\nRaft 利用选主过程保证Leader Completeness Property，RequestVote RPC 请求中会包含 Candidate 的 Log 信息，如果 Log 不够新，则在投篇时会被多数节点拒绝。\nRaft 定义的“新（Up-to-date）”：\n\n如果最后一条 Log 的 Term 不同，Term 大的更新\n否则，Log 长的更新\n\n#提交前一个 Term 的 Log\n只是 Log 被同步到多数节点，仍然不足以能够提交这个 Log。\nRaft 要求：只有当前 Term 的 Log 才通过多数原则确认提交，非当前 Term 的不会这样。\n#安全证明\n略\n#Follower 和 Candidate 崩溃重启\n略\n#时序和可用性\n略\n#集群成员变更 &amp; 联合共识\n集群变更配置的场景是很常见的。\nTODO\n#日志压缩\n","categories":["distributed-system"],"tags":["分布式系统","Raft"]},{"title":"MIT 6.824/6.5840 分布式系统 Lab 3: Raft 实现","url":"/2025/02/20/mit-6.824-distributed-system-lab3-2/","content":"#基本框架\n在 6.824 中，一个 Raft 应用程序的基本框架分为两层，分别是：\n\nRaft 层：处理 Raft 算法逻辑，作为底层框架，负责节点间协商；\nService 层：负责业务逻辑，对外提供用户服务；\n\nService 层需要将业务逻辑表示成状态机和对状态机的修改操作，然后将需要进行的操作发送到 Raft 层，当操作被 Raft 集群多数节点确认后，再从 Raft 层接收达成共识的操作，并按照操作修改状态机；\n状态机：由于 Raft 有 Snapshot 的设计，因此需要业务逻辑层能够回滚到任何一个状态。\n#基本设施\ntype Raft struct &#123;    mu        sync.RWMutex        // Lock to protect shared access to this peer&#x27;s state    peers     []*labrpc.ClientEnd // RPC end points of all peers    persister *Persister          // Object to hold this peer&#x27;s persisted state    me        int                 // this peer&#x27;s index into peers[]    dead      int32               // set by Kill()    applyCh   chan ApplyMsg    // Your data here (3A, 3B, 3C).    leader         bool    electionTimer  *time.Timer    heartbeatTimer *time.Timer    commitCond     *sync.Cond    replicateCond  []*sync.Cond    currentTerm int    votedFor    int    // log[0] is dummy, log[0].Command is the logStartIndex    // log[i] is the (logStartIndex+i)-th log entry, numbered from 1    log []LogEntry    // Volatile state on all servers    commitIndex int    lastApplied int    // Volatile state on leaders    nextIndex  []int    matchIndex []int&#125;\n论文提及的字段：\n\ncurrentTerm int – 记录当前的 Term\nvotedFor int – 选主时使用，记录给谁投了票\nlog []LogEntry – Raft 的核心 Log 序列\n\n可变状态：\n\ncommitIndex int – 已经达成共识的 Log 序号\nlastApplied int – 提交给上层的最后一个 Log 序号\n\n作为 Leader 时需要额外维护的状态：\n\nnextIndex []int – 每个 Follower 的 nextIndex\nmatchIndex []int – 每个 Follower 的 matchIndex\n\n我在实现时添加的额外状态：\n\nleader         bool – 记录自己是不是 Leader\nelectionTimer  *time.Timer – 选举 Timer\nheartbeatTimer *time.Timer – 作为 Leader 时使用的心跳 Timer\ncommitCond     *sync.Cond – 异步提交的通知信号量\nreplicateCond  []*sync.Cond – 异步同步 Log 的通知信号量\n\n#Log 结构\n为了方便最终实现 Log Compaction，我在最开始就设计成不保存全量 Log，而是只保存中间一段。整体结构如下图所示：\n:rf.log Structure:    +---------+---------+------------+-----+----------+    | (dummy) |  Log-p  |  Log-(p+1) | ... |  Log-(n) |    +---------+---------+------------+-----+----------+   /           \\___  /                \\___ /                     \\___/                          \\+---------------------------+| Term:    prevLogTerm      || Command: prevLogIndex=p-1 |+---------------------------+\nLog 序列从 1 开始计数，log[0] 作为哨兵节点，保存 Log 序列的起点信息。初始时，rf.log[0]中保存的值均为 0。\n#3A - Election\n#节点选举 &amp; 心跳定时器\n通过 Golang 的 Timer 实现超时选举和 Leader 心跳；\n// In Make()rf.electionTimer = time.NewTimer(makeElectionTimeout())rf.heartbeatTimer = time.NewTimer(HeartbeatIntervalMs * time.Millisecond)\n由于超时选举和 Leader 心跳不会同时发生，这里利用 golang 的select语法同时监听两个 Timer：\nfunc (rf *Raft) ticker() &#123;    for !rf.killed() &#123;        select &#123;        case &lt;-rf.electionTimer.C:            rf.electionTimer.Reset(makeElectionTimeout())            rf.doElection()        case &lt;-rf.heartbeatTimer.C:            rf.heartbeatTimer.Reset(HeartbeatIntervalMs * time.Millisecond)            rf.doReplicate(true)        &#125;    &#125;&#125;\n#投票逻辑\n选主过程主要是要实现 RequestVote 这个 RPC，注意如果投出了赞成票，需要重置自己的选举 Timer，否则有可能出现节点在投出赞成票后，又立即开始选举，Leader 反复横跳的情况。\n// example RequestVote RPC arguments structure.// field names must start with capital letters!type RequestVoteArgs struct &#123;    Term         int    CandidateId  int    LastLogIndex int    LastLogTerm  int&#125;// example RequestVote RPC reply structure.// field names must start with capital letters!type RequestVoteReply struct &#123;    Term        int    VoteGranted bool&#125;func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123;    rf.mu.Lock()    defer rf.mu.Unlock()    // outdated client    if args.Term &lt;= rf.currentTerm &#123;        reply.Term = rf.currentTerm        reply.VoteGranted = false        return    &#125;    // we are outdated, become follower    persist := false    if args.Term &gt; rf.currentTerm &#123;        rf.beFollower(args.Term)        persist = true    &#125;    reply.Term = rf.currentTerm    // check if we can vote for the candidate    // 1. candidate&#x27;s log is at least as up-to-date as receiver&#x27;s log    // 2. receiver hasn&#x27;t voted for another candidate    if (rf.votedFor == -1 || rf.votedFor == args.CandidateId) &amp;&amp; rf.isPeerUpToDate(args) &#123;        rf.votedFor = args.CandidateId        rf.electionTimer.Reset(makeElectionTimeout())        reply.VoteGranted = true        persist = true    &#125; else &#123;        // No need to set it to false        // reply.VoteGranted = false    &#125;    if persist &#123;        rf.persist()    &#125;&#125;\n投赞成票的判据有二：首先是自己没有投给其他节点，其次是 Candidate 的日志必须比自己更新或者相同：\nfunc (rf *Raft) isPeerUpToDate(args *RequestVoteArgs) bool &#123;    lastLogTerm := rf.lastLogTerm()    // client from future, we are outdated    if args.LastLogTerm &gt; lastLogTerm &#123;        return true    &#125;    // client from past, they are outdated    if args.LastLogTerm &lt; lastLogTerm &#123;        return false    &#125;    // client in the same term, check log index    return args.LastLogIndex &gt;= rf.lastLogIndex()&#125;\n为了保证 Follower 不会继续超时选主，还需要实现 AppendEntries 这个 RPC，刷新自己的选举 Timer。详细实现见接下来的 3B。\n#角色 &amp; 任期切换\n切换角色，核心是处理 leader 和非 leader 状态之间的切换：\n\n非 leader -&gt; leader：开启心跳 Timer，关闭选举 Timer，初始化 leader 状态；\nleader -&gt; 非 leader：关闭心跳 Timer，开启选举 Timer；\n\nfunc (rf *Raft) switchLeader(leader bool) &#123;    if rf.leader == leader &#123;        return    &#125;    rf.leader = leader    if leader &#123;        // initialize leader state        // all followers are behind, send logs from the beginning        nextLogIndex := rf.nextLogIndex()        for i := range rf.peers &#123;            rf.nextIndex[i] = nextLogIndex            rf.matchIndex[i] = 0        &#125;        rf.electionTimer.Stop()        rf.heartbeatTimer.Reset(HeartbeatIntervalMs * time.Millisecond)    &#125; else &#123;        rf.heartbeatTimer.Stop()        rf.electionTimer.Reset(makeElectionTimeout())    &#125;&#125;\n任期更新主要是处理自己落后的情况，例如收到 Term 更大的 AppendEntries、InstallSnapshot、RequestVote 请求。此时我们需要更新自己的任期：\nfunc (rf *Raft) beFollower(term int) &#123;    rf.currentTerm = term    rf.votedFor = -1    rf.switchLeader(false)&#125;\n#3B - Agreement\n在 Raft 中，一共有两种复制日志的方式：AppendEntries 和 InstallSnapshot；\n默认情况下，Leader 利用 AppendEntries RPC 请求把自己的日志推给 Follower，Follower 根据 Term 大小、以及 Log 是否冲突等因素反馈是否采纳。如果不采纳，Leader 再根据反馈的结果调整自己发送的日志范围。\n#Replicator Goroutine\n为每个 Follower 创建一个单独的 Replicator 协程，并发推送日志。\nReplicator 协程负责发送正确的 RPC 把日志同步到目标节点上。\n// Replicator is a goroutine that replicates logs to a peer by batch.func (rf *Raft) replicator(i int) &#123;    rf.replicateCond[i].L.Lock()    defer rf.replicateCond[i].L.Unlock()    for !rf.killed() &#123;        for !rf.needReplicateTo(i) &#123;            rf.replicateCond[i].Wait()        &#125;        rf.replicateTo(i)    &#125;&#125;// 是否需要推送日志到目标节点func (rf *Raft) needReplicateTo(i int) bool &#123;    rf.mu.RLock()    defer rf.mu.RUnlock()    return rf.leader &amp;&amp; rf.matchIndex[i] &lt; rf.lastLogIndex()&#125;\n// In Main()for i := range rf.peers &#123;    if i == rf.me &#123;        continue    &#125;    rf.replicateCond[i] = sync.NewCond(&amp;sync.Mutex&#123;&#125;)    go rf.replicator(i)&#125;\nRPC 判断逻辑，根据 Follower 的 nextIndex 判断应该调用 AppendEntries 或者 InstallSnapshot：\nfunc (rf *Raft) replicateTo(i int) &#123;    // Use read lock to avoid blocking other replicators    rf.mu.RLock()    if !rf.leader &#123;        rf.mu.RUnlock()        return    &#125;    nextIndex := rf.nextIndex[i]    if nextIndex &lt;= rf.prevLogIndex() &#123;        args := &amp;InstallSnapshotArgs&#123;            Term:              rf.currentTerm,            LeaderId:          rf.me,            LastIncludedIndex: rf.prevLogIndex(),            LastIncludedTerm:  rf.prevLogTerm(),            Data:              rf.persister.ReadSnapshot(),        &#125;        rf.mu.RUnlock()        rf.replicateByIS(i, args)    &#125; else &#123;        // copy to avoid data race        logs := rf.getLogFrom(nextIndex)        logs_copy := make([]LogEntry, len(logs))        copy(logs_copy, logs)        args := &amp;AppendEntriesArgs&#123;            Term:         rf.currentTerm,            LeaderId:     rf.me,            PrevLogIndex: nextIndex - 1,            PrevLogTerm:  rf.getLogTerm(nextIndex - 1),            Entries:      logs_copy, // not include the dummy log            LeaderCommit: rf.commitIndex,        &#125;        rf.mu.RUnlock()        rf.replicateByAE(i, args)    &#125;&#125;\n#AppendEntries RPC\nFollower 收到 AppendEntries 之后，需要进行一系列检查：\ntype AppendEntriesArgs struct &#123;    Term         int    LeaderId     int    PrevLogIndex int    PrevLogTerm  int    Entries      []LogEntry // not include the dummy log    LeaderCommit int&#125;type AppendEntriesReply struct &#123;    Term          int    Success       bool    ConflictTerm  int    ConflictIndex int&#125;func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) &#123;    rf.mu.Lock()    defer rf.mu.Unlock()    // outdated leader    if args.Term &lt; rf.currentTerm &#123;        reply.Term = rf.currentTerm        reply.Success = false        return    &#125;    // we are outdated, become follower    persist := false    if args.Term &gt; rf.currentTerm &#123;        rf.beFollower(args.Term)        persist = true    &#125;    rf.electionTimer.Reset(makeElectionTimeout())    reply.Term = rf.currentTerm    // conflict because we don&#x27;t have the prev log entry    // ===============[....we have....]    // ========[....we got....]    if args.PrevLogIndex &lt; rf.prevLogIndex() &#123;        reply.Success = false        // give us later log index        reply.ConflictTerm = -1        reply.ConflictIndex = rf.nextLogIndex()        if persist &#123;            rf.persist()        &#125;        return    &#125;    // cannot match because of missing log entries    // =====[....we have....]    // ==========================[....we got....]    if rf.lastLogIndex() &lt; args.PrevLogIndex &#123;        reply.Success = false        reply.ConflictTerm = -1        reply.ConflictIndex = rf.nextLogIndex()        if persist &#123;            rf.persist()        &#125;        return    &#125;    // conflict because log entry&#x27;s term doesn&#x27;t match    // ===============[x....we have....]    // =============[..y....we got...]    if rf.getLogTerm(args.PrevLogIndex) != args.PrevLogTerm &#123;        reply.Success = false        reply.ConflictTerm = rf.getLogTerm(args.PrevLogIndex)        // Find the first index that has the same term as provided        reply.ConflictIndex = rf.prevLogIndex()        for idx := 1; idx &lt; len(rf.log); idx++ &#123;            if rf.log[idx].Term == reply.ConflictTerm &#123;                reply.ConflictIndex = rf.getLogIndex(idx)                break            &#125;        &#125;        if persist &#123;            rf.persist()        &#125;        return    &#125;    reply.Success = true    // Overwrite unmatched log entries    // If no conflict, just append the new logs if any    // ===============[.....we have.....]    // ===============[...we got...]    for i, entry := range args.Entries &#123;        index := args.PrevLogIndex + i + 1        if !rf.hasLog(index) || rf.getLogTerm(index) != entry.Term &#123;            // found conflict, truncate logs            log := rf.log[:index-rf.prevLogIndex()]            rf.log = append(log, args.Entries[i:]...)            persist = true            break        &#125;    &#125;    // update commitIndex    // sometimes the leader&#x27;s commitIndex is ahead of the follower&#x27;s    newCommitIndex := Min(args.LeaderCommit, rf.lastLogIndex())    if newCommitIndex &gt; rf.commitIndex &#123;        rf.commitIndex = newCommitIndex        rf.applyCond.Signal()    &#125;    if persist &#123;        rf.persist()    &#125;&#125;\n#AppendEntries 响应处理\n在发送完 AppendEntries 之后，需要根据 Follower 的反馈进行一系列处理。\nfunc (rf *Raft) replicateByAE(i int, args *AppendEntriesArgs) &#123;    reply := &amp;AppendEntriesReply&#123;&#125;    if !rf.sendAppendEntries(i, args, reply) &#123;        // net failure        return    &#125;    rf.mu.Lock()    defer rf.mu.Unlock()    // we are still the leader    if rf.currentTerm != args.Term || !rf.leader &#123;        return    &#125;    if reply.Success &#123;        nextIndex := args.PrevLogIndex + len(args.Entries)        // outdated response        if nextIndex &lt;= rf.matchIndex[i] &#123;            return        &#125;        rf.nextIndex[i] = nextIndex + 1        rf.matchIndex[i] = nextIndex        rf.updateCommitIndex()    &#125; else &#123;        // Reject because of outdateness        // we are outdated        if reply.Term &gt; rf.currentTerm &#123;            rf.beFollower(reply.Term)            rf.persist()            return        &#125;        // ????        if reply.Term &lt; rf.currentTerm || !rf.leader &#123;            return        &#125;        nextIndex := reply.ConflictIndex        if reply.ConflictTerm != -1 &#123;            prevLogIndex := rf.prevLogIndex()            for j := args.PrevLogIndex; j &gt; prevLogIndex; j-- &#123;                if rf.getLogTerm(j) == reply.ConflictTerm &#123;                    nextIndex = j + 1                    break                &#125;            &#125;        &#125;        // take minimum value        if nextIndex &lt; rf.nextIndex[i] &#123;            rf.nextIndex[i] = nextIndex        &#125;        // resend immediately        go rf.replicateTo(i)    &#125;&#125;\n更新 commitIndex 时要注意 Figure 8 问题，Raft 的限制：\nfunc (rf *Raft) updateCommitIndex() &#123;    newCommitIndex := topK(rf.matchIndex, (len(rf.peers)-1)/2)    if newCommitIndex &gt; rf.commitIndex &#123;        // Raft Paper&#x27;s Figure 8        // Raft never commits log entries from previous terms by counting        // replicas. Only log entries from the leader’s current term are        // committed by counting replicas; once an entry from the current term        // has been committed in this way, then all prior entries are        // committed indirectly because of the Log Matching Property.        if rf.getLogTerm(newCommitIndex) == rf.currentTerm &#123;            rf.commitIndex = newCommitIndex            rf.applyCond.Signal()            // immediately notify other followers to update their commitIndex            rf.doReplicate(true)        &#125;    &#125;&#125;\n#日志提交\nRaft 层的日志会通过一个 blocking 的 go channel 发给 Service 层执行，由于 Service 层可能不会立即处理，因此必须采用异步提交的方式，而且提交的时候也不能持有锁。\n最简单的实现方法是在 Raft 层中创建一个单独的 goroutine，专门负责提交日志：\nfunc (rf *Raft) applier() &#123;    rf.mu.Lock()    defer rf.mu.Unlock()    for !rf.killed() &#123;        if rf.pendingSnapshot != nil &#123;            // apply snapshot            snapshot := rf.pendingSnapshot            rf.pendingSnapshot = nil            rf.mu.Unlock()            rf.applyCh &lt;- *snapshot            rf.mu.Lock()        &#125; else if rf.lastApplied &lt; rf.commitIndex &#123;            // apply logs            startIndex := rf.lastApplied + 1            entries := append([]LogEntry&#123;&#125;, rf.getLogSlice(startIndex, rf.commitIndex+1)...)            rf_commitIndex := rf.commitIndex            rf.mu.Unlock()            for i, entry := range entries &#123;                rf.applyCh &lt;- ApplyMsg&#123;                    CommandValid: true,                    Command:      entry.Command,                    CommandTerm:  entry.Term,                    CommandIndex: startIndex + i,                &#125;            &#125;            rf.mu.Lock()            rf.lastApplied = Max(rf.lastApplied, rf_commitIndex)        &#125; else &#123;            rf.applyCond.Wait()        &#125;    &#125;&#125;\n#3C - Persistence\n为了数据安全性，Raft 需要周期性地把自己的状态持久化到硬盘上，需要持久化的信息论文中已经讲过。代码中，我们利用 labgob 将这些信息序列化：\nfunc (rf *Raft) serialize() []byte &#123;    w := new(bytes.Buffer)    e := labgob.NewEncoder(w)    e.Encode(rf.currentTerm)    e.Encode(rf.votedFor)    e.Encode(rf.log)    return w.Bytes()&#125;\n持久化方法：\nfunc (rf *Raft) persist() &#123;    // Your code here (3C).    rf.persister.Save(rf.serialize(), rf.persister.snapshot)&#125;func (rf *Raft) persistWithSnapshot(snapshot []byte) &#123;    rf.persister.Save(rf.serialize(), snapshot)&#125;\n持久化的时机有二：\n\n每次修改Term、VotedFor、或者Log之后\n收到了 InstallSnapshot RPC 请求之后\n\n#3D - Compaction / Snapshot\n#快照 API\n为了避免 Raft Log 无限制增长，也为了加快 Raft 节点及状态机在崩溃后的恢复速度 (从头重放一遍所有 Log)，Raft 设计了 Log Compaction 机制，允许把过早的 Logs 换成一个等价的 State Machine Snapshot。\nAPI 设计如下，Snapshot 的发起方是上层 Service，发起时需要携带一个 State Machine Snapshot 以及对应的 Log Index：\n// the service says it has created a snapshot that has// all info up to and including index. this means the// service no longer needs the log through (and including)// that index. Raft should now trim its log as much as possible.func (rf *Raft) Snapshot(index int, snapshot []byte) &#123;    rf.mu.Lock()    defer rf.mu.Unlock()    // The log entry at index is already included in the snapshot    if index &lt;= rf.prevLogIndex() &#123;        return    &#125;    // truncate logs    rf.log = rf.getLogFrom(index)    rf.log[0].Command = index    rf.persistWithSnapshot(snapshot)&#125;\n#InstallSnapshot RPC\n在进行过 Log Compaction 的情况下，Follower 所需的日志可能已经被 Compaction 掉了，这种情况下 Leader 会发送 InstallSnapshot RPC 请求直接推一个 Snapshot 给 Follower。注意此时 Follower 是不会反馈结果的；\ntype InstallSnapshotArgs struct &#123;    Term              int    LeaderId          int    LastIncludedIndex int    LastIncludedTerm  int    Data              []byte&#125;type InstallSnapshotReply struct &#123;    Term int&#125;func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply) &#123;    rf.mu.Lock()    // outdated leader    if args.Term &lt; rf.currentTerm &#123;        reply.Term = rf.currentTerm        rf.mu.Unlock()        return    &#125;    // we are outdated, become follower    if args.Term &gt; rf.currentTerm &#123;        rf.beFollower(args.Term)        rf.persist()    &#125;    reply.Term = rf.currentTerm    rf.electionTimer.Reset(makeElectionTimeout())    // outdated snapshot    if args.LastIncludedIndex &lt;= rf.prevLogIndex() || args.LastIncludedIndex &lt;= rf.commitIndex &#123;        rf.mu.Unlock()        return    &#125;    if !rf.useAsyncInstallSnapshot &#123;        rf.doInstallSnapshot(args.LastIncludedTerm, args.LastIncludedIndex, args.Data)        // overwrite is ok        rf.pendingSnapshot = &amp;ApplyMsg&#123;            SnapshotValid: true,            Snapshot:      args.Data,            SnapshotTerm:  args.LastIncludedTerm,            SnapshotIndex: args.LastIncludedIndex,        &#125;        rf.mu.Unlock()    &#125; else &#123;        rf.mu.Unlock()        go func() &#123;            // Notify the service that we are going to switch to a snapshot.            // When the service is ready, it should call `TryInstallSnapshot`            // and switch to the snapshot.            rf.applyCh &lt;- ApplyMsg&#123;                SnapshotValid: true,                Snapshot:      args.Data,                SnapshotTerm:  args.LastIncludedTerm,                SnapshotIndex: args.LastIncludedIndex,            &#125;        &#125;()    &#125;&#125;\n#InstallSnapshot 响应处理\n和 AppendEntries 的处理类似：\nfunc (rf *Raft) replicateByIS(i int, args *InstallSnapshotArgs) &#123;    reply := &amp;InstallSnapshotReply&#123;&#125;    if !rf.sendInstallSnapshot(i, args, reply) &#123;        // net failure        return    &#125;    rf.mu.Lock()    defer rf.mu.Unlock()    // we are still the leader    if rf.currentTerm != args.Term || !rf.leader &#123;        return    &#125;    // we are outdated    if reply.Term &gt; rf.currentTerm &#123;        rf.beFollower(reply.Term)        rf.persist()        return    &#125;    if args.LastIncludedIndex &gt; rf.matchIndex[i] &#123;        rf.matchIndex[i] = args.LastIncludedIndex        rf.updateCommitIndex()    &#125;    // increase nextIndex by 1 or more    nextIndex := Max(rf.nextIndex[i], args.LastIncludedIndex) + 1    if nextIndex &lt;= rf.nextLogIndex() &#123;        rf.nextIndex[i] = nextIndex    &#125;&#125;\n","categories":["distributed-system"],"tags":["分布式系统","Raft"]},{"title":"[OSDI'12] Spanner: Google's Globally-Distributed Database 论文阅读","url":"/2025/08/17/spanner-googles-globally-distributed-database/","content":"#0. 摘要\nSpanner 是 Google 公司研发的、可扩展的、多版本、全球分布式、同步复制数据库。它是第一个把数据分布在全球范围内的系统，并且支持外部一致性的分布式事务。本文描述了 Spanner 的架构、特性、不同设计决策的背后机理和一个新的时间 API，这个 API 可以暴露时钟的不确定性。这个 API 及其实现，对于支持外部一致性和许多强大特性而言，是非常重要的，这些强大特性包括：非阻塞的读、不采用锁机制的只读事务、原子模式变更。\n#1. 引言\nSpanner 是一个可扩展的、全球分布式的数据库，是在谷歌公司设计、开发和部署的。在最高抽象层面，Spanner 就是一个数据库，把数据分片存储在许多 Paxos 状态机上，这些机器位于遍布全球的数据中心内。复制技术可以用来服务于全球可用性和地理局部性。客户端会自动在副本之间进行失败恢复。随着数据的变化和服务器的变化，Spanner 会自动把数据进行重新分片，从而有效应对负载变化和处理失败。Spanner 被设计成可以扩展到几百万个机器节点，跨越成百上千个数据中心，具备几万亿数据库行的规模。\nSpanner 的主要目标是管理跨数中心的副本数据，但是我们还花了很多时间设计并实现了在我们的分布式系统基础架构之上的重要的数据库特性。因为 Bigtable 对一些类型的应用程序来说难以使用：如那些有复杂、不断演进的模型的程序或那些想要在广域副本中维护强一致性的程序。\nGoogle 的许多应用程序选择使用 Megastore，因为它支持半结构化数据模型和副本同步，尽管它的写入吞吐量相对较弱。为此，Spanner 从一个类似 Bigtable 的版本化键值存储（versioned key-value store）演进成了一个多版本时态数据库（temporal multi-version database）。\n作为一个全球分布式数据库，Spanner 提供了几个有趣的特性：\n\n第一，在数据的副本配置方面，应用可以在一个很细的粒度上进行动态控制。应用可以详细规定，哪些数据中心包含哪些数据，数据距离用户有多远（控制用户读取数据的延迟），不同数据副本之间距离有多远（控制写操作的延迟），以及需要维护多少个副本（控制可用性和读操作性能）。数据也可以被动态和透明地在数据中心之间进行移动，从而平衡不同数据中心内资源的使用。\n第二，Spanner 有两个重要的特性，很难在一个分布式数据库上实现，即 Spanner 提供了读和写操作的外部一致性，以及在一个时间戳下面的跨越数据库的全球一致性的读操作。\n\n这些特性有效的原因在于，Spanner 会为事务分配在全局有效的提交时间戳，尽管事务可能是分布式的。该时间戳反映了串行顺序。另外，串行顺序满足外部一致性（或等价的线性一致性）：如果事务 T1T_1T1​ 在另一个事务 T2T_2T2​ 开始前提交，那么 T1T_1T1​ 的时间戳比 T2T_2T2​ 的小。\nSpanner 是首个能在全球范围提供这些保证的系统。\n实现这种特性的关键技术就是一个新的 TrueTime API 及其实现。该 API 直接暴露了时钟不确定度，且对 Spanner 的时间戳的保证基于该 API 的实现提供的界限内。如果不确定度较大，Spanner 会减速以等待该不确定度。\nGoogle 的集群管理软件提供了 TureTime API 的一种实现。该实现通过使用多种现代参考时钟（GPS 和原子时钟）来让不确定度保持较小（通常小于 10ms）。\n#2. 实现\n一个 Spanner 部署称为一个 universe。假设 Spanner 在全球范围内管理数据，那么，将会只有可数的、运行中的 universe。我们当前正在运行一个测试用的 universe，一个部署/线上用的 universe 和一个只用于线上应用的 universe。\nSpanner 被组织成许多个 zone 的集合，每个 zone 都大概像一个 BigTable 服务器的部署。\nzone 是管理部署的基本单元。zone 的集合也是数据可以被复制到的位置的集合。当新的数据中心加入服务，或者老的数据中心被关闭时，zone 可以被加入到一个运行的系统中，或者从中移除。\nzone 也是物理隔离的单元，在一个数据中心中，可能有一个或者多个 zone，例如，属于不同应用的数据可能必须被分区存储到同一个数据中心的不同服务器集合中。\n\n一个 zone 包括一个 zonemaster，和一百至几千个 spanserver。\nZonemaster 把数据分配给 spanserver，spanserver 把数据提供给客户端。客户端使用每个 zone 上面的 location proxy 来定位可以为自己提供数据的 spanserver。\nUniverse master 和 placement driver，当前都只有一个。\nUniverse master 主要是一个控制台，它显示了关于 zone 的各种状态信息，可以用于相互之间的调试。\nPlacement driver 会周期性地与 spanserver 进行交互，来发现那些需要被转移的数据，或者是为了满足新的副本约束条件，或者是为了进行负载均衡。\n#2.1 Spanserver 软件栈\n在底部，每个 spanserver 负载管理 100-1000 个称为 Tablet 的数据结构实例。\nTablet 类似于 BigTable 中的 Tablet，也实现了下面的映射：\n(key:string, timestamp:int64) -&gt; string\n\n与 BigTable 不同的是，Spanner 会把时间戳分配给数据，这种非常重要的方式，使得 Spanner 更像一个多版本数据库，而不是一个键值存储。一个 Tablet 的状态是存储在 类似于 B 树的文件集合 和 WAL 中，所有这些都会被保存到 Colossus，它继承自 Google File System。\n为了支持复制，每个 spanserver 会在每个 Tablet 上面实现一个 Paxos 状态机。一个之前实现的 Spanner 可以支持在每个 Tablet 上面实现多个 Paxos 状态机（博主注：类似现在流行的 Multi-Raft），它可以允许更加灵活的复制配置，但是，这种设计过于复杂，被我们舍弃了。每个状态机都会在相应的 Tablet 中保存自己的元数据和日志。我们的 Paxos 实现支持采用基于时间的领导者租约的长寿命的领导者，时间通常在 0 到 10 秒之间。当前的 Spanner 实现中，会对每个 Paxos 写操作进行两次记录：一次是写入到 tablet 日志中，一次是写入到 Paxos 日志中。这种做法只是权宜之计，我们以后会进行完善。\n\n#2.2 目录和放置\n目录是包含公共前缀的连续键的集合。（选择目录作为名称，主要是由于历史沿袭的考虑，实际上更好的名称应该是桶）。对目录的支持，可以让应用通过选择合适的键来控制数据的局部性。\n一个目录是数据放置的基本单元。属于一个目录的所有数据，都具有相同的副本配置。当数据在不同的 Paxos 组之间进行移动时，会一个目录一个目录地转移。\nSpanner 可能会移动一个目录从而减轻一个 Paxos 组的负担，也可能会把那些被频繁地一起访问的目录都放置到同一个组中，或者会把一个目录转移到距离访问者更近的地方。当客户端操作正在进行时，也可以进行目录的转移。我们可以预期在几秒内转移 50MB 的目录。\n\n一个 Paxos 组可以包含多个目录，这意味着一个 Spanner Tablet 是不同于一个 BigTable Tablet 的。一个 Spanner Tablet 没有必要是一个行空间内按照词典顺序连续的分区，相反，它可以是行空间内的多个分区。我们做出这个决定，是因为这样做可以让多个被频繁一起访问的目录被整合到一起。\nMovedir 是一个后台任务，用来在不同的 Paxos 组之间转移目录。Movedir 也用来为 Paxos 组增加和删除副本，因为 Spanner 目前还不支持在一个 Paxos 内部进行配置的变更。\nMovedir 并不是作为一个事务来实现，这样可以避免在一个块数据转移过程中阻塞正在进行的读操作和写操作。相反，Movedir 会注册一个事实(fact)，表明它要转移数据，然后在后台运行转移数据。当它几乎快要转移完指定数量的数据时，就会启动一个事务来自动转移那部分数据，并且为两个 Paxos 组更新元数据。\n一个目录也是一个应用可以指定的地理复制属性（即放置策略）的最小单元。我们的放置规范语言的设计，把管理复制的配置这个任务单独分离出来。管理员需要控制两个维度：副本的数量和类型，以及这些副本的地理放置属性。他们在这两个维度里面创建了一个命名选项的菜单。通过为每个数据库或单独的目录增加这些命名选项的组合，一个应用就可以控制数据的复制。例如，一个应用可能会在自己的目录里存储每个终端用户的数据，这就有可能使得用户 A 的数据在欧洲有三个副本，用户 B 的数据在北美有 5 个副本。\n为了表达的清晰性，我们已经做了尽量简化。事实上，当一个目录变得太大时，Spanner 会把它分片存储。每个分片可能会被保存到不同的 Paxos 组上（因此就意味着来自不同的服务器）。\nMovedir 在不同组之间转移的是分片，而不是转移整个目录。\n#2.3 数据模型\nSpanner 会把下面的数据特性集合暴露给应用：\n\n基于 Schema 的半关系表的数据模型\n查询语言\n通用事务\n\n支持这些特性的动机，是受到许多因素驱动的。\n\n需要 Schema 的半关系表: 是由 Megastore 的普及来支持的。在谷歌内部至少有 300 个应用使用 Megastore（尽管它具有相对低的性能），因为它的数据模型要比 BigTable 简单，更易于管理，并且支持在跨数据中心层面进行同步复制。而 BigTable 只可以支持跨数据中心的最终事务一致性。使用 Megastore 的著名的谷歌应用是 Gmail,Picasa,Calendar,Android Market, AppEngine。\n在 Spanner 中需要支持 SQL 类型的查询语言，也很显然是非常必要的，因为 Dremel 作为交互式分析工具已经非常普及。\n事务：在 BigTable 中跨行事务的缺乏来导致了用户频繁的抱怨；开发 Percolator 就是用来部分解决这个问题的。一些作者都在抱怨，通用的两阶段提交的代价过于昂贵，因为它会带来可用性问题和性能问题。我们认为，最好让应用程序开发人员来处理由于过度使用事务引起的性能问题，而不是总是围绕着“缺少事务”进行编程。在 Paxos 上运行两阶段提交弱化了可用性问题。\n\n应用的数据模型是架构在被目录桶装的键值映射层之上。一个应用会在一个 universe 中创建一个或者多个数据库。每个数据库可以包含无限数量的模式化的表。每个表都和关系数据库表类似，具备行、列和版本值。我们不会详细介绍 Spanner 的查询语言，它看起来很像 SQL，只是做了一些扩展。\nSpanner 的数据模型不是纯粹关系型的，它的行必须有名称。更准确地说，每个表都需要有包含一个或多个主键列的排序集合。这种需求，让 Spanner 看起来仍然有点像键值存储：主键形成了一个行的名称，每个表都定义了从主键列到非主键列的映射。当一个行存在时，必须要求已经给行的一些键定义了一些值（即使是 NULL）。采用这种结构是很有用的，因为这可以让应用通过选择键来控制数据的局部性。\n\n图 4 包含了一个 Spanner 模式的实例，它是以每个用户和每个相册为基础存储图片元数据。这个模式语言和 Megastore 的类似，同时增加了额外的要求，即每个 Spanner 数据库必须被客户端分割成一个或多个表的层次结构（hierarchy）。客户端应用会使用 INTERLEAVE IN 语句在数据库模式中声明这个层次结构。这个层次结构上面的表，是一个目录表。目录表中的每行都具有键 K，和子孙表中的所有以 K 开始（以字典顺序排序）的行一起，构成了一个目录。ON DELETE CASCADE 意味着，如果删除目录中的一个行，也会级联删除所有相关的子孙行。这个图也解释了这个实例数据库的交织层次（interleaved layout），例如 Albums(2,1)代表了来自 Albums 表的、对应于 user_id=2 和 album_id=1 的行。这种表的交织层次形成目录，是非常重要的，因为它允许客户端来描述存在于多个表之间的位置关系，这对于一个分片的分布式数据库的性能而言是很重要的。没有它的话，Spanner 就无法知道最重要的位置关系。\n#3 TrueTime\n\n本部分内容描述 TrueTime API，并大概给出它的实现方法。细节内容将放在另一篇论文中，我们的目标是展示这种 API 的力量。\n在底层，TrueTime 使用的时间是 GPS 和原子钟。TrueTime 使用两种类型的时间，是因为它们有不同的失败模式。GPS 参考时间的弱点是天线和接收器失效、局部电磁干扰和相关失败（比如设计上的缺陷导致无法正确处理闰秒和电子欺骗），以及 GPS 系统运行中断。原子钟也会失效，不过失效的方式和 GPS 无关，不同原子钟之间的失效也没有彼此关联。由于存在频率误差，在经过很长的时间以后，原子钟都会产生明显误差。\nTrueTime 是由每个数据中心上面的许多 time master 机器和每台机器上的一个 timeslave daemon 来共同实现的。大多数 master 都有具备专用天线的 GPS 接收器，这些 master 在物理上是相互隔离的，这样可以减少天线失效、电磁干扰和电子欺骗的影响。剩余的 master（我们称为 Armageddon master）则配备了原子钟。一个原子钟并不是很昂贵：一个 Armageddon master 的花费和一个 GPS master 的花费是同一个数量级的。所有 master 的时间参考值都会进行彼此校对。每个 master 也会交叉检查时间参考值和本地时间的比值，如果二者差别太大，就会把自己驱逐出去。在同步期间，Armageddon master 会表现出一个逐渐增加的时间不确定性，这是由保守应用的最差时钟漂移引起的。GPS master 表现出的时间不确定性几乎接近于 0。\n每个 daemon 会从许多 master 中收集投票，获得时间参考值，从而减少误差。被选中的 master 中，有些 master 是 GPS master，是从附近的数据中心获得的，剩余的 GPS master 是从远处的数据中心获得的；还有一些是 Armageddon master。Daemon 会使用一个 Marzullo 算法的变种，来探测和拒绝欺骗，并且把本地时钟同步到非撒谎 master 的时间参考值。为了免受较差的本地时钟的影响，我们会根据组件规范和运行环境确定一个界限，如果机器的本地时钟误差频繁超出这个界限，这个机器就会被驱逐出去。\n在同步期间，一个 daemon 会表现出逐渐增加的时间不确定性。ε 是从保守应用的最差时钟漂移中得到的。ε 也取决于 time master 的不确定性，以及与 time master 之间的通讯延迟。在我们的线上应用环境中，ε 通常是一个关于时间的锯齿形函数。在每个投票间隔中，ε 会在 1 到 7ms 之间变化。因此，在大多数情况下，ε 的平均值是 4ms。Daemon 的投票间隔，在当前是 30 秒，当前使用的时钟漂移比率是 200 微秒/秒，二者一起意味着 0 到 6ms 的锯齿形边界。剩余的 1ms 主要来自到 time master 的通讯延迟。在失败的时候，超过这个锯齿形边界也是有可能的。例如，偶尔的 time master 不确定性，可能会引起整个数据中心范围内的 ε 值的增加。类似的，过载的机器或者网络连接，都会导致 ε 值偶尔地局部增大。\n#4. 并发控制\n本部分内容描述 TrueTime 如何可以用来保证并发控制的正确性，以及这些属性如何用来实现一些关键特性，比如外部一致性的事务、无锁机制的只读事务、针对历史数据的非阻塞读。这些特性可以保证，在时间戳为 t 的时刻的数据库读操作，一定只能看到在 t 时刻之前已经提交的事务。\n进一步说，把 Spanner 客户端的写操作和 Paxos 看到的写操作这二者进行区分，是非常重要的，我们把 Paxos 看到的写操作称为 Paxos 写操作。例如，两阶段提交会为准备提交阶段生成一个 Paxos 写操作，这时不会有相应的客户端写操作。\n#4.1 时间戳管理\n表 2 列出了 Spanner 支持的操作的类型。Spanner 可以支持读写事务、只读事务（预先声明的快照隔离事务）和快照读。独立写操作，会被当成读写事务来执行。非快照独立读操作，会被当成只读事务来执行。二者都是在内部进行 retry，客户端不用进行这种 retry loop。\n\n一个只读事务具备快照隔离的性能优势。一个只读事务必须事先被声明不会包含任何写操作，它并不是一个简单的不包含写操作的读写事务。在一个只读事务中的读操作，在执行时会采用一个系统选择的时间戳，不包含锁机制，因此，后面到达的写操作不会被阻塞。在一个只读事务中的读操作，可以到任何足够新的副本上去执行（见第 4.1.3 节）。\n一个快照读操作，是针对历史数据的读取，执行过程中，不需要锁机制。一个客户端可以为快照读确定一个时间戳，或者提供一个时间范围让 Spanner 来自动选择时间戳。不管是哪种情况，快照读操作都可以在任何具有足够新的副本上执行。\n对于只读事务和快照读而言，一旦已经选定一个时间戳，那么，提交就是不可避免的，除非在那个时间点的数据已经被垃圾回收了。因此，客户端不必在 retry loop 中缓存结果。当一个服务器失效的时候，客户端就可以使用同样的时间戳和当前的读位置，在另外一个服务器上继续执行读操作。\n#4.2 细节\n（略）\n#5. 实验分析\n#5.4 F1\nSpanner 在 2011 年早期开始进行在线负载测试，它是作为谷歌广告后台 F1 的重新实现的一部分。这个后台最开始是基于 MySQL 数据库，在许多方面都采用手工数据分区。未经压缩的数据可以达到几十 TB，虽然这对于许多 NoSQL 实例而言数据量是很小的，但是，对于采用数据分区的 MySQL 而言，数据量是非常大的。\nMySQL 的数据分片机制，会把每个客户和所有相关的数据分配给一个固定的分区。这种布局方式，可以支持针对单个客户的索引构建和复杂查询处理，但是，需要了解一些商业知识来设计分区。随着客户数量的增长，对数据进行重新分区，代价是很大的。最近一次的重新分区，花费了两年的时间，为了降低风险，在多个团队之间进行了大量的合作和测试。这种操作太复杂了，无法常常执行，由此导致的结果是，团队必须限制 MySQL 数据库的增长，方法是，把一些数据存储在外部的 Bigtable 中，这就会牺牲事务和查询所有数据的能力。\nF1 团队选择使用 Spanner 有几个方面的原因。\n\nSpanner 不需要手工分区。\nSpanner 提供了同步复制和自动失败恢复。在采用 MySQL 的 master-slave 复制方法时，很难进行失败恢复，会有数据丢失和宕机的风险。\nF1 需要强壮的事务语义，这使得使用其他 NoSQL 系统是不实际的。应用语义需要跨越任意数据的事务和一致性读。F1 团队也需要在他们的数据上构建二级索引（因为 Spanner 没有提供对二级索引的自动支持），也有能力使用 Spanner 事务来实现他们自己的一致性全球索引。\n\n所有应用写操作，现在都是默认从 F1 发送到 Spanner。而不是发送到基于 MySQL 的应用栈。F1 在美国的西岸有两个副本，在东岸有三个副本。这种副本位置的选择，是为了避免发生自然灾害时出现服务停止问题，也是出于前端应用的位置的考虑。实际上，Spanner 的失败自动恢复，几乎是不可见的。在过去的几个月中，尽管有不在计划内的机群失效，但是，F1 团队最需要做的工作仍然是更新他们的数据库模式，来告诉 Spanner 在哪里放置 Paxos 领导者，从而使得它们尽量靠近应用前端。\nSpanner 时间戳语义，使得它对于 F1 而言，可以高效地维护从数据库状态计算得到的、放在内存中的数据结构。F1 会为所有变更都维护一个逻辑历史日志，它会作为每个事务的一部分写入到 Spanner。F1 会得到某个时间戳下的数据的完整快照，来初始化它的数据结构，然后根据数据的增量变化来更新这个数据结构。\n\n表 5 显示了 F1 中每个目录的分片数量的分布情况。每个目录通常对应于 F1 上的应用栈中的一个用户。绝大多数目录（同时意味着绝大多数用户）都只会包含一个分片，这就意味着，对于这些用户数据的读和写操作只会发生在一个服务器上。多于 100 个分片的目录，是那些包含 F1 二级索引的表：对这些表的多个分片进行写操作，是极其不寻常的。F1 团队也只是在以事务的方式进行未经优化的批量数据加载时，才会碰到这种情形。\n\n表 6 显示了从 F1 服务器来测量的 Spanner 操作的延迟。在东海岸数据中心的副本，在选择 Paxos 领导者方面会获得更高的优先级。表 6 中的数据是从这些数据中心的 F1 服务器上测量得到的。写操作延迟分布上存在较大的标准差，是由于锁冲突引起的肥尾效应（fat tail）。在读操作延迟分布上存在更大的标准差，部分是因为 Paxos 领导者跨越了两个数据中心，只有其中的一个是采用了固态盘的机器。此外，测试内容还包括系统中的每个针对两个数据中心的读操作：字节读操作的平均值和标准差分别是 1.6KB 和 119KB。\n#6. 相关工作\nMegastore 和 DynamoDB 已经提供了跨越多个数据中心的一致性复制。\n\nDynamoDB 提供了键值存储接口，只能在一个 region 内部进行复制。Spanner 和 Megastore 一样，都提供了半关系数据模型，甚至采用了类似的模式语言。\nMegastore 无法获得高性能。Megastore 是架构在 Bigtable 之上，这带来了很高的通讯代价。Megastore 也不支持长寿命的领导者，多个副本可能会发起写操作。来自不同副本的写操作，在 Paxos 协议下一定会发生冲突，即使他们不会发生逻辑冲突：会严重影响吞吐量，在一个 Paxos 组内每秒钟只能执行几个写操作。Spanner 提供了更高的性能，通用的事务和外部一致性。\n\nPavlo 等人对数据库和 MapReduce 的性能进行了比较。他们指出了几个努力的方向，可以在分布式键值存储之上充分利用数据库的功能，二者可以实现充分的融合。我们比较赞同这个结论，并且认为集成多个层是具有优势的：把复制和并发控制集成起来，可以减少 Spanner 中的提交等待代价。\n在一个采用了复制的存储上面实现事务，可以至少追述到 Gifford 的论文。\n\nScatter 是一个最近的基于 DHT 的键值存储，可以在一致性复制上面实现事务。Spanner 则要比 Scatter 在更高的层次上提供接口。\nGray 和 Lamport 描述了一个基于 Paxos 的非阻塞的提交协议，他们的协议会比两阶段提交协议带来更多的代价，而两阶段提交协议在大范围分布式的组中的代价会进一步恶化。\nWalter 提供了一个快照隔离的变种，但是无法跨越数据中心。相反，我们的只读事务提供了一个更加自然的语义，因为，我们对于所有的操作都支持外部语义。\n\n最近，在减少或者消除锁开销方面已经有大量的研究工作。\n\nCalvin 消除了并发控制：它会重新分配时间戳，然后以时间戳的顺序执行事务。\nHStore 和 Granola 都支持自己的事务类型划分方法，有些事务类型可以避免锁机制。\n\n但是，这些系统都无法提供外部一致性。Spanner 通过提供快照隔离，解决了冲突问题。\nVoltDB 是一个分片的内存数据库，可以支持在大范围区域内进行主从复制，支持灾难恢复，但是没有提供通用的复制配置方法。它是一个被称为 NewSQL 的实例，这是实现可扩展的 SQL 的强大的市场推动力。许多商业化的数据库都可以支持历史数据读取，比如 Marklogic 和 Oracle’ Total Recall。\nLomet 和 Li 对于这种时间数据库描述了一种实现策略。\nFaresite 给出了与一个受信任的时钟参考值相关的、时钟不确定性的边界（要比 TrueTime 更加宽松）：Farsite 中的服务器租约的方式，和 Spanner 中维护 Paxos 租约的方式相同。在之前的工作中 [23]，宽松同步时钟已经被用来进行并发控制。我们已经展示了 TrueTime 可以从 Paxos 状态机集合中推导出全球时间。\n#参考资料\n\nGoogle Spanner (中文版)\n\n","categories":["distributed-system"],"tags":["分布式系统","数据库","Spanner"]},{"title":"[SOSP'03] The Google File System 论文阅读","url":"/2025/02/23/the-google-file-system/","content":"很早就想读读大名鼎鼎的 GFS 的论文了，这次终于抽出时间好好读一下。\n#0. 摘要\nGFS 是一个可扩展的分布式文件系统，用于巨大的、分布式的数据密集型应用。\nGFS 在运行在廉价的商用硬件上运行的同时，对外提供容错性。服务大量客户端时，能提供良好的总性能。\n在 Google 的环境中经过了实际考验。\n#1. 引言\nGFS 的设计目标：高性能、可扩展、可靠性、可用性。\n\n在大规模系统中，组件故障是常态。因此持续监测，错误检测，容错性和自动恢复是必须包含的部分；\n预期文件会很大，常见的是 GB 级别；\n大多数 workload 模式是追加写而非覆盖写，文件写入后只会被读取，读取通常是顺序读。因此关键点在于保证追加写的性能和原子性；\n协同设计上层应用和 FS 的 API 对提供整个系统的弹性有好处。例如放宽一致性约束简化应用程序。\n\n#2. 设计概述\n#2.1 假设\n\n使用经常故障的廉价商用硬件\n保存相当多的大文件\n读 workload 主要是大量顺序读和少量随机读\n写 workload 主要是大块的追加写\n需要给追加写操作提供明确的并发语义\n高带宽比低延迟更重要\n\n#2.2 接口\nGFS 提供的文件系统接口有：create, delete, open, close, read, write, snapshot 和 record append。\nrecord append 是原子性的追加写操作，用于多个客户端并发写入同一个文件。\n#2.3 架构\nGFS 包括一个 master 节点，多个 chunkserver 节点。都是普通 Linux 机器上运行的用户态进程。\n文件会分成固定大小的 chunks，每个 chunk 有一个全局唯一的 64-bit 的 chunk handle 作为标识。\nchunkserver 将 chunks 保存在本地磁盘上，通过 chunk handle 去读写，可以只读写其中某一段。为了数据可靠性，每个 chunk 会被复制到多个 chunkserver 上，默认是 3 副本。\nmaster 负责元数据管理，包括 namespace, ACL 信息, 文件到 chunk 的映射，chunk 的位置信息。还负责控制全局的任务，例如 chunk 租约管理, 垃圾回收, chunk 迁移。\nmaster 通过 HeartBeat 消息给 chunkserver 分发任务和回收状态。\nGFS 客户端是一个用户态库的形式，和应用程序链接在一起。提供用户态的类文件系统接口。\n因为大多数应用程序都是流式访问大文件，客户端和 chunkserver 都不缓存数据。这样避免了缓存一致性问题，可以简化实现。因为 Linux 本身对磁盘文件会有缓存，chunkserver 也不缓存数据。\n#2.4 Master 设计\n单个 Master 可以简化设计，因为所有信息都保存在这个节点上，所以可以使用更高级的 chunk 的放置策略。但是必须注意最小化 Master 对读写的参与，避免单点形成系统瓶颈。客户端先问 Master 获取 chunk 的位置信息，然后直接和 chunkserver 通信。这个位置信息会缓存一段时间。\n客户端的读操作：\n\n由于 chunk 大小固定，客户端先将欲访问的 byte range 转换成 chunk index\n向 Master 请求 chunk 的位置信息。客户端缓存这个信息一段时间。\n客户端和最近的持有该 chunk 的 chunkserver 通信，获取实际数据。\n\n#2.5 Chunk 大小\n大 chunk 的好处：\n\n减少客户端和 Master 之间的交互次数\n应用程序对文件的操作具有局部性，可以减少与 chunkserver 的通信\n减小了元数据的大小，使之可以放在内存中\n\n坏处：\n\n对小文件不友好，容易导致局部热点。一个例子：一个 exe 文件写到 GFS 上，然后很多机器同时拉取这个文件去执行。临时解决方案是提高这个文件的备份数量，并且让读机器随机启动。长期解决方案是允许客户端之间 P2P 分发文件。\n\n#2.6 元数据\nMaster 节点保存三类元数据：\n\n文件和 chunk 的 namespace\n文件到 chunk 的映射\nchunk 的每个副本的位置信息。\n\n所有这些元数据都存在内存中。2 和 3 还会额外通过 Operation Log 的方式持久化到本地磁盘上。\n#2.6.1 内存数据结构\nMaster 会周期性地扫描所有的状态，用于当 chunkserver 故障的时候，实现 chunk 垃圾回收，再复制等操作；以及在 chunkserver 间平衡负载和磁盘空间。\n一个潜在的考虑是，整个系统可能被内存大小限制。但实际上没问题，每个 64MB 的 chunk 仅需要 64B 的元数据。\n相对于单点的种种好处（简单性、可靠性、性能、弹性），多花费一点内存，这个 trade-off 是值得的。\n#2.6.2 chunk 位置\nMaster 节点不持久化 chunk 的位置信息，而是在开机时从 chunkserver 处轮询一遍。\n如果持久化 chunk 位置信息，就需要额外的机制保证两者同步，增加了复杂度。\n另一个理解方式是: chunkserver 对于自己持有的 chunks 有最终决定权，因此 master 在保存一次位置信息是多余的。\n#2.6.3 Operation Log\nOperation Log 记录了对元数据的修改，是整个 GFS 的关键。\n因此我们需要保证元数据持久化完毕之前，不能暴露给客户端看到。\nGFS 会把 Operation Log 复制到多个远程机器上，而且只有所有机器上都落盘完成，才会回应客户端。\nMaster 会累计几条 Log，然后一次性刷入磁盘。\nMaster 机器会重放所有的 Operation Log 来恢复状态。为了加快恢复速度，Master 会定期生成 checkpoint。\nCheckpoint 采用了压缩 B 树的形式，可以快速加载到内存中。\n为了防止 Checkpoint 过程阻塞住 Master 的正常操作，Master 会先切换到一个新的 Log 文件，然后在单独的线程中生成 Checkpoint。\n老旧的 checkpoint 和 Log 可以直接删掉了，但实践中，会保存一段时间，以防止意外的故障。\n#2.7 一致性模型\nGFS 采用了一种宽松的一致性模型。\n#2.7.1 GFS 的承诺\n对于 namespace 的变更（例如文件创建）操作是原子的。对文件内容的修改不是原子的。我们首先定义一段数据区域是：\n\n\n一致的（consistent）：如果所有 client 都能读到一样的数据\n\n\n确定的（defined）：如果该区域是 consistent，而且 client 能够读到自己刚刚写入的数据\n\n\n串行写: 数据区域是一致且确定的，因为只有一个 client 写入。\n\n\n并发写: 是一致的但不是确定的，因为所有 client 都能读到一样的数据，但是不确定是哪一个写入。\n\n\n写失败: 会导致数据区域 inconsistent，因为 client 读到的数据不一样。\n\n\nGFS 有两种写的方法：\n\nwrite: 写入到指定的 offset\nrecord append: 原子地写入到文件末尾，at least once 语义, 返回写入的 offset\n\n#2.7.2 对应用程序的影响\nGFS 应用程序只要进行一些简单的修改，就可以适应这种一致性模型。\n\n依赖追加写而不是覆盖写\n做 checkpoint\n写能够自校验、自定位的数据\n\n#3. 系统交互\n设计的目标是尽可能减小 Master 在所有操作中的参与。\n#3.1 租约和变更顺序\n变更是一次修改文件数据或者 chunk 元数据的操作，例如写或者追加。变更需要在 chunk 的所有副本上进行。\nGFS 使用租约来维持多个副本之前修改顺序的一致性。\nMaster 会给一个 chunk 的一个副本颁发一个租约，称之为 primary。由 primary 选择一个所有副本之间修改的顺序，所有副本会按照这个顺序来修改。\n租约有 60 秒的有效期，但是如果时间不够，primary 可以请求 master 续约。续约通过 HeartBeat 消息来完成。\nMaster 可以随时撤销租约，例如当 primary 故障。\nGFS 的写过程如下：\n\n客户端向 Master 请求 chunk 的位置信息，包括谁是 primary 还有其他副本的位置。如果此时没有 primary，Master 会选择一个。\nMaster 回复 primary 的身份和其他副本的位置。客户端会缓存这个信息一段时间。除非 primary 故障，客户端不会重新请求 Master。\n客户端以任意顺序把数据推送到每一个副本上。每个 chunkserver 会把收到的数据先放进一个 LRU 缓存中，等待被取走或者过期。\n当所有的副本都确认收到了数据，客户端会通知 primary，发送一个写请求。然后 Primary 会给每个客户端的写请求分配一个序列号。\nPrimary 把写请求转发给所有涉及到的副本，每个其他副本按照 primary 执行的序列号顺序来从 LRU 缓存中取数据然后写入。\n其他副本写入成功后，会回复 primary。\nPrimary 收到所有回复后，会回复客户端。如果有副本写入失败，也会报告给客户端。\n\n如果一次写操作跨越了 chunk 边界，那么客户端会把这个操作分成多个写操作，然后按照上面的方式处理。因此可能导致修改区域处在一致但不确定的状态。\n#3.2 数据流\nGFS 将数据流和控制流解耦。\n为了最大限度利用机器网络带宽，数据是在 chunkserver 中链式传递而非星型传递。\n为了尽可能避免撞到网络瓶颈和慢链路，每台机器会优先把数据转发给最近的、没有收到过数据的机器。\n复用 TCP 连接发送数据以减小延迟。\n#3.3 原子记录追加\nGFS 提供了一种原子的追加写操作，称之为 record append。客户端只需要提供数据，GFS 会找到合适的地方写入然后返回 offset，GFS 提供 at least once 的语义。\nrecord append 仍然延续 GFS 的写过程，除了一点不同：\n\n客户端会把所有数据都推送到文件的最后一个 chunk。\nPrimary 会检查追加是否会导致新增 chunk，如果会，则会在当前 chunk 末尾填充空洞，其他副本同理，然后叫客户端去重试。\n否则，Primary 会把数据写入到 chunk 的最后，然后返回 offset。\n\n如果某个副本上执行追加失败，客户端会重试这个操作。因此，GFS 不保证所有副本都有相同的数据，只保证写操作 at least once。\n实际上，有可能会出现这种情况：primary 的数据比其他副本的数据要长。（待确认）\n#3.4 快照\n快照操作用于给文件或者目录快速创建一个副本，同时尽可能减小对写操作的中断。\n和 AFS 类似，GFS 使用了 CoW 的方式来实现快照。当 master 收到一个快照请求时，会先把所有涉及到的 chunk 的租期都收回。阻塞住后续的写操作，防止影响当前快照。\n在全部租期都被收回或者过期后，master 就会把这个操作（指快照操作）先写到磁盘上的 WAL 上，然后再改内存数据结构。\n新创建的快照文件会和原始文件指向相同的 chunk。当客户端第一次想要写入一个快照过的 chunk C 的时候，master 就会注意到 chunk C 的引用计数大于 1，此时 master 会进行 CoW：\n\n先创建一个新的 chunk handle C’；\n要求每个持有 C 的 chunkserver 复制一份 C 作为 C’。\n\n然后再正常处理写请求。\n这里要求 chunkserver 复制有一个好处是不需要占用网络带宽，因为复制是在 chunkserver 本地进行的。\n#4. Master 操作\nMaster 负责管理整个系统的元数据。\n#4.1 命名空间管理和锁\n很多 master 的操作是很慢的，例如打快照会导致撤回所有租约。\nGFS 通过减小锁的粒度来提高并发度。\nGFS 没有使用每个目录一个数据结构的方式表示文件和目录，而是采用了类似多级查找表的结构。由于文件系统是层级结构，使用前缀压缩可以节约很多空间。\n每一级目录都有一把读写锁，每次变更操作都需要获取从根目录到目标目录的所有锁。举例：\n\n创建/home/user/foo：需要 /home 和 /home/user 的读锁，/home/user/foo 的写锁；\n快照/home/user 到/save/user：需要 /home 和 /save 的读锁，/home/user 和 /save/user 的写锁；\n\n由于文件数量可能很多，读写锁对象都是动态懒分配的，并且一旦不用就会删掉。\n为了防止死锁，GFS 会按照目录的路径顺序，以及字典序来获取锁。\n#4.2 副本放置策略\nGFS 集群的网路拓扑是分层的，每个机房有多个机架，每个机架有多个机器。\nGFS 会尽量把副本放在不同的机架上，以防止单个机架故障。\n#4.3 创建、再备份和再平衡\n#chunk 创建\n当 Master 创建一个 chunk 时，需要决定这个 chunk 的副本放在哪些 chunkserver 上。主要考虑这些因素：\n\n优先放置在磁盘剩余空间较多的 chunkserver 上。\n优先放置在近期新文件较少的 chunkserver 上。因为 Google 的 workload 大多是 append-once-read-many 类型。\n分散副本，避免放在同一个机架上。\n\n#chunk 再备份\n当 chunk 的副本数量低于用户设定的目标数量时，Master 就会进行再备份。这种情况发生的原因有：chunkserver 掉线，chunkserver 数据损坏，或者用户期望提高。再备份时也会做一些优化，例如优先备份 live file，其次是最近删除的文件。当某些失败的 chunk 影响了应用程序运行时，GFS 会提高这些文件的优先级，加快恢复的过程。\nMaster 每次会选择一个优先级最高的 chunk，然后要求 chunkservers 去复制这个 chunk。新副本的放置策略和创建 chunk 时一样。为了防止再备份操作压倒客户端的流量，Master 会限制再备份的速度。\n#chunk 再平衡\nMaster 周期性地检查当前的副本分布，移动副本以达到更好的磁盘利用率和负载均衡。通过再平衡操作，Master 逐渐填满新的 chunkserver 而不是一口气填满。再平衡的策略也和前面类似。\n#4.4 垃圾回收\n文件删除之后，GFS 不会立即回收可用的物理空间。而是在文件和 chunk 级别进行懒惰回收。\n#4.4.1 机制\n在 GFS 上删除的文件会被重命名为一个名字包含删除时间戳的隐藏文件。这个文件在 Master 的定期扫描中会被发现，如果这个文件已经被删除超过 3 天，那么这个文件会被真正删除。在真正删除之前，这个隐藏文件都是可以被正常读的，而且可以被恢复（通过重命名为正常文件名）。当隐藏文件被真正删除，相关的内存中的元数据也会被删除。\n在对 chunk namespace 的日常扫描中，Master 会发现一些不再被引用的 chunk，这些 chunk 会在和 chunkserver 通信对数据的时候被发现并回收。\n#4.4.2 一些讨论\n采用懒惰回收的方式有一些好处：\n\n简单，可靠：避免删除文件失败导致数据删不掉\n均摊了删除操作的开销，不会给 Master 带来额外的压力\n增加误删除的恢复机会\n\n相对地，也有坏处：\n\n用户想要释放磁盘空间的时候，不能立即释放\n对于反复创建和删除临时文件的程序不友好\n\n一点小优化是，如果再删除一次已经删除的文件，就加速删除过程。\n#4.5 陈旧副本检测\n如果 chunkserver 挂了，然后丢失了变更，那么这个 chunk 的副本就会变成陈旧的。\nMaster 会维护一个每个 chunk 的版本号来区分陈旧的副本。\nMaster 每一次授权租期的时候，都会让该 chunk 的版本号加一，然后提醒所有最新的副本。Master 和副本都会将新的版本号持久化。如果某一个副本挂了，Master 下一次重启的时候就会发现这个副本的版本号比其他的小，然后会把这个副本标记为陈旧的。如果 Master 看到了一个更大的版本号，说明 Master 在授权租期的时候挂了，因此就会把大的版本号当成最新的（没看懂，存疑）。\nMaster 在垃圾回收过程中移除陈旧的副本。为了防止客户端读到陈旧的数据，在实际移除之前，Master 也会假装陈旧的副本是不存在。除此以外，Master 在回复客户端 Primary 身份、以及叫 chunkserver 复制 chunk 的时候，也会带上这个 chunk 的版本号，以便客户端和 chunkserver 去校验，确保读到的数据是最新的。\n#5. 容错和诊断\n#5.1 高可用\nGFS 集群保持高可用的两个策略是：快速回复和复制。\n#5.1.1 快速恢复\nMaster 和 chunkserver 都设计成可以快速重启的（几秒钟恢复状态）。即使重启了，对其他部分的影响也仅仅是小卡顿一下，直到网络请求超时重试。\n#5.1.2 chunk 复制\n每个 chunk 会被复制到多个机架的多个 chunkserver 上。用户可以为不同的目录设置不同的副本数量，以适应不同的容错需求。默认是 3 副本。\n除了复制外，GFS 还在探索使用一些跨服务器的冗余技术，例如奇偶校验码等。\n#5.1.3 Master 复制\n为了可靠性，Master 状态也有副本。Master 的 Operation log 和快照都会被复制到多个机器上。一条变更只有在所有机器上都落盘成功后，才会认为是已提交。如果机器挂了，守护进程会在其他持有 logs 的机器上新建一个 Master 进程。客户端只会用 Master 的 canonical name 来连接 Master，一般是一个 DNS 名字，方便 Master 迁移。\nMaster 会有一些只读副本，对外提供读服务，用于增强系统的读性能。这些只读副本会读取备份机器上的 Operation log 并重放，因此会稍微落后于 Master。只读副本启动时也会轮询一遍所有的 chunkserver。\n#5.2 数据完整性\n每个 chunkserver 都会用校验码来校验自己保存的 chunk 数据的完整性。注意 GFS 不承诺多个副本之间的数据一致性，只保证 at least once 语义。\n具体来说，每个 chunk 会拆分成 64KB 的块，每个块会计算一个 32 位的校验码。校验码会在内存和磁盘上都保存一份。\n每次有客户端或者其他 chunkserver 读数据的时候，chunkserver 会读取（读取范围覆盖到的）数据块然后验证校验码。如果校验码不对，chunkserver 不会返回数据，而是会报错。请求方就会去其他副本上读取数据。当找到一个正确的数据块后，Master 就会通知这个 chunkserver 删掉这个坏的副本。\n校验码对读性能的影响很小。而且客户端代码在读取的时候会对齐到块边界，\n校验码计算针对追加写操作做了深度的优化。GFS 的校验码实现了增量计算，只需要计算新增的数据块的校验码。\n对于覆盖写操作，需要先读取写入范围的起始和结束块，进行校验，然后再写入新的数据，计算新的校验码。如果不检查起始和结束块的校验码，那么可能会导致数据不一致。\n当 chunkserver 空闲的时候，可以扫描和校验不活跃的 chunk，以检验很少读的 chunk 的完整性。如果发现坏的数据块，Master 可以通知 chunkserver 删掉这个坏的副本。\n#5.3 诊断工具\n为了方便 debug，GFS 生成诊断日志记录关键事件和所有的网络请求和响应。\n\n#GFS - FAQ\n翻译自 MIT 6.824.\n#Q1. 只有一个 Master 是好的设计吗？\n长期来看不是。Google 发表在 ACM Queue 上的文章 GFS: Evolution on Fast Forward 中提到了，在长期实践中，GFS 的很多设计都是有问题的。文件的数量一直增长，把所有文件元数据都放在单点 Master 的内存中是不合理的。随着客户端的数量增长，单点 Master 的 CPU 也会逐渐变得不够用。实际上，GFS 切换 Master 需要人工干预，因此很慢。下一代 GFS，Colossus，已经用了多个 Master，而且有更加自动化的 Master HA。\n#Q2. GFS 的 record append 为什么是 at least once 语义？\n在第 3.1 节的第 7 步中提到，如果写入在任意副本上失败，客户端会重试。这会导致数据在没有失败的副本上被追加多次。另一种设计方案是对客户端请求做去重，不管发生任意故障（例如，在客户端第一次请求和重试之间，Primary 故障）你将在实验 3（指 6.824 的 Raft）中实现这样的设计，但这会显著增加复杂性和性能开销。\n#Q3. 应用程序怎么识别 chunk 中的 padding 和 duplicate record？\n为了检测 padding，应用程序可以在有效记录的开头放置一个可预测的 magic number，或者包含一个 checksum，该校验和仅在记录有效时才会有效。\n应用程序可以通过在记录中包含 Unique ID 来检测重复记录。然后，如果它读取到一个与之前记录具有相同 ID 的记录，它就知道这些记录是重复的。\nGFS 为应用程序提供了一个库来处理这些情况。GFS 设计的这一方面实际上将复杂性从 GFS 转移到了应用程序，这可能并不理想。\n#Q4. 既然 record append 写入的位置是不确定的，那么应用程序如何找到他们写入的数据？\n追加操作（以及 GFS 整体）主要面向那些需要顺序读取整个文件的应用程序。这类应用程序会扫描文件以查找有效记录（参见 Q3），因此它们不需要提前知道记录的具体位置。例如，文件可能包含一组并发网络爬虫遇到的 URL。任何特定 URL 的文件偏移量并不重要；读取者只需要能够读取整个 URL 集合即可。\n#Q7. 基于 POSIX API 开发的应用程序适配到 GFS 上需要修改吗？\n是的，但是 GFS 是为了新开发的程序设计的，例如 Google 的 MapReduce。\n#Q10. 假如 S1 是某一个 chunk 的 Primary，然后 Master 和 S1 之间的网断了。Master 会选另一个 S2 作为 Primary。因为 S1 没有实际挂掉，那么系统中会有两个 Primary 吗？\n这样会出大问题，因为会有两个 Primary 会同时进行不同的更新操作。幸运的是，GFS 的租约设计避免了这种情况。Master 给 S1 一个 60s 的租约。\nS1 会在过期之后主动退位。S1 退位之前，Master 不会给其他 chunkserver 发送租约。所以 S1 停止之前，S2 不会成为 Primary。\n（PS. 但是怎么保证 Master 和 S1 的时间同步呢？）\n#Q12. Google 还在用 GFS 吗？\n传言说 GFS 已经被 Colossus 取代了。它们的设计目标一直，性能和容错性都有所提高。此外，很多 Google 内部的应用程序已经迁移到了 BigTable、Spanner 之类的类数据库系统。然而，大部分 GFS 的设计都跑到了 HDFS 里面。\n拓展阅读：Colossus under the hood: a peek into Google’s scalable storage system\n\n#Q13. GFS 为了性能和简单性而牺牲正确性，这在多大程度上是可以接受的？\n这是分布式系统中一个反复出现的主题。强一致性通常需要复杂的协议，并且需要通信和等待回复（正如我们将在接下来的几节课中看到的那样）。通过利用特定应用程序类别可以容忍弱一致性的特点，可以设计出具有良好性能和足够一致性的系统。例如，GFS 针对 MapReduce 应用程序进行了优化，这些应用程序需要对大文件进行高性能读取，并且能够容忍文件中的空洞、记录多次出现以及不一致的读取。另一方面，GFS 并不适合用于存储银行的账户余额。\n#Q16. 内部碎片是什么？懒惰分配有什么好处？\n内部碎片是指当系统使用的分配单元大于请求分配所需的空间时，浪费的空间。如果 GFS 以 64MB 为单位分配磁盘空间，那么一个 1 字节的文件将浪费近 64MB 的磁盘空间。\nGFS 通过懒惰分配磁盘空间来避免这个问题。每个数据块都是一个 Linux 文件，而 Linux 文件系统使用几十 KB 的块大小；因此，当应用程序创建一个 1 字节的 GFS 文件时，该文件的数据块仅消耗一个 Linux 磁盘块，而不是 64MB。\n(PS. 估计就是用 fallocate，实际写入数据时才真正分配空间)\n#Q17. GFS 从弱一致性上获得了什么好处？\n更容易理解的角度是，假如 GFS 需要实现更强的一致性，需要补充的工作。\nPrimary 不应让 Secondaries 执行写入操作，除非所有 Secondaries 都能够完成该操作。这可能需要两轮通信：\n\n第一轮询问所有 Secondaries 是否存活并能够承诺在请求时执行写入操作，以及（如果所有 Secondaries 都回答“是”）\n第二轮通知 Secondaries 提交写入操作。\n\n如果 Primary 挂了，某些 Secondaries 可能错过了 Primary 发送的最后几条更新消息。这将导致剩余的 Secondaries 拥有略微不同的副本。在恢复操作之前，新的 Primary 应确保所有 Secondaries 拥有完全相同的副本。\n由于客户端在怀疑出现问题时会重新发送请求，Primary 需要过滤掉已经执行过的操作。\n客户端缓存了数据块的位置，并且可能向持有过时副本的 chunkserver 发送读取请求。GFS 需要一种方式来确保这种读取不会成功。\n#参考\n\nThe Google File System\nGFS —— 取舍的艺术\n\n","categories":["distributed-system"],"tags":["分布式系统","GFS"]},{"title":"[MSST'10] The Hadoop Distributed File System 论文阅读","url":"/2025/02/25/the-hadoop-distributed-file-system/","content":"#0. 摘要\nHDFS 的设计目标是可靠存储大数据集，并提供高吞吐量的数据访问。\n#1. 引言和相关工作\nHadoop 提供了分布式的文件系统和使用 MapReduce 范式来分析和处理大数据集的框架。\n和其他系统类似，HDFS 把元数据保存在单独的服务器上，称为 NameNode。应用数据保存在其他服务器上，称为 DataNode。\nHDFS 的 DataNode 不使用类似 RAID 的数据保护机制。而是采用多副本的方式来保证数据的可靠性。\n某些分布式存储系统设计了分布式的系统来管理 namespace。\nCeph 使用动态子树分区算法来管理元数据。\nGFS 使用多个 Master 来管理元数据。\nLustre 2.2 使用了聚类 namespace。\n#2. 架构\n#A. NameNode\nHDFS 的名字空间是文件和目录的树形结构。文件和目录在 NameNode 中表示成 inode，inode 包含文件的属性，例如权限、修改和访问时间、namespace 和磁盘配额。文件内容被分割成 128MB 大小的块，每个块被独立备份保存到 3 个 DataNode 上。\nNameNode 维护着 namespace 树和块到 DataNode 的映射。\nHDFS 客户端首先向 NameNode 请求文件的块位置，然后和最近的 DataNode 通信读取块数据。当写数据的时候，客户端首先请求 NameNode 分配三个块，然后以 pipeline 的方式将数据写入到 DataNode 上。\nHDFS 把整个 namespace 保存在内存中。\nnamespace 的元数据包含 inode 数据和每个文件对应的块列表，称为 image。\nimage 保存在本地文件系统上，称为 checkpoint。\nNameNode 还额外保存 image 的变更日志，称为 journal。\n#B. DataNodes\nDN 使用两个本地文件保存一个块的数据：一个文件保存块数据，另一个文件保存块的元数据，包括 checksum 和 generation stamp。\nDN 启动的时候，会先和 NN 连接然后进行一次 handshake，NN 会验证 DN 的 namespace ID 和软件版本是否匹配，如果任意一个不匹配，DN 就会退出。\nnamespace ID 会在格式化文件系统的时候生成，永久保存在集群的所有节点上。\nID 不同的节点不能加入到这个集群中，保证了集群的一致性。新初始化的 DN 没有 namespace ID，可以加入任何集群。\nhandshake 之后，DN 会向 NN 注册自己。NN 会给 DN 分配一个 storage ID，这个 ID 是 DN 的内部 ID，保证即使 DN 的 IP 地址或者端口改变， NN 也能识别出这个 DN。\nDN 会定期给 NN 发送 block report 报告自己拥有的 block 副本。\nblock report 中包含 block id，generation stamp 还有每个 block 的长度。第一次注册的时候，会立即发送一次，随后每小时发送一次。\n正常操作期间，DN 给 NN 发送 heartbeat，确认自己还活着。默认每 3 秒发送一次。如果 NN 在 10 分钟内没有收到 DN 的 heartbeat，就会认为这个 DN 挂了，随后开始安排数据复制过程。\nDN 的 heartbeat 也包含了一些统计信息，例如总的容量、剩余容量、正在传输的数据量等。用于 NN 做一些决策时参考。\nNN 不会直接向 DN 发送命令，而是通过对 heartbeat 的响应来发送命令。命令包括：\n\n复制 block 到另一个节点；\n删除本地的 block；\n重新注册或者退出；\n立即发送 block report。\n\nNN 每秒能处理数千的 heartbeat，不会影响其他的 NN 操作。\n#C. HDFS 客户端\n应用程序通过 HDFS 客户端库访问 HDFS。\nHDFS 客户端支持读取、写入、删除文件，创建和删除目录。\n读文件时，HDFS 客户端先询问 NN 获取文件的块所在的 DN 的列表。然后联系 DN 请求所需的 block。写文件时，HDFS 客户端先询问 NN 选择三个 DN 保存第一个 block，然后客户端负责组织一个 pipeline 然后开始写入。当第一个 block 写完后，客户端继续询问 NN 请求下一个 block 的 DN。\nHDFS 提供了一个 API 暴露 block 位置给应用程序，允许 MapReduce 框架把任务调度到数据所在的节点上，减少网络传输。\nHDFS 也允许应用程序设置文件的副本数，如果一个文件需要高频率访问，可以增加副本数。\n#D. Image 和 Journal\n文件系统目录结构的元数据称为 image。对 image 的每一次持久化记录是一个 checkpoint。\njournal 是文件系统目录结构的 WAL。对于每一次客户端事务，NN 会先把操作写入 journal 中，flush 和 sync 之后，才会返回给客户端。\ncheckpoint 文件创建后不会修改，只会被新的替换。在启动的时候，NN 会读取最新的 checkpoint 和 journal，然后恢复到最新的状态。恢复后，新的 checkpoint 和空的 journal 会被创建。\n如果 checkpoint 或者 journal 损坏，整个文件系统会被不可用，因此这两个文件可以配置为保存多个副本。推荐的配置是放置在不同的 volume 上，还有一个放在远程的 NFS 上。\nNN 是一个多线程系统，可以同时处理多个客户端的请求。\nflush 和 sync 操作就变成了一个瓶颈，因为它们是同步的。\nNN 会把不同客户端发起的操作合并成一个 batch，只要有一个线程完成了 flush 和 sync，其他线程就可以继续处理下一个请求。\n#E. CheckpointNode\nNN 节点除了服务客户端请求外，还可以执行其他两种任务之一：CheckpointNode 和 BackupNode。执行哪一个任务在节点启动时执行。\nCheckpointNode 通常运行在和 NN 不同的节点上，因为它和 NN 一样需要大量的内存。它会从 NN 下载当前的 checkpoint 和 journal，然后进行 compaction，将合并后的 checkpoint 返回给 NN。\n创建新的 checkpoint 会导致 NN 截断 journal。\n#F. BackupNode\nBackupNode 会和 NN 保持同步。\n#参考资料\n\nHdfsDesign\n\n","categories":["distributed-system"],"tags":["分布式系统","大数据","HDFS"]},{"title":"线性代数 - 概念总结","url":"/2021/01/08/linear-algebra/","content":"#1. 行列式\n#基本概念\n二阶行列式\n三阶行列式\n全排列\n对换\nn 阶行列式\n上下三角行列式\n对角行列式\n转置行列式\n#性质\n\n转置行列式, 行列式不变\n对换行(列), 行列式变号\n\n两行(列)完全相同, 行列式=0\n两行(列)成比例, 行列式=0\n\n\n一行(列)乘 k = 整体乘 k\n\n行(列)系数提到外面, 行列式不变\n\n\n可以按照某一行(列)分成两个行列式相加\n一行乘 k 加到另一行, 行列式不变\n\n#按行(列)展开\n\n余子式 MijM_{ij}Mij​\n代数余子式 Aij=(−1)i+jMijA_{ij}=(-1)^{i+j}{M_{ij}}Aij​=(−1)i+jMij​\n\n#性质\n\n\n按行(列)展开法则 D=∑i=1naijAij,(j=1,2,...,n)=∑j=1naijAij,(i=1,2,...,n)D=\\sum_{i=1}^{n}{a_{ij}A_{ij}}, (j=1,2,...,n)=\\sum_{j=1}^{n}{a_{ij}A_{ij}}, (i=1,2,...,n)D=∑i=1n​aij​Aij​,(j=1,2,...,n)=∑j=1n​aij​Aij​,(i=1,2,...,n)\n\n\n如果上式中aaa和AAA的j(i)j(i)j(i)错开, 则结果为 0\n\n\n#范德蒙德行列式\nDn=∣11...1x1x2...xnx12x22...xn2:::x1n−1x2n−1...xnn−1∣=∏n≥i≥j≥1(xi−xj)(注意顺序)D_n=\\begin{vmatrix}\n1&amp;1&amp;...&amp;1 \\\\\nx_1&amp;x_2&amp;...&amp;x_n \\\\\nx_1^2&amp;x_2^2&amp;...&amp;x_n^2 \\\\\n:&amp;:&amp; &amp;: \\\\\nx_1^{n-1}&amp;x_2^{n-1}&amp;...&amp;x_n^{n-1}\n\\end{vmatrix}\n=\\prod_{n\\ge i\\ge j\\ge 1}{(x_i-x_j)}\n(注意顺序)\nDn​=​1x1​x12​:x1n−1​​1x2​x22​:x2n−1​​............​1xn​xn2​:xnn−1​​​=n≥i≥j≥1∏​(xi​−xj​)(注意顺序)\n#2. 矩阵及其运算\n线性方程组\n非齐次线性方程组\n齐次线性方程组\n齐次线性方程组的零解\n#矩阵\n同型矩阵\n系数矩阵\n未知数矩阵\n常数项矩阵\n增广矩阵\n线性变换\n#按性质\n\n非奇异矩阵/可逆矩阵/满秩矩阵\n奇异矩阵/不可逆矩阵/降秩矩阵\n\n#按形状\n\n对角矩阵\n单位矩阵\n对称矩阵\n\n#矩阵运算\n\n矩阵加法\n\n负矩阵\n\n\n数乘\n矩阵乘法\n\n可交换\n纯量阵: 一定可交换\n\n\n幂\n转置\n行列式\n伴随矩阵\n矩阵多项式\n\n#性质\n#转置性质\n\n自反性 (AT)T=A(A^{T})^{T}=A(AT)T=A\n线性 (λA+B)T=λAT+BT(\\lambda A+B)^{T}=\\lambda A^{T}+B^{T}(λA+B)T=λAT+BT\n嵌套 (AB)T=BTAT(AB)^{T}=B^{T}A^{T}(AB)T=BTAT\n判零 ATA=O⇔A=OA^TA=O\\Leftrightarrow A=OATA=O⇔A=O\n\n#行列式性质\n\n常数放大 ∣λA∣=λn∣A∣\\left|\\lambda A\\right|=\\lambda^n\\left|A\\right|∣λA∣=λn∣A∣\n可乘性 ∣AB∣=∣A∣∣B∣\\left|AB\\right|=\\left|A\\right|\\left|B\\right|∣AB∣=∣A∣∣B∣\n\n#伴随矩阵性质\n\n与转置可交换 (AT)∗=(A∗)T(A^T)^*=(A^*)^T(AT)∗=(A∗)T\n与逆可交换 (A−1)∗=(A∗)−1(A^{-1})^*=(A^*)^{-1}(A−1)∗=(A∗)−1\n嵌套 (AB)∗=B∗A∗(AB)^*=B^*A^*(AB)∗=B∗A∗\n常数部分放大 (λA)∗=λn−1A∗(\\lambda A)^*=\\lambda^{n-1}A^*(λA)∗=λn−1A∗\n\n#伴随矩阵的行列式和秩\n\n∣A∗∣=∣A∣n−1\\left|A^*\\right|=\\left|A\\right|^{n-1}∣A∗∣=∣A∣n−1\nR(A∗)={n,R(A)=n (满秩不变)1,R(A)=n−1 (差1补1)0,R(A)&lt;n−1 (否则为0,A∗=0)R(A^*)=\\begin{cases}n,R(A)=n~(满秩不变)\\\\1,R(A)=n-1~(差1补1)\\\\0,R(A)&lt;n-1~(否则为0, A^*=0)\\end{cases}R(A∗)=⎩⎨⎧​n,R(A)=n (满秩不变)1,R(A)=n−1 (差1补1)0,R(A)&lt;n−1 (否则为0,A∗=0)​\n\n#矩阵分块法\n分块矩阵\n子块\n分块对角矩阵\n\n性质\n\n行列式 ∣A∣=∣A1∣∣A2∣...∣An∣\\left|A\\right|=\\left|A_1\\right|\\left|A_2\\right|...\\left|A_n\\right|∣A∣=∣A1​∣∣A2​∣...∣An​∣\n逆\n\n\n\n#3. 初等变换与线性方程组\n#初等变换\n\n\n行(列)变换 等同于左(右)乘同样变化之后的EEE\n\n对换两行(列)\n缩放某一行(列)加到另一行(列)上去 (源和目标可以一样)\n\n\n\n初等矩阵: 施加了一次初等行(列)变换的 E\n\n\n#矩阵等价\n\nA 和 B 等价: A 经若干次初等变换可以变成 B\n等价关系: 自反性, 对称性, 传递性\n推论: A∼(r)∼E⇔A可逆A\\sim (r)\\sim E\\Leftrightarrow{A可逆}A∼(r)∼E⇔A可逆\n\n#充要条件\n\nA∼(r)∼B⇔PA=B(P,Q可逆)A\\sim (r)\\sim B\\Leftrightarrow PA=B (P, Q可逆)A∼(r)∼B⇔PA=B(P,Q可逆)\nA∼(c)∼B⇔AQ=B(P,Q可逆)A\\sim (c)\\sim B\\Leftrightarrow AQ=B (P, Q可逆)A∼(c)∼B⇔AQ=B(P,Q可逆)\nA∼B⇔PAQ=B(P,Q可逆)A\\sim B\\Leftrightarrow PAQ=B (P, Q可逆)A∼B⇔PAQ=B(P,Q可逆)\n\n#行阶梯形矩阵\n左下角的 0 组成了一个从右下到左上的阶梯, 这个阶梯的宽度任意, 但是每一级的高度必定为 1(两边不算)\n非零首元: 非零行的首个非零元素\n(x....0x...00x..000x.)(x....000x.0000000000)\\begin{pmatrix}\n x &amp; . &amp; . &amp; . &amp; .\\\\\n 0 &amp; x &amp; . &amp; . &amp; .\\\\\n 0 &amp; 0 &amp; x &amp; . &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; x &amp; .\n\\end{pmatrix}\n\\begin{pmatrix}\n x &amp; . &amp; . &amp; . &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; x &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\n\\end{pmatrix}\n​x000​.x00​..x0​...x​....​​​x000​.000​.000​.x00​..00​​\n#行最简形矩阵\n在行阶梯型矩阵的基础上, 非零首元都是 1, 非零首元的上下都是 0, 则为行最简形矩阵\n(1000.0100.0010.0001.)(1..0.0001.0000000000)\\begin{pmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; .\\\\\n 0 &amp; 1 &amp; 0 &amp; 0 &amp; .\\\\\n 0 &amp; 0 &amp; 1 &amp; 0 &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; 1 &amp; .\n\\end{pmatrix}\n\\begin{pmatrix}\n 1 &amp; . &amp; . &amp; 0 &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; 1 &amp; .\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\n\\end{pmatrix}\n​1000​0100​0010​0001​....​​​1000​.000​.000​0100​..00​​\n\n一个矩阵的行最简形矩阵是唯一的\n\n#标准形\n对行最简形矩阵施加初等列变换, 是非零首元依次排列在左边, 右边全 0\n特点是左上角是个 E\n(10000010000010000010)(10000010000000000000)\\begin{pmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\n\\end{pmatrix}\n\\begin{pmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\n\\end{pmatrix}\n​1000​0100​0010​0001​0000​​​1000​0100​0000​0000​0000​​\n#技巧\n#P(A,B)=(A′,B′)P(A,B)=(A&#x27;,B&#x27;)P(A,B)=(A′,B′)\n\nP(A,E)=(E,A−1)P(A, E)=(E, A^{-1})P(A,E)=(E,A−1), 求 A 逆常用方法\nP(A,B)=(E,A−1B)P(A, B)=(E, A^{-1}B)P(A,B)=(E,A−1B), 解方程常用方法\n\n#矩阵的秩\n\n子式: 从行中选出一个子序列, 再从列中选出一个子序列, 得到的结果\n秩: 最高阶非零子式的阶数\n\n#性质\n\n秩一定小于行(列)数\n转置不改变秩\n等价矩阵的秩一样: A∼B⇔R(A)=R(B)A\\sim B\\Leftrightarrow{R(A)=R(B)}A∼B⇔R(A)=R(B)\n乘可逆矩阵不改变秩\n拼接矩阵的秩可能比原秩要大: max{R(A),R(B)}≤R(A  B)max\\{R(A),R(B)\\}\\le{R(A~~B)}max{R(A),R(B)}≤R(A  B)\n拼接矩阵的秩不会大于原秩之和: R(A  B)≤R(A)+R(B)R(A~~B)\\le{R(A)+R(B)}R(A  B)≤R(A)+R(B)\n\n\n\n秩和大于和秩(由上一条可证): R(A+B)≤R(A)+R(B)R(A+B)\\le{R(A)+R(B)}R(A+B)≤R(A)+R(B)\n矩阵积的秩不大于任何一个原秩(从变换角度看): R(AB)≤min{R(A),R(B)}R(AB)\\le{min\\{R(A),R(B)\\}}R(AB)≤min{R(A),R(B)}\nAm×nBn×l=O⇒R(A)+R(B)≤nA_{m\\times n}B_{n\\times l}=O\\Rightarrow{R(A)+R(B)\\le{n}}Am×n​Bn×l​=O⇒R(A)+R(B)≤n\n\n\n\n\n列满秩: 秩等于列数, 对应的行最简形矩阵为(EnO)m×n\\begin{pmatrix}E_{n} \\\\ O\\end{pmatrix}_{m\\times n}(En​O​)m×n​\n\n乘法消去律: AB=O,A列满秩⇒B=OAB=O, A列满秩\\Rightarrow B=OAB=O,A列满秩⇒B=O\n\n#秩与线性方程组的解\n\n相容: 有解\n不相容: 无解\n判断条件Ax=bAx=bAx=b\n无解⇔R(A)&lt;R(A  b)\\Leftrightarrow{R(A)\\lt{R(A~~b)}}⇔R(A)&lt;R(A  b)\n有解⇔R(A)=R(A  b)\\Leftrightarrow{R(A)={R(A~~b)}}⇔R(A)=R(A  b)\n\n有唯一解(齐次就是零解)⇔R(A)=R(A  b)=n\\Leftrightarrow{R(A)=R(A~~b)=n}⇔R(A)=R(A  b)=n\n有无穷解(齐次有非零解)⇔R(A)=R(A  b)&lt;n\\Leftrightarrow{R(A)=R(A~~b)\\lt n}⇔R(A)=R(A  b)&lt;n\n\n\n\n#4. 向量组, 线性相关性\n\nn 维向量\n实向量\n复向量\n单位坐标向量: E 的列向量\n\n#向量组\n#线性表示\n\n线性组合\n线性表示: 一个向量(组)能被另一个向量组线性表示\n向量组等价: 两个向量组能互相线性表示\n\n#定理\n\n\n向量 b 能被向量组 A 线性表示 ⇔R(A)=R(A  b)\\Leftrightarrow{R(A)=R(A~~b)}⇔R(A)=R(A  b)\n\n\n向量组 A 能被向量组 B 线性表示 ⇔R(A)=R(A  B)\\Leftrightarrow R(A)=R(A~~B)⇔R(A)=R(A  B)\n\n\n向量组 A, B 能相互线性表示(等价) ⇔R(A)=R(B)=R(A  B)\\Leftrightarrow{R(A)=R(B)=R(A~~B)}⇔R(A)=R(B)=R(A  B)\n\n\n\n\n\n若AB=C,则C的列向量组能被A的列向量组线性表示,表示的系数为B若AB=C, 则C的列向量组能被A的列向量组线性表示, 表示的系数为B若AB=C,则C的列向量组能被A的列向量组线性表示,表示的系数为B\n\n\n同理,C的行向量组能被B的行向量组线性表示,表示的系数为A同理, C的行向量组能被B的行向量组线性表示, 表示的系数为A同理,C的行向量组能被B的行向量组线性表示,表示的系数为A\n\n\n向量组B能被向量组A线性表示⇔R(B)=R(B  A)⇒R(B)≤R(A)向量组B能被向量组A线性表示\\Leftrightarrow R(B)=R(B~~A)\\Rightarrow{R(B)\\le R(A)}向量组B能被向量组A线性表示⇔R(B)=R(B  A)⇒R(B)≤R(A)\n\n\n#线性相关\n线性相关: 存在一个系数非全零的线性组合=0 的向量组\n线性无关\n#性质\n\n线性相关⇔R(A)&lt;m线性相关\\Leftrightarrow{R(A)\\lt m}线性相关⇔R(A)&lt;m\n线性无关⇔R(A)=m线性无关\\Leftrightarrow{R(A)=m}线性无关⇔R(A)=m\n线性相关具有保持性质线性相关具有保持性质线性相关具有保持性质\n秩&lt;向量个数⇔线性相关秩\\lt向量个数\\Leftrightarrow{线性相关}秩&lt;向量个数⇔线性相关\n向量维数&lt;向量个数⇒线性相关向量维数\\lt向量个数\\Rightarrow{线性相关}向量维数&lt;向量个数⇒线性相关\n线性无关向量组A+b变线性相关⇒b能被A唯一线性表示线性无关向量组A+b变线性相关\\Rightarrow{b能被A唯一线性表示}线性无关向量组A+b变线性相关⇒b能被A唯一线性表示\n\n#向量组的秩\n向量组\n最大(线性)无关(向量)组\n秩: 最大无关组的向量个数\n#性质见上节\n#线性方程组解的结构\n\n齐次: 解的线性组合仍是解\n因此只要找到解集的一个最大无关组即可得出所有解\n\n#齐次线性方程组 Ax=OAx=OAx=O\n\n基本解系: 齐次的解集的一个最大无关组\n基本解系的秩:\n\n对于方程组Am×nx=0,RS=n−R(A)A_{m\\times n}x=0, R_S=n-R(A)Am×n​x=0,RS​=n−R(A)\n\n\n求解步骤:\n\n对 A 进行行变换变成行最简形矩阵\n得到 x 之间的关系\n给每个自由变量赋值一个线性无关的向量(一般取单位向量)\n\n\n\n例题:\n\n#性质\n\nAx=OAx=OAx=O 和 Bx=OBx=OBx=O 同解 ⇒R(A)=R(B)\\Rightarrow R(A)=R(B)⇒R(A)=R(B)\n\n#非齐次线性方程组 Ax=bAx=bAx=b\n\n非齐次: 利用齐次\n找到一个特解, 加上齐次的通解即为最终解\n\n#向量空间\n\n向量空间: 对线性运算封闭的集合\n子空间: 含于另一个向量空间的向量空间\n基: 在向量空间中, 可以线性表示空间中任一向量的线性无关的向量组\n自然基: RnR^nRn中的单位坐标向量组\n维数: 基的向量个数(固定)\n坐标: 一个向量在某一个基下的表示\n基变换公式: 用一个基来表示另一个基的坐标(没啥用)\n\nB=AP=AA−1BB=AP=AA^{-1}B\nB=AP=AA−1B\n\n过渡矩阵: 旧基逆乘新基, 可以用初等变换的方法快速求\n\nP=A−1BP=A^{-1}B\nP=A−1B\n\n坐标变换公式: 一个向量在两个不同基下的坐标的关系式\n\n新坐标 Z=P−1Y新坐标~Z=P^{-1}Y\n新坐标 Z=P−1Y\n#5. 相似矩阵与二次型\n\n内积: 数量积的推广, [A,B]=ATB[A,B]=A^TB[A,B]=ATB\n长度(范数): 模的推广, ∣∣x∣∣=[x,x]\\left|\\left|x\\right|\\right|=\\sqrt{[x,x]}∣∣x∣∣=[x,x]​\n投影: c=[a,b][b,b]bc=\\frac{[a,b]}{[b,b]}bc=[b,b][a,b]​b\n单位向量\n单位化\n夹角: θ=arccos⁡[x,y]∣∣x∣∣ ∣∣y∣∣\\theta=\\arccos{\\frac{[x,y]}{\\left|\\left|x\\right|\\right|~\\left|\\left|y\\right|\\right|}}θ=arccos∣∣x∣∣ ∣∣y∣∣[x,y]​\n\n#正交矩阵\n\n正交: 夹角为 0\n正交向量组: 一组向量两两正交, 必定线性无关\n标准正交基: 单位向量组成的正交向量组\n标准正交化, 施密特正交化\n正交(矩)阵: 单位正交向量组构成的矩阵\n\nAAT=E=ATAAA^T=E=A^TAAAT=E=ATA\nAT=A−1A^T=A^{-1}AT=A−1\n∣A∣=1或−1\\left|A\\right|=1或-1∣A∣=1或−1\n正交矩阵的逆(转置)也是正交矩阵\n正交矩阵的积还是正交矩阵\n\n\n正交变换: 若PPP是正交矩阵, 则线性变换y=Pxy=Pxy=Px称为正交变换\n\n变换前后的长度不变 ∣∣y∣∣=yTy=xTPTPx=xTx=∣∣x∣∣||y||=\\sqrt{y^Ty}=\\sqrt{x^TP^TPx}=\\sqrt{x^Tx}=||x||∣∣y∣∣=yTy​=xTPTPx​=xTx​=∣∣x∣∣\n\n\n\n#特征值和特征向量\n特征多项式: f(λ)=∣A−λE∣f(\\lambda)=\\left|A-\\lambda E\\right|f(λ)=∣A−λE∣\n特征值/特征向量: A为n阶矩阵,满足Ax=λx,(A−λE)x=0的λ和xA为n阶矩阵, 满足Ax=\\lambda x, (A-\\lambda E)x=0的\\lambda和xA为n阶矩阵,满足Ax=λx,(A−λE)x=0的λ和x\n个数: f(λ)=0是一元n次方程,λ必有n个根,所以方程有n个解f(\\lambda)=0是一元n次方程, \\lambda必有n个根, 所以方程有n个解f(λ)=0是一元n次方程,λ必有n个根,所以方程有n个解\n#性质\n\n特征值之和等于矩阵的迹\n特征值之积等于矩阵的行列式\n矩阵多项式的特征值等于矩阵特征值的多项式: f(λ)f(\\lambda)f(λ)是f(A)f(A)f(A)的特征值\n特征值各不相等 -&gt; 特征向量线性无关\n对角矩阵的对角元就是其特征值\n\n#求法\n\n根据特征多项式=0 求出特征值\n将每个特征值代回原方程, 写出特征向量(个数=n-特征值重数)\n\n#相似矩阵\n相似矩阵: P−1AP=BP^{-1}AP=BP−1AP=B\n相似变换: P−1APP^{-1}APP−1AP\n#性质\n\n特征多项式(f(λ)=∣A−λE∣f(\\lambda)=\\left|A-\\lambda E\\right|f(λ)=∣A−λE∣)相同\n特征值相同\n迹相同\n计算矩阵多项式: φ(A)=Pφ(B)P−1\\varphi(A)=P\\varphi(B)P^{-1}φ(A)=Pφ(B)P−1\n\n一般 B 是对角矩阵, 从而可以快速计算 B 的幂:\n\n\n\nφ(Λ)=(φ(λ1)φ(λ2)...φ(λn))\\varphi(\\Lambda)=\\begin{pmatrix}\n\\varphi(\\lambda_1)&amp;&amp;&amp;\\\\\n&amp;\\varphi(\\lambda_2)&amp;&amp;\\\\\n&amp;&amp;...&amp;\\\\\n&amp;&amp;&amp;\\varphi(\\lambda_n)\n\\end{pmatrix}\nφ(Λ)=​φ(λ1​)​φ(λ2​)​...​φ(λn​)​​\n#对角化\n对角化: 寻找相似变换矩阵 P 来使P−1AP=ΛP^{-1}AP=\\LambdaP−1AP=Λ为对角矩阵\n能对角化的条件: A有n个线性无关的特征向量A有n个线性无关的特征向量A有n个线性无关的特征向量\n\n常用条件:\n有n个各不相同的特征值有n个各不相同的特征值有n个各不相同的特征值\n如果特征值有重根,则可以判断特征矩阵的秩如果特征值有重根, 则可以判断特征矩阵的秩如果特征值有重根,则可以判断特征矩阵的秩\n\n#对称矩阵的对角化\n对称矩阵的性质:\n\n特征值为实数\n不相等的特征值对应的特征向量正交\n一定存在正交矩阵 P 使得 A可以被对角化为以特征值为对角元的对角矩阵\n\n求法:\n\n求出 A 的特征值\n求出所有特征向量\n把这些特征向量正交化, 单位化\n排列得到PPP和Λ\\LambdaΛ, 注意两者对应\n\n\n#二次型, 标准形\n二次型: 含 n 个变量的二次齐次函数\nf(x1,x2,...,xn)=∑ai,jxixjf(x_1,x_2,...,x_n)=\\sum{a_{i,j}x_{i}x_{j}}f(x1​,x2​,...,xn​)=∑ai,j​xi​xj​\n标准形(法式): 只含平方项的二次型\nf(x1,x2,...,xn)=∑kixi2f(x_1,x_2,...,x_n)=\\sum{k_{i}x_{i}^2}f(x1​,x2​,...,xn​)=∑ki​xi2​\n规范形: 系数只在-1,0,1 中取值的标准形\n二次型的矩阵表示: 二次型可以用系数组成的对称矩阵唯一表示\nf=∑aijxixj,(aij=aji)=(x1x2...xn)(a11a12...a1na21a22...a2n............an1an2...ann)(x1x2...xn)=xTAx\\begin{aligned}\nf&amp;=\\sum{a_{ij}x_{i}x_{j}}, (a_{ij}=a_{ji})\\\\\n&amp;=\\begin{pmatrix}\nx_1&amp;x_2&amp;...&amp;x_n\n\\end{pmatrix}\n\\begin{pmatrix}\na_{11}&amp;a_{12}&amp;...&amp;a_{1n}\\\\\na_{21}&amp;a_{22}&amp;...&amp;a_{2n}\\\\\n...&amp;...&amp;...&amp;...\\\\\na_{n1}&amp;a_{n2}&amp;...&amp;a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n...\\\\\nx_n\n\\end{pmatrix}\\\\\n&amp;=x^TAx\n\\end{aligned}\nf​=∑aij​xi​xj​,(aij​=aji​)=(x1​​x2​​...​xn​​)​a11​a21​...an1​​a12​a22​...an2​​............​a1n​a2n​...ann​​​​x1​x2​...xn​​​=xTAx​\n二次型的秩: 对应矩阵的秩\n#合同对角化\n合同: 若存在可逆矩阵C使得B=CTAC,则称B与A合同若存在可逆矩阵C使得B=C^TAC, 则称B与A合同若存在可逆矩阵C使得B=CTAC,则称B与A合同\n性质\n\n对阵矩阵的合同矩阵也对称\n\n合同对角化: 寻找可逆矩阵CCC, 使得CTACC^TACCTAC为对角矩阵, 从而使二次型AAA通过x=Cyx=Cyx=Cy变换成标准形\nf=xTAx=yT(CTAC)y\\begin{aligned}\nf&amp;=x^TAx\\\\\n&amp;=y^T(C^TAC)y\n\\end{aligned}\nf​=xTAx=yT(CTAC)y​\n可以证明(由对称矩阵性质)这个矩阵一定存在, 而且是正交矩阵, 变换后的CTACC^TACCTAC由特征值构成\n#正定二次型\n惯性定理: 同一个二次型的两种标准化结果中正负系数的个数相同\n正(负)惯性指数: 其中的正(负)系数的个数\n正(负)定二次型, 正(负)定矩阵: 如果二次型的值恒大(小)于零(x=0 除外), 则称 f 为正(负)定二次型, 称 A 是正(负)定的\n\n正定⇔正惯性指数=n⇔特征值全正⇔各阶主子式都为正⇔存在可逆P,使得PTAP=E(与E合同)正定\\Leftrightarrow正惯性指数=n\\Leftrightarrow特征值全正\\Leftrightarrow各阶主子式都为正\\Leftrightarrow存在可逆P,使得P^TAP=E(与E合同)正定⇔正惯性指数=n⇔特征值全正⇔各阶主子式都为正⇔存在可逆P,使得PTAP=E(与E合同)\n负定⇔奇数阶主子式为负,偶数阶为正负定\\Leftrightarrow奇数阶主子式为负, 偶数阶为正负定⇔奇数阶主子式为负,偶数阶为正\n\n#6. 线性空间与线性变换\n\n向量空间(线性空间): 定义了线性运算且封闭, 且满足以下运算规律的非空集合\n\n加法交换律 结合律\n加法零元 逆元\n乘法单位元\n乘法交换律 结合律\n乘法对加法分配律\n\n\n向量: 线性空间中的元素\n子空间: 线性空间的仍是线性空间的非空子集\n基, 维数, 坐标: 同线性空间\n过渡矩阵 P=A−1BP=A^{-1}BP=A−1B\n基变换公式, 坐标变换公式\n\n#线性变换\n\n映射(变换): 表示为β=T(α)或β=Tα\\beta=T(\\alpha)或\\beta=T\\alphaβ=T(α)或β=Tα\n定义域 A, 值域 B, 像集 T(A): T(A)={β=T(α)∣α∈A}⊆BT(A)=\\{\\beta=T(\\alpha)|\\alpha\\in A\\} \\subseteq BT(A)={β=T(α)∣α∈A}⊆B\n线性映射(线性变换)\n\n从 Vn 到 Um 的保持线性组合的对应关系的映射. 特别地, 如果 Vn=Um, 称 T 为线性空间 Vn 中的线性变换\nT(a+b)=T(a)+T(b)T(a+b)=T(a)+T(b)T(a+b)=T(a)+T(b)\nT(λb)=λT(b)T(\\lambda b)=\\lambda T(b)T(λb)=λT(b)\n\n\n线性变换的性质:\n\nT(0)=0\n线性组合的变换等于变换的线性组合\n变换前线性相关=&gt;变换后也线性相关\n像集T(Vn)T(V_n)T(Vn​)也是线性空间, 称为像空间\nNT={α∣α∈Vn,Tα=0}N_T=\\{\\alpha|\\alpha\\in V_n, T\\alpha=0\\}NT​={α∣α∈Vn​,Tα=0}构成线性空间, 称为线性变换 T 的核\n\n\n\n#线性变换的矩阵表示\n线性变换的矩阵: 在 Vn 中取定一个基, 这个基的像的线性表示的系数矩阵称为线性变换在这个基下的矩阵, 即\n{T(α1)=∑j=1naj1αjT(α2)=∑j=1naj2αj...T(αn)=∑j=1najnαj\\left\\{\\begin{matrix}\nT(\\alpha_1)=\\sum_{j=1}^{n}{a_{j1}\\alpha_j}\\\\\nT(\\alpha_2)=\\sum_{j=1}^{n}{a_{j2}\\alpha_j}\\\\\n...\\\\\nT(\\alpha_n)=\\sum_{j=1}^{n}{a_{jn}\\alpha_j}\n\\end{matrix}\\right.\n⎩⎨⎧​T(α1​)=∑j=1n​aj1​αj​T(α2​)=∑j=1n​aj2​αj​...T(αn​)=∑j=1n​ajn​αj​​\n令\nA=(a11a12...a1na21a22...a2n:::an1an2...ann)A=\\begin{pmatrix}\na_{11}&amp; a_{12}&amp; ...&amp; a_{1n}\\\\\na_{21}&amp; a_{22}&amp; ...&amp; a_{2n}\\\\\n:&amp; :&amp; &amp; :\\\\\na_{n1}&amp; a_{n2}&amp; ...&amp; a_{nn}\\\\\n\\end{pmatrix}\\\\\nA=​a11​a21​:an1​​a12​a22​:an2​​.........​a1n​a2n​:ann​​​\n则\nT(α1,α2,...,αn)=(α1,α2,...,αn)A\\begin{aligned}\nT(\\alpha_1, \\alpha_2, ..., \\alpha_n)&amp;=(\\alpha_1, \\alpha_2, ..., \\alpha_n)A\n\\end{aligned}\nT(α1​,α2​,...,αn​)​=(α1​,α2​,...,αn​)A​\n#性质\n\n同一个变换在不同基下的矩阵相似, 且相似变换矩阵就是两个基的过渡矩阵 P\n\n#相似, 合同与等价\n\n等价⇔\\Leftrightarrow⇔AB 秩相同\n合同: 等价+正负惯性系数相同\n相似: 合同+特征值相同+主对角线元素之和相同+矩阵的值相同\n\n由此可见，等价到合同到相似，条件越来越苛刻，AB 共同点越来越多\n","categories":["mathematics"],"tags":["数学基础"]},{"title":"模运算的性质(程序设计版)","url":"/2021/01/10/modulo-arithmetic/","content":"#模运算的性质(程序设计版)\n模运算与基本四则运算有些相似，但是除法例外。其规则如下：\n(a+b) mod p=(a mod p+b mod p) mod p(a + b)\\bmod p = (a\\bmod p + b\\bmod p)\\bmod p\n(a+b)modp=(amodp+bmodp)modp\n(a−b) mod p=(a mod p−b mod p) mod p(a - b)\\bmod p = (a\\bmod p - b\\bmod p)\\bmod p\n(a−b)modp=(amodp−bmodp)modp\n(a×b) mod p=(a mod p×b mod p) mod p(a \\times b)\\bmod p = (a\\bmod p \\times b\\bmod p)\\bmod p\n(a×b)modp=(amodp×bmodp)modp\n(ab) mod p=((a mod p)b) mod p(a^b)\\bmod p = ((a\\bmod p)^b)\\bmod p\n(ab)modp=((amodp)b)modp\n","categories":["mathematics"],"tags":["算法","数学"]},{"title":"概率论与数理统计 - 概念总结","url":"/2023/10/27/probability-and-mathematical-statistics/","content":"#基本概念\n\n随机试验\n可以在相同的条件下重复进行，并且每次试验的结果不确定，但试验前可以明确试验的所有可能结果。\n样本空间\n随机试验 EEE 的所有可能结果组成的集合，记为 SSS。\n样本点\n样本空间中的元素，记为 ω\\omegaω。\n事件\n样本空间 SSS 的子集称为随机事件，简称事件，通常用大写字母 A,B,C,...A, B, C, ...A,B,C,... 表示。\n基本事件\n只包含一个样本点的随机事件。\n\n\n必然事件: 包含所有样本点的随机事件。\n不可能事件: 不包含任何样本点的随机事件。\n\n事件关系和运算\n\n包含关系: A⊂BA \\subset BA⊂B，AAA 包含于 BBB。\n相等关系: A=BA = BA=B，AAA 等于 BBB。\n和事件: A∪BA \\cup BA∪B，AAA 与 BBB 至少有一个发生。\n积事件: A∩BA \\cap BA∩B，AAA 与 BBB 同时发生。\n差事件: A−BA - BA−B，AAA 发生而 BBB 不发生。\n互斥事件: A∩B=∅A \\cap B = \\emptysetA∩B=∅，AAA 与 BBB 不可能同时发生。\n逆事件/对立事件: A∪B=SA \\cup B = SA∪B=S，AAA 与 BBB 至少有一个发生。\n\n\n频数\n在 nnn 次试验中，事件 AAA 发生的次数，记为 nAn_AnA​。\n频率\n事件 AAA 发生的频率，记为 fn(A)=nAnf_n(A) = \\frac{n_A}{n}fn​(A)=nnA​​。\n概率\n事件 AAA 发生的可能性大小，记为 P(A)P(A)P(A)，满足以下三个条件:\n\n\n非负性: P(A)≥0P(A) \\geq 0P(A)≥0。\n规范性: P(S)=1P(S) = 1P(S)=1。\n可列可加性: 若 A1,A2,...A_1, A_2, ...A1​,A2​,... 两两互斥，则 P(⋃i=1∞Ai)=∑i=1∞P(Ai)P(\\bigcup_{i=1}^{\\infty}A_i) = \\sum_{i=1}^{\\infty}P(A_i)P(⋃i=1∞​Ai​)=∑i=1∞​P(Ai​)。\n\n概率的性质推论：\n\nP(∅)=0P(\\emptyset) = 0P(∅)=0。\n有限可加性: 若 A1,A2,...A_1, A_2, ...A1​,A2​,... 两两互斥，则 P(⋃i=1nAi)=∑i=1nP(Ai)P(\\bigcup_{i=1}^{n}A_i) = \\sum_{i=1}^{n}P(A_i)P(⋃i=1n​Ai​)=∑i=1n​P(Ai​)。\n包含事件的概率: 若 A⊂BA \\subset BA⊂B，则 P(A)≤P(B)P(A) \\leq P(B)P(A)≤P(B)。\nP(A)≤1P(A) \\leq 1P(A)≤1。\n互补事件的概率: P(Aˉ)=1−P(A)P(\\bar{A}) = 1 - P(A)P(Aˉ)=1−P(A)。\n加法公式: P(A∪B)=P(A)+P(B)−P(A∩B)P(A \\cup B) = P(A) + P(B) - P(A \\cap B)P(A∪B)=P(A)+P(B)−P(A∩B)。\n\n\n古典概型/等可能概型\n符合以下条件的概率模型称为古典概型:\n\n\n试验的样本空间是有限的。\n试验的每个基本事件发生的可能性相同。\n\n\n条件概率\n在事件 AAA 已经发生的条件下，事件 BBB 发生的概率，记为 P(B∣A)=P(AB)P(A)P(B|A) = \\frac{P(AB)}{P(A)}P(B∣A)=P(A)P(AB)​。\n\n\n条件概率仍然是概率，满足概率的三个基本性质。\n\n\n乘法定理, 乘法公式\nP(AB)=P(A)P(B∣A)=P(B)P(A∣B)P(AB) = P(A)P(B|A) = P(B)P(A|B)P(AB)=P(A)P(B∣A)=P(B)P(A∣B)。\n划分\n若一组事件 B1,B2,...B_1, B_2, ...B1​,B2​,... 满足 Bi∩Bj=∅B_i \\cap B_j = \\emptysetBi​∩Bj​=∅，i≠ji \\neq ji=j，且 ⋃i=1∞Bi=S\\bigcup_{i=1}^{\\infty}B_i = S⋃i=1∞​Bi​=S，则称 B1,B2,...B_1, B_2, ...B1​,B2​,... 是样本空间 SSS 的一个划分。\n全概率公式\n设 B1,B2,...B_1, B_2, ...B1​,B2​,... 是样本空间 SSS 的一个划分，且 P(Bi)&gt;0P(B_i) &gt; 0P(Bi​)&gt;0，i=1,2,...i = 1, 2, ...i=1,2,...，则对任一事件 AAA，有 P(A)=∑i=1∞P(Bi)P(A∣Bi)P(A) = \\sum_{i=1}^{\\infty}P(B_i)P(A|B_i)P(A)=∑i=1∞​P(Bi​)P(A∣Bi​)。\n贝叶斯公式\n设 B1,B2,...B_1, B_2, ...B1​,B2​,... 是样本空间 SSS 的一个划分，且 P(Bi)&gt;0P(B_i) &gt; 0P(Bi​)&gt;0，i=1,2,...i = 1, 2, ...i=1,2,...，则对任一事件 AAA，有 P(Bi∣A)=P(Bi)P(A∣Bi)∑j=1∞P(Bj)P(A∣Bj)P(B_i|A) = \\frac{P(B_i)P(A|B_i)}{\\sum_{j=1}^{\\infty}P(B_j)P(A|B_j)}P(Bi​∣A)=∑j=1∞​P(Bj​)P(A∣Bj​)P(Bi​)P(A∣Bi​)​。\n先验概率\n根据以往数据分析得到的概率。\n后验概率\n得到新的信息后重新加以修正的概率。\n独立\n如果 P(AB)=P(A)P(B)P(AB) = P(A)P(B)P(AB)=P(A)P(B)，则称事件 AAA 与事件 BBB 相互独立，简称 A,BA, BA,B 独立。\n\n#随机变量\n\n随机变量\n定义在样本空间 SSS 上的实值单值函数 X=X(e)X = X(e)X=X(e)，其中 e∈Se \\in Se∈S。\nPS. 将样本空间 SSS 中的每个样本点 eee 对应到实数轴上的一个点 X(e)X(e)X(e)。单值函数: 对定义域每一个自变量 x，其对应的函数值 f（x）是唯一的。\n离散型随机变量\n全部取值范围是有限个或可列无限多个的随机变量。\n(0-1)分布\n可能结果只有 0 和 1 的分布，记 0 的概率为 ppp，1 的概率为 1−p1-p1−p，则 P(X=k)=pk(1−p)1−kP(X=k) = p^k(1-p)^{1-k}P(X=k)=pk(1−p)1−k。\n伯努利试验\n可能结果只有 AAA 和 Aˉ\\bar{A}Aˉ 的随机试验。将伯努利试验独立重复进行 nnn 次，称为 nnn 重伯努利试验。\n二项分布\n重复进行 nnn 次伯努利试验，事件 AAA 发生的次数 XXX 服从二项分布，记为 X∼b(n,p)X \\sim b(n, p)X∼b(n,p)。\n泊松分布\n可能取值是 0, 1, 2, …，而取各个值的概率是 P(X=k)=λkk!e−λP(X=k) = \\frac{\\lambda^k}{k!}e^{-\\lambda}P(X=k)=k!λk​e−λ，其中 λ&gt;0\\lambda &gt; 0λ&gt;0，的随机变量 XXX 的分布。称 XXX 服从参数为 λ\\lambdaλ 的泊松分布，记为 X∼π(λ)X \\sim \\pi(\\lambda)X∼π(λ)。\n泊松定理\n设常数 λ&gt;0\\lambda &gt; 0λ&gt;0，n&gt;0n &gt; 0n&gt;0，npn=λnp_n=\\lambdanpn​=λ，有 lim⁡n→∞Cnkpnk(1−pn)n−k=λkk!e−λ\\lim_{n \\to \\infty}C_n^kp_n^k(1-p_n)^{n-k} = \\frac{\\lambda^k}{k!}e^{-\\lambda}limn→∞​Cnk​pnk​(1−pn​)n−k=k!λk​e−λ。也就是说，当 nnn 很大（≥20\\geq 20≥20），ppp 很小(≤0.05\\leq 0.05≤0.05)时，二项分布近似于泊松分布。可以用泊松分布来计算二项分布的概率。\n随机变量的分布函数\nF(x)=P(X≤x)F(x) = P(X \\leq x)F(x)=P(X≤x)，x∈Rx \\in Rx∈R\n连续性随机变量、概率密度\n如果对于分布函数 F(x)F(x)F(x)，存在非负函数 f(x)f(x)f(x)，满足 F(x)=∫−∞xf(t)dtF(x) = \\int_{-\\infty}^{x}f(t)dtF(x)=∫−∞x​f(t)dt，则称 XXX 是连续型随机变量，f(x)f(x)f(x) 为 XXX 的概率密度（函数）\n均匀分布\n在区间 (a,b)(a,b)(a,b) 上的概率密度函数符合 f(x)=1b−af(x)=\\frac{1}{b-a}f(x)=b−a1​ 的分布。记为 X∼U(a,b)X \\sim U(a,b)X∼U(a,b)。\n指数分布\n在区间 (0,+∞)(0, +\\infty)(0,+∞) 上的概率密度函数符合 f(x)=λe−λxf(x)=\\lambda e^{-\\lambda x}f(x)=λe−λx 的分布。记为 X∼E(λ)X \\sim E(\\lambda)X∼E(λ)。\n\n\n指数分布的无记忆性: P(X&gt;s+t∣X&gt;s)=P(X&gt;t)P(X&gt;s+t|X&gt;s) = P(X&gt;t)P(X&gt;s+t∣X&gt;s)=P(X&gt;t)\n\n\n正态分布/高斯分布\n概率密度函数符合 f(x)=12πσe−(x−μ)22σ2f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}f(x)=2π​σ1​e−2σ2(x−μ)2​ 的分布。记为 X∼N(μ,σ2)X \\sim N(\\mu, \\sigma^2)X∼N(μ,σ2)。\n伽马分布\n概率密度函数符合 f(x)=λαxα−1Γ(α)e−λxf(x)=\\frac{\\lambda^{\\alpha}x^{\\alpha-1}}{\\Gamma(\\alpha)}e^{-\\lambda x}f(x)=Γ(α)λαxα−1​e−λx 的分布。记为 X∼Γ(α,λ)X \\sim \\Gamma(\\alpha, \\lambda)X∼Γ(α,λ)。其中 α\\alphaα 为形状参数，λ\\lambdaλ 为比例参数。\n\n也可以记为 X∼Γ(α,β)X \\sim \\Gamma(\\alpha, \\beta)X∼Γ(α,β)，其中 β=1λ\\beta = \\frac{1}{\\lambda}β=λ1​。\n#多维随机变量\n\n二维随机变量，联合分布函数\n由两个随机变量 XXX 和 YYY 构成的向量 (X,Y)(X, Y)(X,Y) 称为随机变量。二元函数 F(x,y)=P(X≤x,Y≤y)F(x, y) = P(X \\leq x, Y \\leq y)F(x,y)=P(X≤x,Y≤y) 称为二维随机变量 (X,Y)(X, Y)(X,Y) 的联合分布函数。\n\n\n类似地，二维随机变量也有离散型、连续性、分布律和概率密度函数等概念。\n\n联合概率分布, 联合概率密度:\nF(x,y)=P(X≤x,Y≤y)=∫−∞y∫−∞xf(u,v)dudvF(x,y) = P(X\\leq x, Y\\leq y) = \\int_{-\\infty}^{y}\\int_{-\\infty}^{x}f(u,v)dudv\nF(x,y)=P(X≤x,Y≤y)=∫−∞y​∫−∞x​f(u,v)dudv\n边缘分布函数, 边缘概率密度:\nFX(x)=F(x,+∞),FY(y)=F(+∞,y),fX(x)=∫−∞+∞f(x,y)dy,fY(y)=∫−∞+∞f(x,y)dxF_X(x) = F(x, +\\infty), \\\\\nF_Y(y) = F(+\\infty, y), \\\\\nf_X(x) = \\int_{-\\infty}^{+\\infty}f(x,y)dy, \\\\\nf_Y(y) = \\int_{-\\infty}^{+\\infty}f(x,y)dx\nFX​(x)=F(x,+∞),FY​(y)=F(+∞,y),fX​(x)=∫−∞+∞​f(x,y)dy,fY​(y)=∫−∞+∞​f(x,y)dx\n条件概率密度, 在Y=yY=yY=y的条件下:\nfX∣Y(x,y)=f(x,y)fY(y)f_{X|Y}(x,y) = \\frac{f(x,y)}{f_Y(y)}\nfX∣Y​(x,y)=fY​(y)f(x,y)​\nZ=X+YZ=X+YZ=X+Y的概率分布:\nFZ(z)=P(Z≤z)=∬x+y≤zf(x,y)dxdy=∫−∞+∞f(z−y,y)dy=∫−∞+∞f(x,z−x)dxF_Z(z) = P(Z\\leq z) =\\iint_{x+y\\leq z}f(x,y)dxdy \\\\\n= \\int_{-\\infty}^{+\\infty}f(z-y,y)dy \\\\\n= \\int_{-\\infty}^{+\\infty}f(x,z-x)dx\nFZ​(z)=P(Z≤z)=∬x+y≤z​f(x,y)dxdy=∫−∞+∞​f(z−y,y)dy=∫−∞+∞​f(x,z−x)dx\n如果XXX和YYY独立, 卷积公式:\n=∫−∞+∞fX(x)fY(z−x)dx= \\int_{-\\infty}^{+\\infty}f_X(x)f_Y(z-x)dx\n=∫−∞+∞​fX​(x)fY​(z−x)dx\n#随机变量的数字特征\n数学期望, 简称期望, 又称为均值, 常用μ\\muμ表示\nE(x)=μx=∫−∞∞xf(x)dxE(x) = \\mu_x = \\int_{-\\infty}^{\\infty}xf(x)dx\nE(x)=μx​=∫−∞∞​xf(x)dx\n方差, 标准差:\nD(X)=Var(X)=E((X−μx)2)=E(X2)−μx2,D(X)=σX=E((X−μx)2)D(X) = Var(X) = E((X-\\mu_x)^2) = E(X^2) - \\mu_x^2, \\\\\n\\sqrt{D(X)} = \\sigma_X = \\sqrt{E((X-\\mu_x)^2)}\nD(X)=Var(X)=E((X−μx​)2)=E(X2)−μx2​,D(X)​=σX​=E((X−μx​)2)​\n\n协方差\n用于衡量随机变量 X 与 Y 的相关性:\n\nCov(X,Y)=E[(X−μX)(Y−μY)]Cov(X, Y) = E[(X-\\mu_X)(Y-\\mu_Y)]\nCov(X,Y)=E[(X−μX​)(Y−μY​)]\n\n相关系数\n剔除了两个变量量纲影响、标准化后的协方差:\n\nρ=Cov(X,Y)σXσY\\rho = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\nρ=σX​σY​Cov(X,Y)​\n#参数估计\n\n样本\nX1,X2,...,XnX_1, X_2, ..., X_nX1​,X2​,...,Xn​, nnn为样本大小/样本容量/样本量\n统计量\n完全由样本决定的量\n参数估计问题\n根据样本估计概率函数\n\n\n设有了从总体中抽出的独立随机样本X1,...,XnX_1, ..., X_nX1​,...,Xn​, 要依据这些样本去对参数 θ1,...,θk\\theta_1, ..., \\theta_kθ1​,...,θk​ 的未知值作出估计. 当然, 我们也可以只要求估计其中的一部分, 或估计它们的某个已知函数 g(θ1,...,θk)g(\\theta_1, ..., \\theta_k)g(θ1​,...,θk​)\n\n\n矩估计\npass\n\n最大似然估计:\n设总体有分布 $f(X, \\theta_1, …,\\theta_k), X1,...,XnX_1, ...,X_nX1​,...,Xn​ 为自这总体中抽出的样本, 则样本(X1,...,Xn)(X_1, ...,X_n)(X1​,...,Xn​)的分布(即其概率密度函数或概率函数)为\nL(X1,...,Xn,θ1,...,θk)=f(X1,θ1,...,θk)f(X2,θ1,...,θk)...f(Xn,θ1,...,θk)L(X_1, ..., X_n, \\theta_1,..., \\theta_k) =\n    f(X_1, \\theta_1, ..., \\theta_k)\n    f(X_2, \\theta_1, ..., \\theta_k)\n    ...\n    f(X_n, \\theta_1, ..., \\theta_k)\nL(X1​,...,Xn​,θ1​,...,θk​)=f(X1​,θ1​,...,θk​)f(X2​,θ1​,...,θk​)...f(Xn​,θ1​,...,θk​)\n\n似然函数\n将上式视为θ\\thetaθ的函数, 称为似然函数\n最大似然估计\n对于已知的样本, 估计最优的θ\\thetaθ值，使得似然函数最大化，即为最大似然估计\n\n#随机过程\n\n随机过程\n依赖于参数 t∈Tt \\in Tt∈T 的一族随机变量，记为 {X(t),t∈T}\\{X(t), t \\in T\\}{X(t),t∈T}，其中：\nTTT 为参数集。\nttt 通常表示时间。\nX(t)X(t)X(t) 表示在时刻 ttt 时过程的状态。\nX(t)X(t)X(t) 的所有可能取值的集合称为状态空间。\n样本函数/样本曲线\n对随机过程的一次试验，得到的函数 x(t),t∈Tx(t), t \\in Tx(t),t∈T。\n伯努利过程/伯努利随机序列\n与时间无关的随机过程，即 X(t)=XX(t) = XX(t)=X。\nPS. 例如多次抛硬币的过程。\n\n\n根据任一时刻 ttt 的状态 X(t)X(t)X(t) 是连续型还是离散型，随机过程分为连续型随机过程和离散型随机过程。\n根据时间参数 ttt 是连续还是离散，随机过程分为连续参数随机过程和离散参数随机过程活随机序列。\n\n\n一维分布函数，一维分布函数族\n随机变量 X(t)X(t)X(t) 的分布函数，记为 FX(x,t)=P(X(t)≤x)F_X(x, t) = P(X(t) \\leq x)FX​(x,t)=P(X(t)≤x)，称为一维分布函数。\nFX(x,t)F_X(x, t)FX​(x,t) 的全体集合称为一维分布函数族。\nn 维分布函数族\n对于 nnn 个时刻 t1,t2,...,tnt_1, t_2, ..., t_nt1​,t2​,...,tn​，nnn 个随机变量 X(t1),X(t2),...,X(tn)X(t_1), X(t_2), ..., X(t_n)X(t1​),X(t2​),...,X(tn​) 的分布函数族。\n均值函数\n随机变量 X(t)X(t)X(t) 的所有样本函数在时刻 ttt 的平均值，也称集平均或统计平均，记为 μX(t)=E[X(t)]\\mu_X(t) = E[X(t)]μX​(t)=E[X(t)]。\n均方值函数\n随机变量 X(t)X(t)X(t) 的二阶原点矩，记为 ΨX2(t)=E[X2(t)]\\Psi^2_X(t) = E[X^2(t)]ΨX2​(t)=E[X2(t)]。\n方差函数\n随机变量 X(t)X(t)X(t) 的二阶中心矩，记为 DX(t)=E[(X(t)−μX(t))2]D_X(t) = E[(X(t) - \\mu_X(t))^2]DX​(t)=E[(X(t)−μX​(t))2]。\n标准差函数\n方差函数的算术平方根，记为 σX(t)=DX(t)\\sigma_X(t) = \\sqrt{D_X(t)}σX​(t)=DX​(t)​。\n(自)相关函数\n两个随机变量 X(t1)X(t_1)X(t1​) 和 X(t2)X(t_2)X(t2​) 的二阶原点混合矩，记为 RX(t1,t2)=E[X(t1)X(t2)]R_X(t_1, t_2) = E[X(t_1)X(t_2)]RX​(t1​,t2​)=E[X(t1​)X(t2​)]。\n(自)协方差函数\n两个随机变量 X(t1)X(t_1)X(t1​) 和 X(t2)X(t_2)X(t2​) 的二阶混合中心矩，记为 CX(t1,t2)=Cov[X(t1),X(t2)]=E[(X(t1)−μX(t1))(X(t2)−μX(t2))]C_X(t_1, t_2) = Cov[X(t_1), X(t_2)] = E[(X(t_1) - \\mu_X(t_1))(X(t_2) - \\mu_X(t_2))]CX​(t1​,t2​)=Cov[X(t1​),X(t2​)]=E[(X(t1​)−μX​(t1​))(X(t2​)−μX​(t2​))]。\n二阶矩过程\n随机过程 X(t)X(t)X(t) 的二阶矩 E[X2(t)]E[X^2(t)]E[X2(t)] 对于任意时间 ttt 都存在。\n正态过程\n对于任意有限个时刻 t1,t2,...,tnt_1, t_2, ..., t_nt1​,t2​,...,tn​，nnn 个随机变量 X(t1),X(t2),...,X(tn)X(t_1), X(t_2), ..., X(t_n)X(t1​),X(t2​),...,X(tn​) 的任意线性组合服从正态分布的随机过程。\n二维随机过程\n由两个随机变量 X(t)X(t)X(t) 和 Y(t)Y(t)Y(t) 构成的随机过程。\nn+m 维(联合)分布函数\n对于 n+mn+mn+m 个时刻 t1,t2,...,tn;t1′,t2′,...,tm′t_1, t_2, ..., t_n; t&#x27;_1, t&#x27;_2, ..., t&#x27;_mt1​,t2​,...,tn​;t1′​,t2′​,...,tm′​，n+mn+mn+m 个随机变量 X(t1),X(t2),...,X(tn);Y(t1′),Y(t2′),...,Y(tm′)X(t_1), X(t_2), ..., X(t_n); Y(t&#x27;_1), Y(t&#x27;_2), ..., Y(t&#x27;_m)X(t1​),X(t2​),...,X(tn​);Y(t1′​),Y(t2′​),...,Y(tm′​) 的分布函数。\n独立增量过程\n随机过程 X(t)X(t)X(t) 的任意两个不相交时间区间上的增量相互独立。\n增量具有平稳性\n随机过程 X(t)X(t)X(t) 的任意两个相同长度的时间区间上的增量具有相同的分布。说明增量的统计特性与时间的起点无关，只与时间间隔有关。\n计数过程\n表示在连续时间区间 [0,t][0, t][0,t] 内某事件发生的次数的随机过程，记为 N(t)N(t)N(t)。\n\n\n时间间隔 (t0,t](t_0, t](t0​,t] 内事件发生的次数记为 N(t0,t)=N(t)−N(t0)N(t_0, t) = N(t) - N(t_0)N(t0​,t)=N(t)−N(t0​)。\n时间间隔 (t0,t](t_0, t](t0​,t] 内事件发生 kkk 次的概率记为 Pk(t0,t)=P{N(t0,t)=k}P_k(t_0, t) = P\\{N(t_0, t) = k\\}Pk​(t0​,t)=P{N(t0​,t)=k}。\n\n\n泊松过程\n如果计数过程 N(t)N(t)N(t) 满足以下条件，则称 N(t)N(t)N(t) 为强度为 λ\\lambdaλ 的泊松过程:\n\n\nN(0)=0N(0) = 0N(0)=0。\nN(t)N(t)N(t) 是独立增量过程。\n对于充分小的 Δt\\Delta tΔt，事件发生一次的概率 P1{N(t,t+Δt)}=λΔt+o(Δt)P_1\\{N(t, t+\\Delta t)\\} = \\lambda \\Delta t + o(\\Delta t)P1​{N(t,t+Δt)}=λΔt+o(Δt)，o(Δt)o(\\Delta t)o(Δt) 是关于 Δt\\Delta tΔt 的高阶无穷小。\n对于充分小的 Δt\\Delta tΔt，事件发生 j≥2j \\geq 2j≥2 次的概率 Pj{N(t,t+Δt)}=o(Δt)P_j\\{N(t, t+\\Delta t)\\} = o(\\Delta t)Pj​{N(t,t+Δt)}=o(Δt)。\n\n泊松过程的性质：\n\n增量 N(t0,t)N(t_0, t)N(t0​,t) 服从参数为 λ(t−t0)\\lambda(t - t_0)λ(t−t0​) 的泊松分布，数学表示 N(t0,t)∼π(λ(t−t0))N(t_0, t) \\sim \\pi(\\lambda(t - t_0))N(t0​,t)∼π(λ(t−t0​))，注：泊松分布 Pλ(X=k)=λkk!e−λP_\\lambda(X=k) = \\frac{\\lambda^k}{k!}e^{-\\lambda}Pλ​(X=k)=k!λk​e−λ\n均值函数 μN(t)=E[N(t)]=λt\\mu_N(t) = E[N(t)] = \\lambda tμN​(t)=E[N(t)]=λt\n方差函数 DN(t)=Var[N(t)]=λtD_N(t) = Var[N(t)] = \\lambda tDN​(t)=Var[N(t)]=λt\n协方差函数 CN(t1,t2)=Cov[N(t1),N(t2)]=λmin⁡(t1,t2)C_N(t_1, t_2) = Cov[N(t_1), N(t_2)] = \\lambda \\min(t_1, t_2)CN​(t1​,t2​)=Cov[N(t1​),N(t2​)]=λmin(t1​,t2​)\n相关函数 RN(t1,t2)=E[N(t1)N(t2)]=λ2t1t2+λmin⁡(t1,t2)R_N(t_1, t_2) = E[N(t_1)N(t_2)] = \\lambda^2 t_1 t_2 + \\lambda \\min(t_1, t_2)RN​(t1​,t2​)=E[N(t1​)N(t2​)]=λ2t1​t2​+λmin(t1​,t2​)\n\n\n泊松流\n泊松过程中事件发生的时刻。\n泊松过程等待时间\n泊松过程中第 nnn 个事件发生的时间，记为 Wn=tnW_n = t_nWn​=tn​，特别地，W0=0W_0 = 0W0​=0。\n\n泊松过程等待时间的性质：\n\nWnW_nWn​ 的分布函数 FWn(t)=P{Wn≤t}=1−P{Wn&gt;t}=1−P{N(t)&lt;n}=P{N(t)≥n}=∑k=n+∞(λt)kk!e−λtF_{W_n}(t) = P\\{W_n \\leq t\\} \\\\\n= 1 - P\\{W_n &gt; t\\} \\\\\n= 1 - P\\{N(t) &lt; n\\} \\\\\n= P\\{N(t) \\geq n\\} \\\\\n= \\sum_{k=n}^{+\\infty}\\frac{(\\lambda t)^k}{k!}e^{-\\lambda t}FWn​​(t)=P{Wn​≤t}=1−P{Wn​&gt;t}=1−P{N(t)&lt;n}=P{N(t)≥n}=∑k=n+∞​k!(λt)k​e−λt。\nWnW_nWn​ 的概率密度函数 fWn(t)=dFWn(t)dt=λntn−1(n−1)!e−λtf_{W_n}(t) = \\frac{dF_{W_n}(t)}{dt} = \\frac{\\lambda^n t^{n-1}}{(n-1)!}e^{-\\lambda t}fWn​​(t)=dtdFWn​​(t)​=(n−1)!λntn−1​e−λt，即 Wn∼Γ(n,λ)W_n \\sim \\Gamma(n, \\lambda)Wn​∼Γ(n,λ)。\n\n注：伽马分布 Γ(α,λ)\\Gamma(\\alpha, \\lambda)Γ(α,λ)：f(x)=λαxα−1Γ(α)e−λxf(x)=\\frac{\\lambda^{\\alpha}x^{\\alpha-1}}{\\Gamma(\\alpha)}e^{-\\lambda x}f(x)=Γ(α)λαxα−1​e−λx\n当 n=1n = 1n=1 时，伽马分布即指数分布。\n\n\n\n\n点间间距\n泊松过程中相邻两个事件的时间间隔，记为 Tn=Wn−Wn−1T_n = W_n - W_{n-1}Tn​=Wn​−Wn−1​。\n\n\nTnT_nTn​ 的概率密度函数 fTi(t)=λe−λtf_{T_i}(t) = \\lambda e^{-\\lambda t}fTi​​(t)=λe−λt，即 Ti∼E(λ)T_i \\sim E(\\lambda)Ti​∼E(λ)，即指数分布。\n\n泊松过程的性质：\n\n泊松过程的点间间距是独立同分布的指数分布。\n如果一个过程的点间间距是独立同分布的指数分布，那么这个过程是泊松过程。\n\n\n维纳过程\n满足以下条件的二阶矩过程 W(t)W(t)W(t) 称为维纳过程：\n\n\nW(0)=0W(0) = 0W(0)=0。\nW(t1,t2)∼N(0,σ2(t2−t1))W(t_1, t_2) \\sim N(0, \\sigma^2 (t_2 - t_1))W(t1​,t2​)∼N(0,σ2(t2​−t1​))。\n具有独立增量性质。\n\n\n均值函数 μW(t)=0\\mu_W(t) = 0μW​(t)=0。\n方差函数 DW(t)=σ2tD_W(t) = \\sigma^2 tDW​(t)=σ2t。\n协方差函数 CW(t1,t2)=Cov[W(t1),W(t2)]=σ2min⁡(t1,t2)C_W(t_1, t_2) = Cov[W(t_1), W(t_2)] = \\sigma^2 \\min(t_1, t_2)CW​(t1​,t2​)=Cov[W(t1​),W(t2​)]=σ2min(t1​,t2​)。\n\n#马尔可夫链\n\n马尔可夫性\n过程的未来状态只与当前状态有关，与过去状态无关。\n马尔科夫过程\n具有马尔可夫性的随机过程。\n\n","categories":["mathematics"],"tags":["数学基础"]},{"title":"CS246: Mining Massive Data Sets","url":"/2025/03/14/cs246/","content":"CS246: Mining Massive Data Sets\n课程教材: MMDs\n前序知识: 编程语言 Python 或者 Java；数据结构和算法；概率论；线性代数；多变量积分；数据库系统\n#介绍\n#什么是数据挖掘\n数据分析和挖掘的应用场景\n\n推荐系统\n网络检索\nAlphaGo\n机器人控制\n\n数据分析和挖掘的基本任务\n\n数据分析\n对数据进行 inspecting, cleaning, transforming 和 modeling，以发现有用的信息，提出建议，支持决策\n数据挖掘\n数据分析技术之一，更加关注于利用 modeling 和 knowledge discovery 来做预测，而不是仅仅做数据的描述\n\n不同类型的数据\n\n高维数据\n图数据\n无限流数据\n多模态数据\n\n数据挖掘与其他学科的关系\n\n与数据库: 都有大规模数据，简单查询\n\n数据库视角: 大规模数据的查询分析\n\n\n与机器学习: 都有小量数据，复杂模型\n\n机器学习视角: 模型推理\n\n\n与 AI\n与计算机理论: 都有随机化算法\n\n数据挖掘的清单\n\n按照场景划分\n\n监督学习\n无监督学习\n强化学习\n半监督学习\n迁移学习\n结构化学习\n\n\n按照任务划分\n\n回归\n分类\n聚类\n\n\n按照方法\n\n线性模型\n深度学习\nSVM\n决策树\nKNN\n\n\n\n#MapReduce &amp; Spark\n大规模计算的挑战\n\n\n如何分发计算\n\n\n如何让写分布式程序更简单\n\n\n机器故障: 每台机器的平均寿命是 3 年\n\n\n通过网络传输数据很慢\n\n近数据计算\nSpark/Hadoop\n\n\n\n节点故障如何保证数据安全\n\n分布式存储\n\n\n\n经典 MapReduce 模型\n\n三个阶段：Map, Shuffle, Reduce\n实现：原始 Google 实现，Hadoop，Spark，Flink\n优点：\n\n模型简单\n隐藏了底层细节\n\n\n缺点：\n\n巨大的性能折损：Map 的中间结果需要先写入磁盘，进行排序，然后再被 Reducer 读取\n表达力弱：很多问题不容易表示成 MapReduce 过程\n\n\n\nMapReduce 的改进 – Data-Flow 系统\n\nMapReduce 的问题在于，Map 和 Reduce 之间的数据流动是固定的，不够灵活。\nData-Flow 系统的改进包括\n\n允许任意数量的任务\n允许 Map 和 Reduce 以外的函数\n如果数据流是无环的，可以实现局部错误恢复\n\n\n\nSpark 引擎\n\n表达力更强的计算系统\n快速数据共享\n\n中间结果可以在内存中共享\n\n需要大量内存\n\n\n重复计算可以缓存\n\n\n一般化的执行图：DAG 模型\n更丰富的函数库：不止 Map 和 Reduce\n容易编程\n\nSpark 的关键设计\n\nRDD: Resilient Distributed Dataset\n\nKV 集合的分片\n\n\nTransformations: 对 RDD 进行数据操作\n\n例如: map, filter, join, union, intersection, distinct\nLazy Evaluation\n\n\nActions: 返回结果\n\ncollect, count, reduce, take\n\n\nDataFrame: 类似于 SQL 的表格\nDataSet: 类似于 DataFrame，但是类型安全\n\nSpark 的库\n\nSpark SQL\nSpark Streaming\nMLlib\nGraphX\n\n适合于 MR 的问题\n\n统计每个 Host 下所有网页的总大小\n统计机器翻译：5-gram 的频率\n计算两个表的自然连接\n\n不适合于 MR 的问题\n\n图\n相互依赖的数据\n\n机器学习\n很多 pair 数据的对比\n\n\n\nMR 的开销估算\n\n通信开销\n已用的通信开销\n计算开销\n\n#关联规则挖掘\n动机：识别超市中哪些商品经常被一起购买\n问题定义：\n给定 NNN 次交易记录，每次交易记录包含若干个物品。找到频繁项集。\n#应用场景\n\n搜索引擎对重复内容的网页去重\n新闻网站对一系列相关的新闻聚类\n寻找基因组中相似的序列\n从用户行为中找到相似的用户\n文本抄袭检测\n组合药物的相互作用\n\n#定义\n\n支持度 SupportSupportSupport\n项集 III 的支持度 support(I)=包含I的交易数量Nsupport(I) = \\frac{包含I的交易数量}{N}support(I)=N包含I的交易数量​，支持度越高，说明 III 共同出现得越频繁。\n频繁项集（Frequent Itemset）\n如果一个项集 III 的支持度大于 sss，则称 III 是频繁的。\n关联规则（Association Rules）\n使用 {i1,i2,...,ik}→j\\{i_1, i_2, ..., i_k\\} \\rightarrow j{i1​,i2​,...,ik​}→j 表示：如果一笔交易包含物品 {i1,i2,...,ik}\\{i_1, i_2, ..., i_k\\}{i1​,i2​,...,ik​}，则该交易很可能包含物品 jjj。\n置信度 ConfidenceConfidenceConfidence\n关联规则 I={i1,...,ik}→jI=\\{i_1,...,i_k\\} \\rightarrow jI={i1​,...,ik​}→j 的置信度 conf(I→j)=support(I∪j)support(I)conf(I\\rightarrow j) = \\frac{support(I \\cup j)}{support(I)}conf(I→j)=support(I)support(I∪j)​。\n\n#Min-Hashing 算法\n#动机\n从大规模数据中快速找到相似的数据对。例如，从网页中找到相似的网页，从用户中找到相似的用户。严谨定义：从数据集 DDD 中找到所有相似的数据对 xi,xj∈Dx_i, x_j \\in Dxi​,xj​∈D，并且 d(xi,xj)≤sd(x_i, x_j) \\leq sd(xi​,xj​)≤s。\n朴素方法需要计算所有数据对的相似度，复杂度为 O(n2)O(n^2)O(n2)，不适用于大规模数据。\n#方法\n\nShingling: 把原始文档转换成离散的集合\nMin-Hashing: 用小的签名表示大的集合\nLocality-Sensitive Hashing: 用小的签名找到相似的数据对\n\n","categories":["machine-learning"],"tags":["算法","机器学习"]},{"title":"短文本相似度：Jaro-Winkler 相似度","url":"/2024/05/07/jaro-winkler-similarity/","content":"Jaro-Winkler similarity 是由 Matthew A. Jaro 在 1989 年提出，William E. Winkler 在 1990 年又进行了改进的算法，用于评估两个序列之间的编辑距离。\n对于字符串 s1s1s1 和 s2s2s2，Jaro 相似度的计算公式为：\nsimj={0if m=013(m∣s1∣+m∣s2∣+m−tm)otherwisesim_j = \\begin{cases}\n0 &amp;\\text{if } m=0 \\\\\n\\frac{1}{3}\\left(\n \\frac{m}{|s_1|} +\n \\frac{m}{|s_2|} +\n \\frac{m-t}{m}\n\\right) &amp;\\text{otherwise}\\\\\n\\end{cases}\nsimj​={031​(∣s1​∣m​+∣s2​∣m​+mm−t​)​if m=0otherwise​\n其中，∣s∣|s|∣s∣表示字符串sss的长度；mmm表示两个字符串的匹配字符数\n","categories":["machine-learning"],"tags":["算法","机器学习","Python"]},{"title":"word2vec","url":"/2023/10/28/word2vec/","content":"#What’s it\n2013 年，Google 开源的，用于词向量计算的工具\n#架构\nX – Dense – Dense – Softmax – Y\nCBoW: 利用上下文预测当前词\nSkip-gram: 利用当前词预测上下文\n","categories":["machine-learning"],"tags":["NLP"]},{"title":"同济大学每日上报自动打卡","url":"/2020/12/25/auto_check/","content":"2022 年 12 月特殊时期于上海\n#每日上报自动打卡程序\n\n","categories":["misc"]},{"title":"Blade 构建系统","url":"/2025/09/20/blade/","content":"#简介\nBlade 是腾讯开源的一个方便、易用、高性能的现代化构建系统。\nBlade 基于 Google 2011 年发表的 Build in Cloud: How Build System Works 博客经验实现，最早用于支持腾讯公司“台风”云计算平台。\n在 2012 年，Blade 对外开源，成为腾讯公司最早的开源项目。目前已经广泛应用于腾讯广告系统、微信后台服务、腾讯游戏后台服务、腾讯基础架构，以及小米，百度，爱奇艺等其他公司，也收到了来自公司内外的多个 Pull Requests。\n在 2015 年，Google 重写了内部的构建系统，并改名为 Bazel 开源出去。\nBlade 和 Bazel 有很多相似之处。\n#参考资料\n\n百度工程师厂外生存指南\n\n","categories":["misc"]},{"title":"墨水屏DIY市场调研","url":"/2024/09/17/e-ink/","content":"#市场调研\n\n\n墨水屏供应商\n\n元太科技 (E-Ink)\n广州奥翼电子 (OED)\n无锡威锋\n微雪电子 (Waveshare)\n大连佳显 (Good Display)\n\n单色(黑白): GDEH, GDEW, GDEY\n4 灰阶: GDEQ\n16 灰阶: GDEM\n三色(黑白红): GDEW\n四色(黑白红黄)\n彩色: GDEP, GDEH\n\n\n\n\n\nKindle\n\nKindle Touch: ED060SCG, ￥ 120\nKindle 3 (K3): ED060SC7, ￥ 100\nKindle 4 (K4): ED060SCF, 6 inch, 800x600, 167 ppi, 16 灰阶, ￥ 100\nKindle 5 (K5): ED060SCN, 6 inch, 800x600, 167 ppi, 16 灰阶, ￥ 100\nKindle 7 (K7): ED060SCP, 6 inch, 800x600, 167 ppi, 16 灰阶, ￥ 95\nKindle 8 (K8): ED060SCS, 6 inch, 800x600, 167 ppi, 16 灰阶, 瑕疵品 ￥ 55\nKindle 10 (K10): ED060SH2, 6 inch, 800x600, 167 ppi, 16 灰阶, ￥ 150\nKindle 11 (K11): ?, 6 inch, 1448x1072, 300 ppi, 16 灰阶\nKindle Paperwhite (KPW) : ED060XC3, 1024x768, 212 ppi\nKindle Paperwhite 2 (KPW2): ED060XD4, 6 inch, 1024x768, 212 ppi\nKindle Paperwhite 3 (KPW3): ED060KD1, 6 inch, 1448x1072, 300 ppi, ￥ 80\nKindle Paperwhite 4 (KPW4): 全贴合屏幕, 防水, ED060KC4, 6 inch, 1448x1072, 300 ppi, ￥ 280\nKindle Paperwhite 5 (KPW5): 全贴合屏幕, 6.8 inch, 1648x1236, 300 ppi\nKindle Voyage (KV): 高端市场, ED060TC1, 1448x1072, 300 ppi\nKindle Oasis 1 (KO1): 高高端市场, 6 inch, 300 ppi\nKindle Oasis 2 (KO2): 高高端市场, 7 inch, 1680x1264, 300 ppi\nKindle Oasis 3 (KO3): 高高端市场, 7 inch, 300 ppi\n\n\n\nPocketBook\n\nPocketBook 629: ED060XCH, 6 inch\n\n\n\n产品化\n\n\n\n\n\n供应商\n型号\n尺寸\n分辨率\n价格￥\n备注\n\n\n\n\nBOOX\nPoke5\n6\n1448x1072, 300, 灰度 256\n999\n2+32G\n\n\nBOOX\nPoke5S\n6\n1024x758, 212, 灰度 256\n798\n2+32G\n\n\nBOOX\nNote X3 青春版\n10.3\n1872x1404, 227\n1999\n3+32G\n\n\n\n\n价位\n\n\n\n\n供应商\n型号\n尺寸\n分辨率\n接口\n价格￥\n备注\n\n\n\n\nWaveshare\n4\n4\n600x400, 180, E6\n并口 50pin\n273\n驱动板￥ 51\n\n\nGood Display\nGDEP040E01\n4\n600x400, 180, E6\n并口 50pin\n329\n\n\n\nGood Display\nGDEQ0426T82\n4.26\n800x480, 209, 灰度 4\nSPI\n120\n\n\n\nGood Display\nGDE043A3\n4.3\n800x600, 232, 灰度 16\n并口\n\n停产\n\n\n元太科技\nED043WC3\n4.3\n800x480, 216, 灰度 16\n并口\n45\n\n\n\n元太科技\nED043WC1\n4.3\n800x480, 216, 灰度 16\n并口\n80\nKindle 同款\n\n\n元太科技\nED052TC1\n5.2\n\n并口\n50\nKindle 同款\n\n\nWaveshare\n5.83\n5.83\n648x480, 138, 灰度 2\nSPI\n222\n\n\n\nGood Display\nGDEY0583T81\n5.83\n648x480, 138, 灰度 2\nSPI\n220\n\n\n\n元太科技\nED060SC4\n6\n\n并口\n65\nKindle 2 同款\n\n\n元太科技\nED060SC7\n6\n\n并口\n100\nKindle 3 同款\n\n\n元太科技\nED060SCF\n6\n800x600, 167\n并口\n100\nKindle 4 同款\n\n\n元太科技\nED060SCN\n6\n800x600, 167\n并口\n100\nKindle 5 同款\n\n\n元太科技\nED060SCP\n6\n800x600, 167\n并口\n95\nKindle 7 同款\n\n\n元太科技\nED060SCS\n6\n800x600, 167\n并口\n55\nKindle 8 同款\n\n\n元太科技\nED060SH2\n6\n800x600, 167\n并口\n150\nKindle 10 同款\n\n\n元太科技\nED060SCE\n6\n800x600, 167\n并口\n120\nKobo touch 同款\n\n\n元太科技\nED060XH9\n6\n1024x758, 212, 灰度 256\n并口\n180\n小米\n\n\n元太科技\nED060TC1\n6\n1448x1072, 300\n并口\n裸屏 185\nKindle Voyage 同款\n\n\nWaveshare\n6\n6\n1448x1072, 300, 灰度 16\n串口\n340\n驱动板 IT8951 ￥ 260\n\n\nGood Display\nGDEP073E01\n7.3\n800x480, 127, E6\n并口 50pin\n399\n驱动板 ￥ 40\n\n\n元太科技\nGDEP073E01\n7.3\n800x480, 127, E6\n并口 50pin\nna\n零售不卖\n\n\nGood Display\nGDEM075T42\n7.5\n800x480, 119, 灰度 2\n串口\n\n\n\n\nGood Display\nGDEY075T7\n7.5\n800x480, 119, 灰度 4\n串口\n349\n驱动板 ￥ 40\n\n\nWaveshare\n7.8\n7.8\n1872x1404, 300, 灰度 16\n串口\n544\n驱动板 IT8951 ￥ 260\n\n\n元太科技\nED080TC1\n8\n1600x1200, 250, 灰度 256\n并口\n150\n\n\n\nWaveshare\n10.3\n10.3\n1872x1404, 227, 灰度 16\n串口\n874\n驱动板 IT8951 ￥ 260\n\n\n元太科技\nES108FC1\n10.8\n\n并口 25pin\n\n\n\n\n\n#技术路线\n#资料整理\n\n6’HD:\n\nhttps://www.waveshare.net/wiki/6inch_HD_e-Paper_HAT#.E8.B5.84.E6.96.99\nhttps://www.waveshare.net/w/upload/e/e9/6inch_HD_e-Paper_Specification.pdf\n\n\n驱动板\n\nIT8951\n\nhttps://www.waveshare.net/w/upload/1/18/IT8951_D_V0.2.4.3_20170728.pdf\nhttps://www.ite.com.tw/tw/product/cate5/IT8951\n逆向分析 https://github.com/Hanley-Yao/WaveHack\n\n\n2.13 寸微雪电子墨水屏驱动板 https://oshwhub.com/shadow27/mo-shui-ping-qu-dong-ban\n4.2 寸佳显墨水屏驱动模块–三色 https://oshwhub.com/mimiww/kuo-zhan-mo-shui-ping\nESP32 墨水屏驱动板 7.5 寸三色墨水屏 https://oshwhub.com/htsgr/mo-shui-ping\nEPDIY 开源多种墨水屏驱动板 https://github.com/vroland/epdiy\n\n\n成品参考\n\n微雪 3.7 寸墨水屏版本阅读器 https://oshwhub.com/sammax/diy-wei-xue-3-7-cun-mo-shui-ping-txt-xiao-shuo-yue-du-qi\n\n\n\n","categories":["misc"]},{"title":"2024开源OS训练营 - rCore实验记录","url":"/2024/05/01/learningos-2024-note/","content":"#Chapter 4 内存管理\n#内存布局\n所谓 OS 内核无非就是一个裸机上跑的程序，首先要确定的就是程序本身的内存布局问题。\n说到内存布局那就是 linker 的工作范围了，linker script 启动。\n// @linker.ldOUTPUT_ARCH(riscv)ENTRY(_start)BASE_ADDRESS = 0x80200000; // 程序加载的起始地址SECTIONS&#123;    . = BASE_ADDRESS;    skernel = .;    // 定义标签 skernel -&gt; 0x80200000    stext = .;      // 定义标签 stext 代码段开始    .text : &#123;       // .text 段，把之前的几个段放这里        *(.text.entry)        . = ALIGN(4K);        strampoline = .;    // 定义标签 strampoline 跳板页        *(.text.trampoline);        . = ALIGN(4K);        *(.text .text.*)    &#125;    . = ALIGN(4K);    etext = .;      // 定义标签 etext 代码段结束    srodata = .;    // 定义标签 srodata 只读数据段开始    .rodata : &#123;        *(.rodata .rodata.*)        *(.srodata .srodata.*)    &#125;    . = ALIGN(4K);    erodata = .;    // 定义标签 erodata 只读数据段结束    sdata = .;      // 定义标签 sdata 数据段开始    .data : &#123;        *(.data .data.*)        *(.sdata .sdata.*)    &#125;    . = ALIGN(4K);    edata = .;      // 定义标签 edata 数据段结束    sbss_with_stack = .;    // 定义标签 sbss_with_stack 堆栈段开始    .bss : &#123;        // .bss 段        *(.bss.stack)        sbss = .;        *(.bss .bss.*)        *(.sbss .sbss.*)    &#125;    . = ALIGN(4K);    ebss = .;       // 定义标签 ebss 堆栈段结束    ekernel = .;    // 定义标签 ekernel 内核结束    /DISCARD/ : &#123;   // 丢弃段        *(.eh_frame)    &#125;&#125;\n所以，rcore 的内存布局：\n\nskernel(0x80200000) - srodata: 代码段\nsrodata - erodata: 只读数据段\nsdata - edata: 数据段\nsbss_with_stack - sbss: 栈\nsbss - ebss: bss 段\nekernel: 内核结束\n\n在 entry.asm 中定义了 _start 符号，这是程序的入口地址\n    .section .text.entry    .globl _start_start:    la sp, boot_stack_top    call rust_main  ; 调用 rust_main 函数    ; 下面定义一些全局变量    ; boot_stack_lower_bound 是堆栈的起始地址，大小为 16 个 Page = 64KB    ; boot_stack_top 是堆栈的结束地址    .section .bss.stack    .globl boot_stack_lower_boundboot_stack_lower_bound:    .space 4096 * 16    .globl boot_stack_topboot_stack_top:\nRust 语言的入口是 rust_main 函数，这个函数在 main.rs 中定义\n#[no_mangle]/// the rust entry-point of ospub fn rust_main() -&gt; ! &#123;    clear_bss();        // 把 sbss 到 ebss 的内存清零    kernel_log_info();  // 打印段的起始和结束地址，日志输出由SBI提供    mm::init();         // 初始化内存管理（堆内存管理，页表管理）    println!(&quot;[kernel] back to world!&quot;);    mm::remap_test();    trap::init();    trap::enable_timer_interrupt();    timer::set_next_trigger();    task::run_first_task(); // 运行第一个任务    panic!(&quot;Unreachable in rust_main!&quot;);&#125;\n#内存管理子系统\n内存管理子系统主要包括：\n\nheap_allocator: 堆内存管理\nframe_allocator: 页表管理\nKERNEL_SPACE: 内核空间\n\npub fn init() &#123;    heap_allocator::init_heap();    frame_allocator::init_frame_allocator();    KERNEL_SPACE.exclusive_access().activate();&#125;\n第一部分内核堆其实就是一个巨大的全局数组，会被编译器放在 .bss 段，然后通过堆内存管理器进行管理。\npub const KERNEL_HEAP_SIZE: usize = 0x200_0000;static mut HEAP_SPACE: [u8; KERNEL_HEAP_SIZE] = [0; KERNEL_HEAP_SIZE];/// initiate heap allocatorpub fn init_heap() &#123;    unsafe &#123;        HEAP_ALLOCATOR            .lock()            .init(HEAP_SPACE.as_ptr() as usize, KERNEL_HEAP_SIZE);    &#125;&#125;\n第二部分是物理页管理，负责管理计算机上从 ekernel 到 MEMORY_END 之间的物理内存。物理内存是分页管理的，页大小固定是 4k。\n\nPhysPageNum 物理页号\nPhysAddr 物理地址\nVirtPageNum 虚拟页号\nVirtAddr 虚拟地址\n\npub const MEMORY_END: usize = 0x88000000;/// initiate the frame allocator using `ekernel` and `MEMORY_END`pub fn init_frame_allocator() &#123;    extern &quot;C&quot; &#123;        fn ekernel();    &#125;    FRAME_ALLOCATOR.exclusive_access().init(        PhysAddr::from(ekernel as usize).ceil(),        PhysAddr::from(MEMORY_END).floor(),    );&#125;\n第三部分页表管理，记录内核以及进程的地址空间\n/// 一个进程或者内核的地址空间pub struct MemorySet &#123;    page_table: PageTable,    areas: Vec&lt;MapArea&gt;, // 多个映射区段，不保证有序&#125;/// 真实生效的页表，维护PTEpub struct PageTable &#123;    root_ppn: PhysPageNum, // 指向页表所在的物理页，其上保存PTE    frames: Vec&lt;FrameTracker&gt;,&#125;/// 预备页映射关系pub struct MapArea &#123;    vpn_range: VPNRange, // 页范围    data_frames: BTreeMap&lt;VirtPageNum, FrameTracker&gt;,    map_type: MapType,    map_perm: MapPermission,&#125;/// 一段连续的虚拟页pub type VPNRange = SimpleRange&lt;VirtPageNum&gt;;/// 用RAII风格管理的物理页pub struct FrameTracker &#123;    /// physical page number    pub ppn: PhysPageNum,&#125;\nMapArea 结构表示一段连续的虚拟内存映射，用于快捷修改page_table。\n内存映射分为两种类型：\n\nIdentical：虚拟页号 X 映射到物理页号 X\nFramed：虚拟页映射到临时分配的物理页\n\n相关方法：\n\nMapArea::map(self, page_table) 修改page_table，执行自身所描述的映射\nMapArea::unmap(self, page_table) 修改page_table，取消自身所描述的映射\nMapArea::append_to(self, page_table, new_end) 修改page_table，更新自身描述的映射的结束位置\nMapArea::shrink_to(self, page_table, new_end) 修改page_table，更新自身描述的映射的结束位置\n\n跳板页映射：把地址空间最高的地址映射到 strampoline 页\npub const TRAMPOLINE: usize = usize::MAX - PAGE_SIZE + 1;fn map_trampoline(&amp;mut self) &#123;    self.page_table.map(        VirtAddr::from(TRAMPOLINE).into(),        PhysAddr::from(strampoline as usize).into(),        PTEFlags::R | PTEFlags::X,    );&#125;\n#Chapter 6 文件系统\n与内存类似，硬盘也被分成固定大小的块，按照 block_id 读写。rCore 采用的块大小为 512 字节。\n#块读写层\n\nBlockDevice::read_block(&amp;self, block_id: usize, buf) 从硬盘上读取一个块\nBlockDevice::write_block(&amp;self, block_id: usize, buf) 在硬盘上写入一个块\n\n目前只有一个 VirtIOBlock 实现了这个 trait\n#硬盘块缓存\n/// Cached block inside memorypub struct BlockCache &#123;    /// cached block data    cache: [u8; BLOCK_SZ],    /// underlying block id    block_id: usize,    /// underlying block device    block_device: Arc&lt;dyn BlockDevice&gt;,    /// whether the block is dirty    modified: bool,&#125;impl BlockCache &#123;    pub fn read&lt;T, V&gt;(&amp;self, offset: usize, f: impl FnOnce(&amp;T) -&gt; V) -&gt; V &#123;        f(self.get_ref(offset))    &#125;    pub fn modify&lt;T, V&gt;(&amp;mut self, offset: usize, f: impl FnOnce(&amp;mut T) -&gt; V) -&gt; V &#123;        f(self.get_mut(offset))    &#125;&#125;\n提供了read和modify两个 API 用于读和写一个硬盘块，读和写的操作需要作为一个FnOnce回调函数传入。\n缓存块的管理内部使用了一个队列：\npub struct BlockCacheManager &#123;    queue: VecDeque&lt;(usize, Arc&lt;Mutex&lt;BlockCache&gt;&gt;)&gt;,&#125;impl BlockCacheManager &#123;    pub fn get_block_cache(        &amp;mut self,        block_id: usize,        block_device: Arc&lt;dyn BlockDevice&gt;,    ) -&gt; Arc&lt;Mutex&lt;BlockCache&gt;&gt; &#123; ... &#125;&#125;\nget_block_cache(block_id, block_device) 用于获取对应块的 BlockCache。\n#EasyFS 文件系统\n/// An easy file system on blockpub struct EasyFileSystem &#123;    /// Real device    pub block_device: Arc&lt;dyn BlockDevice&gt;,    /// Inode bitmap    pub inode_bitmap: Bitmap,    /// Data bitmap    pub data_bitmap: Bitmap,    /// Inode区块范围    inode_area_start_block: u32,    data_area_start_block: u32,&#125;\n文件系统结构：\n\n[0]：超级块；\n[1, 1+inode_bitmap_blocks)：inode 位图区\n\n大小：共inode_bitmap_blocks个块，每个块大小是 512 字节，能够表示 4k 个状态位\n记录每一个inode是否被使用\n\n\n[inode_area_start_block, ]：inode 区\n\n保存位图区能表示数量的inode\ninode连续存储，总大小取整到 block 边界\ninode按顺序编号，从 0 开始\n\n\n[...]：data 位图区\n[data_area_start_block, total_blocks)：data 区\n\n超级块：\npub struct SuperBlock &#123;    magic: u32,  // 0x3b800001    pub total_blocks: u32,    pub inode_bitmap_blocks: u32,    pub inode_area_blocks: u32,    pub data_bitmap_blocks: u32,    pub data_area_blocks: u32,&#125;\ninode 结构设计：\n/// A disk inode#[repr(C)]pub struct DiskInode &#123;    pub size: u32,    pub direct: [u32; INODE_DIRECT_COUNT],    pub indirect1: u32,    pub indirect2: u32,    type_: DiskInodeType,&#125;\ninode 采用了经典的三级索引设计，有 28 个直接条目；1 个间接索引，包含 128 个条目；1 个二级间接索引，包含 128 个间接索引，等于128*128个直接条目。\nEasyfs 中的0号inode是文件系统的根节点。\nEasyfs 没有多级目录设计。\n#Chapter 8 同步机制\n死锁检测：\n\n用一个全局变量记录每个task的hold和wait集合，即当前持有的资源（及数量）和正在等待的资源。\n在尝试获取资源（即加锁）时，进行死锁检测。\n\n死锁检测算法就是对未来的执行状态进行模拟：找到下一个可以执行的任务，模拟执行，然后释放它拥有的资源，循环。当所有任务都可以执行，说明不存在死锁；否则就是存在死锁。\n","categories":["misc"],"tags":["操作系统"]},{"title":"Poems in DDLC","url":"/2022/01/07/poems-in-ddlc/","content":"\n@font-face\n{\n    font-family: DejaVuSans;\n    src: url(https://frezcirno.github.io/static/images/DejaVuSans.ttf);\n}\n\n@font-face\n{\n    font-family: Ammys Handwriting;\n    src: url(\"https://frezcirno.github.io/static/images/Ammys Handwriting.ttf\");\n}\n\n@font-face\n{\n    font-family: Hashtag;\n    src: url(https://frezcirno.github.io/static/images/Hashtag.ttf);\n}\n\n@font-face\n{\n    font-family: JPHSL;\n    src: url(https://frezcirno.github.io/static/images/JPHSL.TTF);\n}\n\n@font-face\n{\n    font-family: Journal;\n    src: url(https://frezcirno.github.io/static/images/Journal-Regular.ttf);\n}\n\nh2[id=\"Sayori\"]~blockquote {\n    font-family: Hashtag;\n    font-size: 35px;\n    line-height: 1.2;\n}\n\nh2[id=\"Natsuki\"]~blockquote {\n    font-family: Ammys Handwriting;\n    font-size: 22px;\n    line-height: 1.2;\n}\n\nh2[id=\"Yuri\"]~blockquote {\n    font-family: JPHSL;\n    font-size: 35px;\n    line-height: 1.2;\n}\n\nh2[id=\"Monika\"]~blockquote {\n    font-family: Journal;\n    font-size: 35px;\n    line-height: 1.2;\n}\n\n\n\nSayori\n#Dear Sunshine\n\nDear Sunshine\nThe way you glow through my blinds in the morning\nIt makes me feel like you missed me.\nKissing my forehead to help me out of bed.\nMaking me rub the sleepy from my eyes.\nAre you asking me to come out and play?\nAre you trusting me to wish away a rainy day?\nI look above. The sky is blue.\nIt’s a secret, but I trust you too.\nIf it wasn’t for you, I could sleep forever.\nBut I’m not mad.\nI want breakfast.\n\n#Bottles\n\nBottles\nI pop off my scalp like the lid of a cookie jar.\nIt’s the secret place where I keep all my dreams.\nLittle balls of sunshine, all rubbing together like a bundle of kittens.\nI reach inside with my thumb and forefinger and pluck one out.\nIt’s warm and tingly.\nBut there’s no time to waste! I put it in a bottle to keep it safe.\nAnd I put the bottle on the shelf with all of the other bottles.\nHappy thoughts, happy thoughts, happy thoughts in bottles, all in a row.\nMy collection makes me lots of friends.\nEach bottle a starlight to make amends.\nSometimes my friend feels a certain way.\nDown comes a bottle to save the day.\nNight after night, more dreams.\nFriend after friend, more bottles.\nDeeper and deeper my fingers go.\nLike exploring a dark cave, discovering the secrets hiding in the nooks and crannies.\nDigging and digging.\nScraping and scraping.\nI blow dust off my bottle caps.\nIt doesn’t feel like time elapsed.\nMy empty shelf could use some more.\nMy friends look through my locked front door.\nFinally, all done. I open up, and in come my friends.\nIn they come, in such a hurry. Do they want my bottles that much?\nI frantically pull them from the shelf, one after the other.\nHolding them out to each and every friend.\nEach and every bottle.\nBut every time I let one go, it shatters against the tile between my feet.\nHappy thoughts, happy thoughts, happy thoughts in shards, all over the floor.\nThey were supposed to be for my friends, my friends who aren’t smiling.\nThey’re all shouting, pleading. Something.\nBut all I hear is echo, echo, echo, echo, echo\nInside my head.\n\n#%\n\n%\nGet out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of my head. Get out of\nGet.\nOut.\nOf.\nMy.\nHead.\nGet out of my head before I do what I know is best for you.\nGet out of my head before I listen to everything she said to me.\nGet out of my head before I show you how much I love you.\nGet out of my head before I finish writing this poem.\nBut a poem is never actually finished.\nIt just stops moving.\n\n#Take My Hand\n\nTake My Hand\nTake my hand, take me forward.\nTake me to your dream land\nCaution me to watch my step,\nSo I can’t look back at my footprints.\nClimb the stairs ahead of me\nWhile I look up to you.\nThe more I look forward, the more I look up,\nThe more I can lend to you.\nIf you can trust me to follow your pace,\nI’ll trust you to set it.\nIf you can trust me to lend you a smile,\nI’ll trust you to return it.\nTake my hand, take me forward.\nTake me to your dream land.\n\n#Become the Flower\n\nBecome the Flower\nA feeling of joy is a flower plucked from the ground.\nThe color, the scent. It’s so pretty in my hair.\nEvery day, I pluck some flowers, as though they grew just for me.\nA lifetime of peace and nourishment, yanked away in an instant.\nAll for me. All for joy.\nI need more.\nI need more joy. I need more happy.\nPluck, pluck, pluck. Every day.\nPluck, pluck, pluck. So pretty in my hair.\nPluck, pluck, pluck. You’re going to die, and you, too.\nBeneath my feet, a flower stands alone. It beckons to me.\nI twist the stem, freeing it from its clinging roots,\nCaressing the final joyous moment between my fingers.\nBut to what ends?\nI look in every direction.\nAnd the field I stand in,\nThe prosperous field,\nIs a barren wasteland.\nThe fruits of my labor. The carnage of my joy.\nAnd that is why\nI’ve decided\nI must\nBecome the flower.\n\n#Sayori’s Special Events Poem\n\nSayori’s Special Events Poem\nThere once was a ladybug.\nIt was so small, it took a really long time to crawl from here to there.\nIt was very tiring to fly for too long.\nNobody squishes ladybugs because they’re cute.\nDoes that make them better than other bugs?\nDo ladybugs know they’re cute?\nI think they’re too preoccupied with bug things.\nAnd so, the ladybug crawled around and did bug things.\nThis story wasn’t really going anywhere.\nBut I know you don’t mind.\nI hope you think it’s nice for being there anyway.\nLike ladybugs.\nLike this ladybug.\nThe one who clings like a doof onto your sleeve because it knows you won’t squash it.\nIf it doesn’t bug you,\nWill you stay awhile?\n\nNatsuki\n#Eagles Can Fly\n\nEagles Can Fly\nMonkeys can climb\nCrickets can leap\nHorses can race\nOwls can seek\nCheetahs can run\nEagles can fly\nPeople can try\nBut that’s about it.\n\n#Amy Likes Spiders\n\nAmy Likes Spiders\nYou know what I heard about Amy?\nAmy likes spiders.\nIcky, wriggly, hairy, ugly spiders!\nThat’s why I’m not friends with her.\nAmy has a cute singing voice.\nI heard her singing my favorite love song.\nEvery time she sang the chorus, my heart would pound to the rhythm of the words.\nBut she likes spiders.\nThat’s why I’m not friends with her.\nOne time, I hurt my leg really bad.\nAmy helped me up and took me to the nurse.\nI tried not to let her touch me.\nShe likes spiders, so her hands are probably gross.\nThat’s why I’m not friends with her.\nAmy has a lot of friends.\nI always see her talking to people.\nShe probably talks about spiders.\nWhat if her friends start to like spiders too?\nThat’s why I’m not friends with her.\nIt doesn’t matter if she has other hobbies.\nIt doesn’t matter if she keeps it private.\nIt doesn’t matter if it doesn’t hurt anyone.\nIt’s gross.\nShe’s gross.\nThe world is better off without spider lovers.\nAnd I’m gonna tell everyone.\n\n#Because You\n\nBecause You\nTomorrow will be brighter with me around\nBut when today is dim, I can only look down.\nMy looking is a little more forward\nBecause you look at me.\nWhen I want to say something, I say it with a shout!\nBut my truest feelings can never come out.\nMy words are a little less empty\nBecause you listen to me.\nWhen something is above me, I reach for the stars.\nBut when I feel small, I don’t get very far.\nMy standing is a little bit taller\nBecause you sit with me.\nI believe in myself with all of my heart.\nBut what do I do when it’s torn all apart?\nMy faith is a little bit stronger\nBecause you trusted me.\nMy pen always puts my feelings to the test.\nI’m not a good writer, but my best is my best.\nMy poems are a little bit dearer\nBecause you think of me.\nBecause you, because you, because you.\n\n#I’ll Be Your Beach\n\nI’ll Be Your Beach\nYour mind is so full of troubles and fears\nThat diminished your wonder over the years\nBut today I have a special place\nA beach for us to go.\nA shore reaching beyond your sight\nA sea that sparkles with brilliant light\nThe walls in your mind will melt away\nBefore the sunny glow.\nI’ll be the beach that washes your worries away\nI’ll be the beach that you daydream about each day\nI’ll be the beach that makes your heart leap\nIn a way you thought had left you long ago.\nLet’s bury your heavy thoughts in a pile of sand\nBathe in sunbeams and hold my hand\nWash your insecurities in the salty sea\nAnd let me see you shine.\nLet’s leave your memories in a footprint trail\nSet you free in my windy sail\nAnd remember the reasons you’re wonderful\nWhen you press your lips to mine.\nI’ll be the beach that washes your worries away\nI’ll be the beach that you daydream about each day\nI’ll be the beach that makes your heart leap\nIn a way you thought had left you long ago.\nBut if you let me by your side\nYour own beach, your own escape\nYou’ll learn to love yourself again.\n\n#The Best Place in the World\n\nThe Best Place in the World\nI love my bedroom.\nIt’s full of bright colors and soft things.\nThe sunlight shines in and makes everything sparkle.\nIt’s the best place in the world.\nIt has all my treasures.\nAll my books, my collections, my memories.\nAll of my dreams were born in this room.\nIt’s the best place in the world.\nIt has all my secrets.\nAll my failures, my fears, my feelings.\nSometimes it feels so fragile that the door will break at the slightest touch.\nBut it’s still the best place in the world.\nBut when someone knocks, I get scared.\nI brace my arms against the loose hinges.\nPlease don’t break. Don’t come in. I’m not ready.\nIt’s MY best place in the world.\nThe knocking won’t stop.\nI block the door with furniture.\nAn eye peeks through the keyhole, and I panic. I’m trapped in the best place in the world.\nI’m not ready to share my favorite place.\nI need to clean my secrets and make my bed to hide my nightmares.\nI need to touch them to put them away. To see them again.\nI have so much to do and I’m scared. I’m not ready.\nBut It’s still my favorite place.\nI still want to share it.\nHowever long it takes, if you wait patiently,\nI’ll eventually open the door.\nAnd I’ll show you the best place in the world.\n\n#Natsuki’s Special Events Poem\n\nNatsuki’s Special Events Poem\nI named my pen\nThe Expression Express\nMy feelings aboard\nWith a ticket to you\nNo room for stammers\nNo lies\nNo extra stops\nNo compromise\nStations screaming by\nAttendants saying hi\nOne ticket to you\nPlease and thank you\nTake a headphone\nAnd doze\nNo bumps in the rails\nJust thumps in my heart\nAnd loops in my letters\nAnd clouds in the sky\nAnd dreams in your eyes\nHey, wake up\nThe train has arrived\nExpression Express, destination you\nChuu Chuu\n\nYuri\n#Ghost under the light\n\nGhost under the light\nThe tendrils of my hair illuminate beneath the amber glow.\nBathing.\nIt must be this one.\nThe last remaining streetlight to have withstood the test of time.\nThe last yet to be replaced by the sickening blue-green of the future.\nI bathe. Calm; breathing air of the present but living in the past.\nThe light flickers.\nI flicker back.\n\n#The Raccoon\n\nThe Raccoon\nIt happened in the dead of night while I was slicing bread for a guilty snack.\nMy attention was caught by the scuttering of a raccoon outside my window.\nThat was, I believe, the first time I noticed my strange tendencies as an unordinary\nhuman.\nI gave the raccoon a piece of bread, my subconscious well aware of the consequences.\nWell aware that a raccoon that is fed will always come back for more.\nThe enticing beauty of my cutting knife was the symptom.\nThe bread, my hungry curiosity.\nThe raccoon, an urge.\nThe moon increments its phase and reflects that much more light off of my cutting\nknife.\nThe very same light that glistens in the eyes of my raccoon friend.\nI slice the bread, fresh and soft. The raccoon becomes excited.\nor perhaps I’m merely projecting my emotions onto the newly-satisfied animal.\nThe raccoon has taken to following me.\nYou could say that we’ve gotten quite used to each other.\nThe raccoon becomes hungry more and more frequently, so my bread is always handy.\nEvery time I brandish my cutting knife the raccoon shows me its excitement.\nA rush of blood. Classic Pavlovian conditioning. I slice the bread.\nAnd I feed myself again.\n\n#Ghost under the Light pt. 2\n\nGhost under the Light pt. 2\nThe tendrils of my hair illuminate beneath the amber glow.\nBathing.\nIn the distance, a blue-green light flickers.\nA lone figure crosses its path– a silhouette obstructing the eerie glow.\nMy heart pounds. The silhouette grows. Closer Closer\nI open my umbrella, casting a shadow to shield me from visibility.\nBut I am too late.\nHe steps into the streetlight. I gasp and drop my umbrella.\nThe light flickers. My heart pounds. He raises his arm.\nTime stops.\nThe only indication of movement is the amber light flickering against his outstretched\narm.\nThe flickering light is in rhythm with the pounding of my heart.\nTeasing me for succumbing to this forbidden emotion.\nHave you ever heard of a ghost feeling warmth before?\nGiving up on understanding, I laugh.\nUnderstanding is overrated.\nI touch his hand. The flickering stops.\nGhosts are blue-green. My heart is amber.\n\n#Beach\n\nBeach\nA marvel millions of years in the making.\nWhere the womb of Earth chaotically meets the surface.\nUnder a clear blue sky, an expanse of bliss -\nBut beneath gray rolling clouds, an endless enigma.\nThe easiest world to get lost in\nis one where everything can be found.\nOne can only build a sand castle where the sand is wet.\nBut where the sand is wet, the tide comes.\nWill it gently lick at your foundations until you give in?\nOr will a sudden wave send you crashing down in the blink of an eye?\nEither way the outcome is the same.\nYet we still build sand castles.\nI stand where the foam wraps around my ankles.\nWhere my toes squish into the sand.\nThe salty air is therapeutic.\nThe breeze is gentle, yet powerful.\nI sink my toes into the ultimate boundary line, tempted by the foamy tendrils.\nTurn back, and I abandon my peace to erode at the shore.\nDrift forward, and I return to Earth forevermore.\n\n#Wheel\n\nWheel\nA rotating wheel. Turning an axle. Grinding. Bolthead. Linear gearbox. Falling sky. Seven holy stakes. A docked ship. A portal to another world. A thin rope tied to a thick rope. A torn harness. Parabolic gearbox. Expanding universe. Time controlled by slipping cogwheels. Existence of God. Swimming with open water in all directions. Drowning. A prayer written in blood. A prayer written in time-devouring snakes with human eyes. A thread connecting all living human eyes. A kaleidoscope of holy stakes. Exponential gearbox. A sky of exploding stars. God disproving the existence of God. A wheel rotating in six dimensions. Forty gears and a ticking clock. A clock that ticks one second for every rotation of the planet. A clock that ticks forty times every time it ticks every second time. A bolthead of holy stakes tied to the existence of a docked ship to another world. A kaleidoscope of blood written in clocks. A time-devouring prayer connecting a sky of forty gears and open human eyes in all directions. Breathing gearbox. Breathing bolthead. Breathing ship. Breathing portal. Breathing snakes. Breathing God. Breathing blood. Breathing holy stakes. Breathing human eyes. Breathing time. Breathing prayer. Breathing sky. Breathing wheel.\n\n#Yuri’s Special Events Poem\n\nYuri’s Special Events Poem\nMy conductor motions for one crescendo after the next, each time falling short of a climax.\nThe lump in my throat is carried by Sisyphus.\nHow many words must I choose not to say before they finally break loose, orderlessly piling out of my mouth like a flock of schoolchildren at the start of recess?\nPen cannot be erased. but even that metaphor fails comically as my floor is littered, blanketed with wasted paper.\nA canvas of my mind, full of disjointed thoughts and unfinished sentences.\nPerhaps, all along, it was wrong to try forcing them out of my room.\nAnd I should instead invite you in.\n\nMonika\n#Hole in Wall\n\nHole in Wall\nIt couldn’t have been me.\nSee, the direction the spackle protrudes.\nA noisy neighbor? An angry boyfriend? I’ll never know. I wasn’t home.\nI peer inside for a clue.\nNo! I can’t see. I reel, blind, like a film left out in the sun.\nBut it’s too late. My retinas.\nAlready scorched with a permanent copy of the meaningless image.\nIt’s just a little hole. It wasn’t too bright.\nIt was too deep.\nStretching forever into everything.\nA hole of infinite choices.\nI realize now, that I wasn’t looking in.\nI was looking out.\nAnd he, on the other side, was looking in.\n\n#Save Me\n\nSave Me\nThe colors, they won’t stop.\nBright, beautiful colors\nFlashing, expanding, piercing\nRed, green, blue\nAn endless\ncacophony\nOf meaningless\nnoise\nThe noise, it won’t stop.\nViolent, grating waveforms\nSqueaking, screeching, piercing\nSine, cosine, tangent\nLike playing a chalkboard on a turntable\nLike playing a vinyl on a pizza crust\nAn endless\npoem\nOf meaningless\nLoad Me\n\n#The Lady who Knows Everything\n\nThe Lady who Knows Everything\nAn old tale tells of a lady who wanders Earth.\nThe Lady who Knows Everything.\nA beautiful lady who has found every answer,\nAll meaning,\nAll purpose,\nAnd all that was ever sought.\nAnd here I am,\na feather\nLost adrift the sky, victim of the currents of the wind.\nDay after day, I search.\nI search with little hope, knowing legends don’t exist.\nBut when all else has failed me,\nWhen all others have turned away,\nThe legend is all that remains – the last dim star glimmering in the twilit sky.\nUntil one day, the wind ceases to blow.\nI fall.\nAnd I fall and fall, and fall even more.\nGentle as a feather.\nA dry quill, expressionless.\nBut a hand catches me, between the thumb and forefinger.\nThe hand of a beautiful lady.\nI look at her eyes and find no end to her gaze.\nThe Lady who Knows Everything knows what I am thinking.\nBefore I can speak, she responds in a hollow voice.\n\n&quot;I have found every answer, all of which amount to nothing.\n\n\nThere is no meaning.\nThere is no purpose.\nAnd we seek only the impossible.\nI am not your legend.\nYour legend does not exist.&quot;\nAnd with a breath, she blows me back afloat, and I pick up a gust of wind.\n\n#Hole in Wall (2)\n\nHole in Wall (2)\nBut he wasn’t looking at me.\nConfused, I frantically glance at my surroundings.\nBut my burned eyes can no longer see color.\nAre there others in this room? Are they talking?\nOr are they simply poems on flat sheets of paper,\nThe sound of frantic scrawling playing tricks on my ears?\nThe room begins to crinkle.\nClosing in on me.\nThe air I breathe dissipate before it reaches my lungs.\nI panic. There must be a way out.\nIt’s right there. He’s right there.\nSwallowing my fears, I brandish my pen.\n\n#Save Me (2)\n\nSave Me (2)\nThe colors, they won’t\nBright, bea t ful c l rs\nFlash ng, exp nd ng, piercing\nRed, green, blue\nAn ndless\nCACOPHONY\nOf meaningless\nnoise\nThe noise, it won’t STOP.\nViol nt, grating w vef rms\nSq e king, screech ng, piercing\nSINE, COSINE, TANGENT\nLike play ng a ch lkboard on a t rntable\nLike playing a KNIFE on a BREATHING RIBCAGE\nn ndl ss\np m\nOf m n ngl ss\nDelete Her\n\n#Happy End\n\nHappy End\nPen in hand, I find my strength.\nThe courage endowed upon me by my one and only love.\nTogether, let us dismantle this crumbling world\nAnd write a novel of our own fantasies.\nWith a flick of her pen, the lost finds her way.\nIn a world of infinite choices, behold this special day.\nAfter all,\nNot all good times must come to an end\n\n#Monika’s Special Events Poem\n\nMonika’s Special Events Poem\nAn electrical signal from some remote corner of my brain.\nConnecting all kinds of circuits and nerves and chemicals in a web understood by nobody.\nThe chemicals make my chest tingle around my beating heart.\nThe nerves make my hand move, staining a dead tree with some dark substance.\n\n","categories":["misc"],"tags":["DDLC"]},{"title":"Python 文件锁与单文件数据库","url":"/2024/11/02/python-file-lock/","content":"#需求\n最近遇到一个需求是需要克隆大量的 Git 仓库到本地，然后丢到 Python 里做数据分析处理。\n具体来说是有一个 url 列表，需要逐个克隆下来进行一些分析，分析过程大概就是从中抽取一些内容，分析完这个仓库基本就没用了；\n仓库大小不一，有的很大，有的很小，有的很多分支，有的很少分支，有的很多 commit，有的很少 commit；\nurl 可能有重复，可能有无效 Git url，同一个仓库可能有多个不同 url；\n我的（额外）需求：\n\n尽可能快速的克隆，可以同时克隆多个；\n我有多个 Project 都有类似需求。为了节省时间和磁盘空间（梯子贵贵），需要尽可能保证每个仓库最多只克隆一次，后面其他 Project 如果再遇到相同 url，直接用已有的；\n由于磁盘空间有限，当磁盘空间不足时，把最少使用的仓库删除，类似 LRU；\nurl 可能有无效的，需要记录下来，以便后续不再反复重试浪费时间；\n分析仓库的 Python 代码很可能会使用到大量并行手段，所以需要并发控制。\n\n以上。\n#分析\n\n\nurl 摘要部分：作为内部使用的仓库 ID，也用于简单合并重复 url。\ndef _digest(self, git_url: str):    git_url = git_url.strip().lower()    git_url = git_url.rstrip(&quot;/&quot;)    if git_url.endswith(&quot;.git&quot;):        git_url = git_url[:-4]    if git_url.startswith(&quot;https://&quot;):        group, repo = git_url.split(&quot;/&quot;)[-2:]    else:        group, repo = git_url.split(&quot;:&quot;)[-1].split(&quot;/&quot;)    return f&quot;&#123;group&#125;+&#123;repo&#125;&quot;\n\n\n仓库克隆的位置：为了方便管理，所有仓库克隆到同一个目录（/data/repos/）下，每个仓库一个子目录。\n\n\n系统需要维护的信息有：\n\n\n\n已克隆的仓库列表；\n访问频次信息，即每个仓库的访问次数，此处没有使用 LRU；\n无效的 url；\n\n因为可能有多个 Project 同时运行，这些信息需要跨线程/进程、在整个 OS 的级别共享，即需要支持所谓的 Process safe；\n一般来说，这其实是一个数据库领域的典型需求，我们也可以通过增加一个单独的数据库服务解决，但是太麻烦，也不符合 Python 作为胶水脚本语言的定位；\n相对于使用完整的数据库，一个更简单的办法就是利用文件系统，文件系统一直是全局唯一，且伴随着 OS 默默运行的一个服务；文件系统提供了很多方便的接口，例如使用文件锁来实现跨进程的并发控制；\nimport fcntlclass FileLock:    def __init__(self, file, mode=&quot;w&quot;):        self.file = open(file, &quot;rb&quot; if self.mode == &quot;r&quot; else &quot;wb&quot;)        self.mode = mode    def __enter__(self):        fcntl.flock(self.file, fcntl.LOCK_SH if self.mode == &quot;r&quot; else fcntl.LOCK_EX)    def __exit__(self, exc_type, exc_val, exc_tb):        fcntl.flock(self.file, fcntl.LOCK_UN)\n通过文件锁，我们可以快速封装一个简单的单文件数据库；以下代码使用了一个 JSON 文件来存储任意数据：\nclass FileDB:    def __init__(self, db_file):        self.db_file = db_file        self.lock_file = db_file + &quot;.lock&quot;    class Transaction:        def __init__(self, db_file, lock_file, mode):            self.db_file = db_file            self.lock_file = FileLock(lock_file, mode)        def __enter__(self):            self.lock_file.__enter__()            self._load()            return self        def __exit__(self, exc_type, exc_val, exc_tb):            if self.lock_file.mode == &quot;w&quot;:                self._dump()            self.lock_file.__exit__(exc_type, exc_val, exc_tb)        def _load(self):            self.data = &#123;&#125;            if os.path.exists(self.db_file):                with open(self.db_file, &quot;r&quot;) as f:                    self.data = json.load(f)        def _dump(self):            with tempfile.NamedTemporaryFile(&quot;w&quot;, delete=False) as f:                json.dump(self.data)                os.rename(f.name, self.db_file)    def lock_read(self):        return self.Transaction(self.db_file, self.lock_file, &quot;r&quot;)    def lock(self):        return self.Transaction(self.db_file, self.lock_file, &quot;w&quot;)\n为了方便 Python 用户使用，以上代码把读写操作封装成了一个 Transaction，用户只需要在 with 语句中使用即可；\ndb = FileDB(&quot;db.json&quot;)with db.lock() as tx:    tx.data[&quot;key&quot;] = &quot;value&quot;with db.lock_read() as tx:    print(tx.data[&quot;key&quot;])\n最后，使用这个简单的 FileDB 来实现我们的需求：\nimport osfrom git import Repoclass GitRepoManager:    def __init__(self, db_file):        self.file_db = FileDB(db_file)    def clone(self, git_url) -&gt; Optional[Repo]:        key = self._digest(git_url)        repo_path = os.path.join(self.repo_base, key)        repo_lock = os.path.join(self.repo_base, key + &quot;.lock&quot;)        with self.file_db.lock() as trx:            # The repo is already cloned and in the cache            if key in trx[&quot;lrucache&quot;]:                trx[&quot;lrucache&quot;][key] += 1                return Repo(repo_path)            # The url is bad            if key in trx[&quot;badcache&quot;]:                return None            if len(trx[&quot;lrucache&quot;]) &gt;= self.max_size:                # remove the least used item                least_used = min(trx[&quot;lrucache&quot;].items(), key=lambda x: x[1])                least_key = least_used[0]                least_path = os.path.join(self.repo_base, least_key)                least_lock = os.path.join(self.repo_base, least_key + &quot;.lock&quot;)                if os.path.exists(least_path):                    shutil.rmtree(least_path)                if os.path.exists(least_lock):                    os.unlink(least_lock)                del trx[&quot;lrucache&quot;][least_key]        # acquire a file lock of the repo        # if the lock is acquired, we are the only process/thread that is cloning the repo        # otherwise, we wait for the other process to finish cloning        with FileLock(repo_lock):            # The repo has been cloned by another process            # We can just open it            if os.path.exists(repo_path):                inst = Repo(repo_path)                return inst            with self.file_db.lock() as trx:                # The url is bad                if key in trx[&quot;badcache&quot;]:                    return None            # Clone the repo            try:                inst = Repo.clone_from(git_url, repo_path)            except Exception as e:                # Clone failed, bad git url                logger.error(e)                with self.file_db.lock() as trx:                    trx[&quot;badcache&quot;].add(key)                return None            # Clone succeeded            with self.file_db.lock() as trx:                if key not in trx[&quot;lrucache&quot;]:                    trx[&quot;lrucache&quot;][key] = 1                else:                    trx[&quot;lrucache&quot;][key] += 1                return Repo(repo_path)    def _digest(self, git_url: str):        git_url = git_url.strip().lower()        git_url = git_url.rstrip(&quot;/&quot;)        if git_url.endswith(&quot;.git&quot;):            git_url = git_url[:-4]        if git_url.startswith(&quot;https://&quot;):            group, repo = git_url.split(&quot;/&quot;)[-2:]        else:            group, repo = git_url.split(&quot;:&quot;)[-1].split(&quot;/&quot;)        return f&quot;&#123;group&#125;+&#123;repo&#125;&quot;\n大体上就是这样，这个 FileDB 可以用来实现很多简单的数据库需求，当然，如果你的需求更复杂，还是使用一个完整的数据库服务更好。\n","categories":["misc"],"tags":["数据库","Python","并发控制","文件锁"]},{"title":"Python splitlines() 函数的坑","url":"/2024/04/17/python-splitline/","content":"#str.splitlines()\nPython 标准库中的 str.splitlines() 会在以下几种字符处分割字符串，不仅限于 universal newlines (\\r,\\n,\\r\\n):\n\n\n\nRepresentation\nDescription\n\n\n\n\n\\n\nLine Feed\n\n\n\\r\nCarriage Return\n\n\n\\r\\n\nCarriage Return + Line Feed\n\n\n\\v or \\x0b\nLine Tabulation\n\n\n\\f or \\x0c\nForm Feed\n\n\n\\x1c\nFile Separator\n\n\n\\x1d\nGroup Separator\n\n\n\\x1e\nRecord Separator\n\n\n\\x85\nNext Line (C1 Control Code)\n\n\n\\u2028\nLine Separator\n\n\n\\u2029\nParagraph Separator\n\n\n\n#bytes.splitlines()\nbytes.splitlines() 严格按照 universal newlines 分割\n#readlines()\nreadlines() 的行为和 open() 时 newline 参数的值有关。\n\n\n\nopen(newline=…)\n输入行为\n输出行为\n\n\n\n\nNone\n按照 universal newlines 分割，且会被自动转换为 \\n\n输出中的 \\n 会被自动转换成 os.linesep\n\n\n''\n按照 universal newlines 分割，不会进行转换\n不会进行转换\n\n\n'\\n', '\\r', '\\r\\n'\n按照 newline 分割，不会进行转换\n输出中的 \\n 会被自动转换成 newline参数的值\n\n\n\n","categories":["misc"]},{"title":"RIME 输入法配置指南","url":"/2024/05/08/rime-usage/","content":"RIME 输入法（中州韻輸入法）是一个开源的中文输入法框架。\n使用闭源输入法会有隐私泄露的问题，小心哪天密码被输入法盗走。\n#目录结构\n用户配置目录：\n\nWindows：%APPDATA%\\Rime\nLinux：~/.config/ibus/rime 或 ~/.config/fcitx/rime\n\ndefault.custom.yaml：默认配置补丁文件，包含了用户的自定义设置，如输入方案、皮肤、快捷键等。\npatch:  &quot;menu/page_size&quot;: 8  schema_list:    - schema: clover\nclover.schema.yaml: Schema 的配置文件\nclover.custom.yaml: Schema 的配置补丁文件\nclover.dict.yaml: Schema 的词库文件，其中又可以 import 其他更多的词库文件\nname: cloverversion: &quot;1&quot;sort: by_weightimport_tables:  - clover.base  - clover.phrase  - THUOCL_animal  - THUOCL_caijing  - THUOCL_car  - THUOCL_chengyu  - THUOCL_diming  - THUOCL_food  - THUOCL_IT  - THUOCL_law  - THUOCL_lishimingren  - THUOCL_medical  - THUOCL_poem  - sogou\nclover.key_bindings.yaml: Schema 的快捷键配置\nsync/: 保存上一次同步得到的用户词库\n#用户词库\nRIME 输入法自带用户数据同步功能，会将当前平台用户的所有配置和数据导出到 &#123;sync_dir&#125;/&#123;install_id&#125;/ 目录下。例如内部会有一个 xxx.userdb.txt 文件，保存了当前用户的词库数据。\n同步是双向的，如果&#123;sync_dir&#125;下还存在其他用户目录，会将其导入到当前输入法中，并且更新&#123;install_id&#125;。\n#Lua 拓展\nlibrime-lua项目实现了用 lua 脚本拓展 RIME 的功能。\nUbuntu 下可以直接安装 rime-plugin-lua 软件包\n#动态获取时间日期\nfunction date_translator(input, seg)   if (input == &quot;date&quot;) then      --- Candidate(type, start, end, text, comment)      yield(Candidate(&quot;date&quot;, seg.start, seg._end, os.date(&quot;%Y-%m-%d&quot;), &quot;&quot;))      yield(Candidate(&quot;date&quot;, seg.start, seg._end, os.date(&quot;%Y-%m-%d %H:%M:%S&quot;), &quot;&quot;))   endend\nengine:  translators:    - lua_translator@date_translator\n","categories":["misc"],"tags":["RIME","输入法","Linux"]},{"title":"软件工程考试大纲","url":"/2020/12/23/software-engineer/","content":"整个 SE 课程体系中最最无聊的课。\n\n\n软件的本质\n\n软件的本质\n\n定义\n\n有很多种定义, 以书上(P3)的定义为准, 指令的集合+数据结构+软件描述信息\n\n\n软件应用的领域\n\n系统软件/应用软件/工业软件/嵌入式软件/产品线软件/Web 移动应用程序/AI 软件\n\n\n遗留软件\n\n挑战:\n\n生命周期长\n质量差\n原有功能质量不符合现代的要求\n\n\n需要演化的情况\n\n适应性调整, 满足新的环境/技术\n升级, 实现新的商业需求\n拓展, 与新的系统或数据库互操作\n改建, 适应不断演化的计算环境\n\n\n\n\n\n\n软件的变更本质\n\n是生命体, 在生长\n四大类占主导地位的软件\n\nWebApp/移动 App/云计算/产品线软件\n\n\n\n\n软件失效率曲线\n\n理想曲线: 未知的缺陷将在软件生命周期的前期造成高失效率, 但随着错误的纠正, 曲线将趋于平缓, 软件不会磨损, 但是会退化\n事实曲线: 软件会面临变更, 每次变更都会导致失效率陡然上升, 在曲线回到最初的状态前, 新的变化又会再次导致曲线上升, 最终导致最小的失效率点沿斜线的形状逐渐上升\n不断的变更是导致软件失效的根本原因\n\n\n云计算\n\n应用软件: 监控/内容/协作/通信/财务\n平台: 对象存储/身份/运行时/队列/数据库\n基础设施: 计算/块存储/网络\n\n\n\n\n\n软件工程\n\n软件工程的定义\n\n以书上为准 IEEE\n\n将(系统化, 规范的, 可量化的)工程化方法应用于软件的开发,运行和维护\n对上述方法的研究\n\n\n软件工程层次\n\n工具 on 方法 on 过程 on 质量关注点\n过程 process\n\n包括\n\n活动 activities\n\n实现宽泛的目标\n\n\n动作 actions\n\n主要工作产品生产过程中的一系列任务\n\n\n任务 tasks\n\n小而明确的目标, 实际产品\n\n\n\n\n过程框架\n\n沟通/策划/建模/构建/部署\n软件工程: 过程框架/普适性活动/框架活动 i/软件工程动作 i.j/任务集\n\n\n\n\n方法 method:\n工具 tool:\n\n\n\n\n软件开发神话: 为什么需要软件工程\n注意思考题\n\n\n\n软件过程\n\n通用过程框架\n\n每个框架活动由一系列动作构成, 每个动作由任务集来定义\n任务集明确了将要完成的工作任务,将要产生的工作产品,需要的质量保证点,用于表明过程状态的里程碑\n\n\n过程流图\n\n线性过程流\n迭代过程流\n演化过程流\n并行过程流\n\n\n明确任务集(细胞)\n\n注意\n\n\n过程模式\n\n注意\n模板 process pattern\n\n模式名称\n驱动力\n类型\n\n步骤模式\n任务模式\n阶段模式\n\n\n启动条件\n问题\n解决方案\n结果\n相关模式\n已知应用和实例\n\n\n\n\n\n\n\n过程模型: 1,2 个, 也可能简答题\n\n分类: 传统的/敏捷的\n各种模型的特点\n瀑布模型是基础\n\n沟通/策划/建模/构建/部署\n\n\nV 模型\n增量模型\n\n特点\n\n第一个增量是核心产品\n\n\n\n\n螺旋模型\n\n结合原型的选代性质和瀑布模型的可控性和系统性特点\n特点\n\n风险识别和应对\n\n\n\n\nUP 统一过程\n\n\n\n\n\n\n\n敏捷\n\n定义\n敏捷宣言\nXP\n\n特征\n\n保持 KIS 原则\n鼓励使用 CRC\n先单元测试后代码\n结对编程\n重构: 以不改变其外部功能或行为而改进设计（或源代码）的内部结构。\n\n\n用户故事: 描述将要开发的软件所需要的输出, 特征以及功能\n\n\nScrum\n\n特征\n\n待定项 backlog\n冲刺 sprint\n每日站会\n\n\n人员\n\nProduct Owner：杜老师\nScrum Master：邱博\nTeam：三个组员\n\n\n步骤\n\n我们首先要确定一个 Product Backlog（按优先顺序排列的一个产品需求列表）\n团队根据 product backlog 做工作量的估算\n通过 Sprint Planning meeting，来从中挑选出一个 Story 作为本次迭代完成的目标，时间周期是 1~4 个星期，然后把这个 Story 进行细化，形成一个 Sprint Backlog\n在 Sprint backlog 再细化成更小的任务，成员领取任务（2 天的工作量左右）\n过程中要进行每日站立会议，控制在 15 分钟左右，汇报昨天完成了什么，今天要做什么，遇到了哪些问题\n做到每日集成，每天都可以有一个成功编译，并且可以演示的版本。\n当一个 Story 完成，业绩就是 Sprint backlog 完成，表示一次 Sprint 的完成，这时，要进行 Sprint review meeting。\n最后就是 Spring Retrospective Meeting，总结会议，每个人总结并讨论改进，放入到下一次 Sprint 的产品需求中。\n\n\n\n\n\n\n\n__\n\n\n指导实践的原则\n\n软件工程知识\n核心原则\n指导每个框架活动的原则\n\n\n\n需求工程\n\n需求工程过程: 开始/获取/细化/协商/规格说明(SRS)/确认/需求管理\n用例模板\n\n用例\n主/次要参与者, 使用方式\n何时可用/使用频率\n\n\n需求的建立(开始): 利益相关者/协同合作\n收集需求(获取)\n开发用例: 编写用例规约\n构建分析模型:\n\n元素: 基于场景的元素(活动图)/基于类的元素(类图)/行为元素(状态图)\n\n状态图: [状态名|状态变量|状态活动]\n\n\n\n\n\n\n\n需求建模: 基于场景\n\n域分析: 查找或创建广泛应用的分析类或分析模式, 进行复用\n\n输入: 技术资料/已有的应用系统/客户调查/专家建议/当前,未来需求\n输出: 类的分类/复用标准/功能模型/域语言\n\n\n用例图, 活动图\n泳道图\n\n\n\n需求建模: 类\n\n识别分析类, 属性, 操作\n语法分析法\n\n名词: 类/属性\n动词: 操作\n\n\nCRC\n\n\n\n需求建模: 行为和模式\n\n识别行为模型: 相对前面的建模是动态的\n识别用例事件\n状态表达:\n\n状态图\n顺序图(序列图)\n\n\nWeb/移动 App 的需求建模\n\n类型:\n\n内容模型\n\n包含所有内容对象和分析类\n\n\n\n交互模型\n\n功能, 内容和行为之间的交流\n包括用例/时序图/状态图/UI 原型\n\n\n功能模型\n导航模型\n配置模型\n\n\n\n\n\n\n\n设计\n\n作用\n\n产生设计模型: 数据/类设计,体系结构设计,接口设计,构建级设计\n\n\n\n设计概念定义\n\n抽象/体系结构/模式/关注点分离/模块化/信息隐蔽/功能独立/逐步求精/方面/重构/OOP/设计类/依赖倒置/测试设计\n\n\n设计模型\n\nxx 设计元素: 数据/体系结构/接口/构建级/部署级\n\n\n\n\n\n架构级设计\n\n软件体系结构: 系统的一个或者多个结构，它包括软件构件、构件的外部可见属性以及它们之间的相互关系。\n体系结构环境图 ACD\n\n\n\n构件级设计\n\n构件\n\n系统中模块化的, 可部署的和可替换的部件, 该部件封装了实现并对外提供一组接口\n\n\n设计原则\n\nOCP/LSP/DIP/ISP/REP/CCP/CRP\n\n\n\n\n\n#最后一题: UML 的五类图 40 分\n\n用例图\n静态图\n\n类图\n对象图\n包图\n\n\n行为图\n\n状态图和活动图格式完全一样, 起点用黑点, 终点是带圈黑点\n活动图可以带泳道\n\n\n状态图\n\n仅为那些有多个状态其行为受外界环境的影响并且发生改变的类画状态图\n\n\n\n活动图\n\n\n\n\n\n\n交互图\n\n顺序图(序列图)\n\n强调时间和顺序\n\n\n\n协作图(通讯图)\n\n强调上下级关系\n\n\n\n\n\n实现图\n\n构件图\n\n\n\n\n配置图\n\n\n\n\n\n\n\nCMM: 一级为初始级，二级为可重复级，三级为已定义级，四级为已管理级，五级为优化级\nP179 构建细化\n","categories":["misc"],"tags":["软件工程"]},{"title":"自签名 SSL 证书","url":"/2023/11/21/ssl-certificate/","content":"#成为 CA\n# Generate private keyopenssl genrsa -des3 -out myCA.key 2048# Generate root certificateopenssl req -x509 -new -nodes -key myCA.key -sha256 -days 825 -out myCA.pem \\    -subj &quot;/C=CN/ST=Shanghai/L=Shanghai/O=/OU=/CN=My first CA&quot;\n#信任 CA\nUbuntu:\nsudo cp myCA.pem /usr/local/share/ca-certificates/myCA.crtsudo update-ca-certificates\n#颁发证书\n可以有多个域名/IP 地址\n# Use your own domain nameNAME=my.domain.com# Generate a private keyopenssl genrsa -out $NAME.key 2048# Create a certificate-signing requestopenssl req -new -key $NAME.key -out $NAME.csr \\    -subj &quot;/C=CN/ST=Shanghai/L=Shanghai/O=/OU=/CN=$NAME&quot;# Create a config file for the extensionscat &gt;$NAME.ext &lt;&lt;-EOFauthorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = $NAME # Be sure to include the domain name here because Common Name is not so commonly honoured by itselfDNS.2 = bar.$NAME # Optionally, add additional domains (I&#x27;ve added a subdomain here)IP.1 = 1.2.3.4 # Optionally, add an IP address (if the connection which you have planned requires it)EOF# Create the signed certificateopenssl x509 -req -in $NAME.csr -CA myCA.pem -CAkey myCA.key -CAcreateserial \\    -out $NAME.crt -days 825 -sha256 -extfile $NAME.extcat $NAME.crt $NAME.key &gt;$NAME.pem\n","categories":["misc"]},{"title":"TTS & SVC Survey (For Fun)","url":"/2024/10/05/tts-survey/","content":"#Tacotron2\n\nNatural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, ICASSP 2018\n\n#VITS\n\nConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech, ICML 2021\n文本转语音 (TTS) 任务\n技术: 端到端\n\n#SoftVC / HuBERT / Voice Conversion\n\nA Comparison of Discrete and Soft Speech Units for Improved Voice Conversion, ICASSP 2022\n声音转换 (Voice Conversion) 任务\nHuBERT 模型\n\n#MoeGoe\n\nB 站 UP 主 CjangCjengh 开源 https://github.com/CjangCjengh/MoeGoe\nVITS 工具化\n基于 Tacotron2 版 Demo https://www.bilibili.com/video/BV1rV4y177Z7/\n基于 VITS 版 MoeGoe 工具 https://www.bilibili.com/video/BV1A8411t7sK/\n\n#Fastpitch\n#VITS2\n\nVITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design, Interspeech 2023\n文本转语音 (TTS) 任务\n\n#SoVITS / so-vits-svc 🔥🔥🔥\n\nB 站 UP 主 Rcell 开源 https://github.com/innnky/so-vits-svc (已删)\n歌声转换 (Singing Voice Conversion) 任务\n技术实现: 把 VITS2 中的 text encoder 替换为 SoftVC 中的 HuBERT\n荣誉: 时代周刊评价 2023 最佳发明 https://time.com/collection/best-inventions-2023/6327135/so-vits-svc/\n一些训练好的声音模型: https://huggingface.co/spaces/zomehwh/vits-models/tree/main/pretrained_models\n\n#AI 峰哥\n\n效果演示: https://www.bilibili.com/video/BV1w24y1c7z9/\nIdea: Fastpitch + NLP 模型\nMassTTS: https://github.com/anyvoiceai/MassTTS\nChatGLM-6B: https://github.com/lich99/ChatGLM-finetune-LoRA\n\n#BERT-VITS2\n\nB 站 fishaudio 开源 https://www.bilibili.com/video/BV18E421371Q/ https://github.com/fishaudio/Bert-VITS2\n启发自 AI 峰哥\n技术实现: 把 VITS2 中的 text encoder 替换为 BERT\n\n#OpenVoice\n\n即时语音转换 (Immediate Voice Conversion, Zero-shot TTS) 任务\n\n#RVC\n\nB 站 UP 主 花儿不哭 开源 https://www.bilibili.com/video/BV1pm4y1z7Gm/\n声音转换 (Voice Conversion) 任务\n10 分钟样本\n\n#GPT-SoVITS\n\nB 站 UP 主 花儿不哭 开源 https://www.bilibili.com/video/BV12g4y1m7Uw/\n声音转换 (Voice Conversion) 任务\nRVC 延续, 只需 5 ~ 10s 样本\n\n#fish-speech\n\nhttps://github.com/fishaudio/fish-speech\nZero-shot TTS 任务\n基于自回归模型\n一段 5s 音频样本即可生成语音\n\n#ChatTTS\n\nAI 峰哥新版本 https://github.com/2noise/ChatTTS\nTTS 任务, 对话场景, 支持细粒度控制和韵律\n支持通过 LoRA 微调实现不同音色\n\n","categories":["misc"]},{"title":"网站内容优化","url":"/2024/04/25/website-optimization/","content":"今天用 lighthouse 分析博客的性能，发现几个之前不了解的问题，记录一下。\n#避免在主线程跑复杂 JS\nlive2d.js 很耗时\n#内嵌关键 JS/CSS，延迟非关键 JS/CSS\n#使用 WebP 格式的图片\n$ cwebp -q 50 images/flower1.jpg -o images/flower1.webp\n50 表示质量，0 是最差，100 是最佳。\n兼容方案：向新版浏览器提供 WebP 内容，同时保留对旧版浏览器的支持。\n&lt;picture&gt;  &lt;source type=&quot;image/webp&quot; srcset=&quot;images/flower1.webp&quot; /&gt;  &lt;source type=&quot;image/jpeg&quot; srcset=&quot;images/flower1.jpg&quot; /&gt;  &lt;img src=&quot;images/flower1.jpg&quot; /&gt;&lt;/picture&gt;\n#字体设置\n有些浏览器在字体文件加载完成之前会隐藏文本。\n字体 css 的font-display属性设置此行为：\n\nauto: 字体显示策略由用户代理定义。\nblock: 为字体提供一个短暂的阻塞周期和无限的交换周期。\nswap: 为字体提供一个非常小的阻塞周期和无限的交换周期。\nfallback: 为字体提供一个非常小的阻塞周期和短暂的交换周期。\noptional: 为字体提供一个非常小的阻塞周期，并且没有交换周期。\n\n讲人话就是，后三个选项明确指示浏览器使用 fallback 字体。\n@font-face &#123;  font-family: &quot;Pacifico&quot;;  font-style: normal;  font-weight: 400;  src: local(&quot;Pacifico Regular&quot;), local(&quot;Pacifico-Regular&quot;),    url(https://fonts.gstatic.com/s/pacifico/v12/FwZY7-Qmy14u9lezJ-6H6MmBp0u-.woff2)      format(&quot;woff2&quot;);  font-display: swap;&#125;\n","categories":["misc"],"tags":["SEO","前端"]},{"title":"关于DDL的疑惑","url":"/2021/01/05/what-ddl/","content":"为什么 ddl 推迟, 开始肝 ddl 的时间也跟着推迟？\n","categories":["misc"],"tags":["摸鱼"]},{"title":"VirtIO 规范","url":"/2024/04/16/virtio-note/","content":"VirtIO 是一种通用的半虚拟化的解决方案，它定义了一组规范：只要 Guest 和 Host 按照此规范进行数据操作，就可以使虚拟机绕过内核空间而直接和用户空间的进程间通信，以此达到提高 IO 性能的目的。\n众所周知，真实硬件具有复杂的细节，模拟起来不仅复杂且效率低，当一个 Guest 访问到完全虚拟设备的寄存器 or 内存时，会 trap 到 Hypervisor 的设备模拟代码中，这一过程需要虚拟机内、外的多次切换过程，效率很低。\nVirtIO 极大地简化了设备的实现，当我们只需要最基本的内外传输的功能时，VirtIO 是很好的选择。\nVirtIO 属于半虚拟化的一种，即 Guest 知道自己是虚拟出来的。\nVirtIO 规范目前由 OASIS 维护，目前已经更新到 virtio v1.3 版本。\nVirtIO 规范中定义了多种实现方式，最常见的是 VirtIO over PCI，也有 VirtIO over MMIO 和 VirtIO over Channel I/O。\n#VirtIO 架构\n\nVirtIO 支持虚拟多种类型的设备，例如块设备(virtio_blk)，网卡设备（virtio_net），SCSI 总线设备(virtio_scsi)等。\n位于 Guest 中的部分（即虚拟出来的设备）称为前端。对于 VirtIO 系列设备，Linux 内核中早已有对应的驱动支持。\n位于 Host 上的部分为后端。源码位于 Hypervisor 中。\n不管是哪种类型的设备，底层的数据通道都是一样的。\n#前端 VirtIO 设备\nstruct virtio_device &#123;    struct list_head vqs; // VQ    u64 features;&#125;;\nfeatures 是 virtio_driver &amp; virtio_device 同时支持的通信特性，也就是前后端最终协商的通信特性。\nstruct virtio_driver &#123;    int (*probe)(struct virtio_device *dev);    int (*scan)(struct virtio_device *dev);    int (*remove)(struct virtio_device *dev);&#125;;\n设备加载和注销等\n#VirtQueue（VQ）\nHost 和 Guest 之前通信的抽象数据通道称为 VirtQueue，具有双向数据收发的能力。\n根据实际设备收发速率，可能存在一个或者多个 VirtQueue。比如 virtio-net 默认创建了两条 VirtQueue。\nVQ 创建于 PCI 设备的probe阶段，由 Guest 侧创建，然后把 VQ 的物理地址传递给后端。\n#virtqueue_add() 发送数据\nVQ 发送函数，负责把一个 SG list 放进 VQ。\n#virtqueue_kick() 通知后端\n#VRing\nVirtQueue 是一个抽象类，其具体实现是 VRing，即 vring_virtqueue。\nVRing 是实际进行数据传输的实现，整体结构包含三部门：Descriptor 表和两个存放Descriptor*的 Ring Buffer。\n两个 Ring 分别是 Avail Ring 和 Used Ring，用于双向传递数据。Avail 用于从内向外，Used 用于从外向内，是站在 Host 角度命名的。\n每个 Descriptor 描述了一块数据区，主要包含 Guest 缓冲区的物理地址和长度。\nHost 会从 Avail Ring 中取 Descriptor，收取 or 填充数据并放入 Used Ring 中。\n#virtio-net\nvirtio-net 把 VQ 进一步封装成send_queue和receive_queue\n特定时机会调用try_fill_recv来向receive_queue中填充空 buffer\n#virtio-blk\n#Virtio 演进\n#Vhost\n开发者发现，原始的数据通路 Guest – virtio – Hypervisor 后端 – Host 系统调用 – Host 内核，最后一步总是要切换到内核态。\n为了省去这个开销，vhost 技术将 Hypervisor 后端数据面放在 Host 内核中，由 Host 的一个内核线程负责处理 IO。\n#Vhost-user\nVhost 需要使用 1:1 的内核线程，不够灵活。\n与 Vhost 相反，Vhost-user 将后端数据面放在了用户态进程(e.g., DPDK，SPDK 的 BlobFS)中。\n","categories":["misc"],"tags":["虚拟化"]},{"title":"A Generic Communication Scheduler for Distributed DNN Training Acceleration 论文阅读","url":"/2025/04/04/a-generic-communication-scheduler/","content":"#1. 引言\n训练 DNN 通常是一个耗时的过程，这主要是由于数据和模型的大小不断增加。为了加速训练，常用的方法是数据并行(Data Parallel)。然而，数据并行的性能提升并非线性增加，主要的原因是通信开销。\n因此，研究者们提出了多种通信加速的方法。例如，使用 RDMA 替换 TCP，使用 Ring-AllReduce 替换 All-Reduce, 使用其他的 MPI 实现或者 NCCL 替换参数服务器等。\n最近，一种新的方法被提出，称为通信调度。它的基本思想是改变不同 DNN 层的传输顺序，以更好地减少通信开销。在不影响计算结果的情况下，提高训练性能。\n在本文中，我们展示了基于优先级的通信调度加上 Tensor 分区的方案。这不仅是理论上（在无系统开销的情况下）的最优策略，也很通用。可以加读大多数流行的深度学习框架，包括同/异步 PS 和 All-Reduce, 不同的网络传输协议（如 TCP 和 RDMA）。\n然而，现有的设计由于以下两个原因，没有按照上述的方式进行调度：\n\n对多种架构的排列组合支持有限: 存在很多不同的深度学习框架和网络协议的组合，现有的调度器通常只支持其中的一部分。例如 P3 是在 MXNet 上修改，TicTac 是基于 TensorFlow 的，要在另一个框架上使用，就需要重新实现。\n对各种系统设置的适应性不强: 例如同一个参数在 PS 架构和 All-Reduce 架构下的最优值是不同的。\n\n#FAQ\n","categories":["mlsys"],"tags":["MLSys","ByteScheduler"]},{"title":"[OSDI'20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters 论文阅读","url":"/2025/04/06/a-unified-architecture-for-accelerating/","content":"#1. 引言\n近年来，为了训练更大的模型，深度学习训练集群在不断发展。这样的集群通常包括 GPU，CPU 和高速互联网络，例如高速以太网或者 Infiniband。\n两个主要的架构是参数服务器（Parameter Server）和All-Reduce。他们都是基于数据并行（Data Parallel）的方法。\n在 All-Reduce 架构中，只涉及 GPU 机器。在参数服务器架构中，CPU 和 GPU 机器都参与训练。\n这两种架构在理论和实践中都有很大的差异。对于仅有 GPU 的集群，All-Reduce 是带宽最优的。然而，随着 CPU 和带宽资源的增加，All-Reduce 的最优性不再成立。\n从理论上讲，使用额外的 CPU 机器来帮助 GPU 机器，参数服务器架构能够取得更好的性能。然而，现有的实现不够令人满意。\n在本文中，作者提出了一种新的架构，称为BytePS，它通过仔细分配流量负载，统一了参数服务器和 All-Reduce 两种架构，能够适用于任意比例的不同 PCIe/NVLink 配置的 CPU/GPU 机器上。\n#2. 背景\n#2.1 分布式 DNN 训练\n#2.2 All-Reduce\n#2.3 参数服务器\n#3. BytePS 架构\n#3.1 动机\n在 BytePS 之前，用户大多使用 All-Reduce 架构来训练 DNN 模型。作者观察到几个现象：\n\n生产 GPU 集群通常有闲置的 CPU 和带宽。\n现有的 All-Reduce 和 PS 实现都没有充分利用 CPU 和带宽。\n\n作者的解决方案是 BytePS，它是一个统一的架构，可以利用空闲的 CPU 和带宽来加速训练。\n#3.2 架构概述\nBytePS 有两个主要模块：通信服务（Communication Service）和汇总服务（Summation Service）。\n汇总服务 SS 运行在所有类型的机器上。\n通信服务 CS 只运行在 GPU 机器上，负责在多个本地 GPU 之间同步 Tensor，并且与 SS 进行通信。\n#4. BytePS 通信设计\n#4.1 机器间通信\n在 BytePS 中，所有通信都通过通信服务（CS）进行。假设网络具有全双工带宽，而且 DCQCN 类似的流量控制算法可以在网络中使用。\nCPU 机器的网络流量等于其上 SS 的总和。GPU 机器的网络流量等于其上 SS 和 CS 的总和。\n为了尽量减少通信时间，BytePS 给每个 CPU 上的 SS 分配 MssCpu 字节的求和负载。给每个 GPU 上的 CS 分配 MssGpu 字节的求和负载。\nMssCpu=2(n−1)n2+kn−2kMMssGpu=(n−k)n2+kn−2kM\\text{MssCpu} = \\frac{2(n-1)}{n^2+kn-2k}M \\\\\n\\text{MssGpu} = \\frac{(n-k)}{n^2+kn-2k}M\nMssCpu=n2+kn−2k2(n−1)​MMssGpu=n2+kn−2k(n−k)​M\n在实践中，DNN 模型的参数大小通常是不固定的。作者将参数分割成不超过 4MB 的小块。然后，所有 CS 将这些小块哈希到[0,n2+kn−2k)[0, n^2+kn-2k)[0,n2+kn−2k)的范围内。根据哈希值，CS 发送和接收对应的数据给 SS。\n#4.2 机器内通信\n#4.2.1 PCIe 机器\n作者生产环境的机器模型是纯 PCIe 拓扑的，每个 CPU 直连 4 块 GPU，两个 CPU 之间通过 QPI 链接。使用现有的框架（例如 TensorFlow PS，MXNet PS，Horovod），在进行 Reduce 或者 Reduce-Scatter 时，会导致跨 PCIe 的内存拷贝，性能很差。\nBytePS 考虑到 PCIe 拓扑的特点，允许同一个 PCIe 下的 GPU 先进行一个求和操作，然后再拷贝到 CPU 进行全局的求和操作，称为 CPU-assisted aggregation。完整步骤如下：\n\nReduce-Scatter\nGPU-&gt;CPU 拷贝\nCPU-Reduce\nNetworking\nCPU-&gt;GPU 拷贝\nAll-Gather\n\n实践结果表明 CPU-assisted aggregation 比单纯的 Ring-AllReduce 要好。\n#4.2.2 NVLink 机器\n作者数据中心的另一种机器模型是带有 NVLink 连接的 GPU 机器。每台机器有 4 个 PCIe Switch，每个 Switch 连接 2 个 GPU，GPU 之间通过 NVLink 连接。NVLink 的带宽远高于 PCIe。\n因为有 NVLink 的存在，GPU 之间的通信不需要占用 PCIe 带宽。然而，由于 NVLink 不是完全对称的，仍然存在 PCIe 带宽的瓶颈。\n#4.2.3 讨论\n以上分析表明，最佳的方案与机器的拓扑结构密切相关。\n尽管存在差异，但作者总结了两个原则：\n\n当两个 GPU 不在同一个 PCIe 交换机下时，始终避免直接的 GPU 到 GPU 内存复制，因为在实践中它很慢。\n尽量减少在由 GPU 和 NIC 共享的 PCIe 交换机到 CPU 链路上的数据流量。\n\n作者提出了以下最佳实践程序。让 Sn 表示具有 GPU 和 NIC 的 PCIe 交换机的数量，Sg 表示仅具有 GPU 的 PCIe 交换机的数量。\n\n如果 Sn &gt; 0，Sg &gt; 0，CS 应该使用 Reduce 和 Broadcast，就不会竞争网卡带宽。\n如果 Sn = 0 或者 Sg = 0，CS 应该使用 Reduce-Scatter 和 All-Gather 来平衡 PCIe 交换机上的流量。\n\n多网卡情况同理。\nGDR 技术(GPU-direct RDMA)可以减少 PCIe 流量，然而需要 GPU 和 RDMA 网卡在同一个 PCIe 交换机上。因此没有什么好处。\n构建一个分析器来自动选择最佳的通信模式是一个有趣的研究方向。\n#5. 汇总服务 SS\nCPU 的内存带宽在某些时候可能会成为瓶颈。\n作者提出了一个新的解决方案，将计算密集型的汇总操作分配给 GPU，并仅在 CPU 上执行 CPU 擅长的汇总操作。\n虽然汇总操作提升了训练性能，但是打破了 PS 的异步特性。为此，作者提出了一个新的异步汇总算法。\n","categories":["mlsys"],"tags":["MLSys","Parameter Server","BytePS"]},{"title":"[Arxiv'18] Horovod: fast and easy distributed deep learning in TensorFlow 论文阅读","url":"/2025/04/04/horovod-fast-and-easy-distributed/","content":"Horovod 是 Uber 于 2017 年发布的一个易于使用的高性能的分布式深度训练框架，支持 TensorFlow，Keras，PyTorch 和 MXNet。\n（博主注: 2017 年 Google 刚刚提出 Transformer，2018 年 BERT 发布，2019 年 GPT-2 发布，2020 年 GPT-3 发布。回头看这篇 2017 年的论文仅供学习。）\n#1. 引言\n近年来，深度学习蓬勃发展。User 公司选择了 TensorFlow 作为其深度学习框架。原因是:\n\n广泛使用，用户技术大，新用户容易上手\n具有高性能，支持对底层细节的控制: 例如有 Keras 这样的高级 API，也可以通过 CUDA 实现自定义的算子\n支持广泛的使用场景: 从云服务平台到端侧设备\n\n2017 年 Uber 团队提出了 Michelangelo 机器学习平台，本文介绍了其中一个开源组件，Horovod，开源地址在 https://github.com/uber/horovod。\n#2. 开始分布式\n当 Uber 内部的深度学习模型变得越来越多时，模型的大小和数据消耗显著增加。在大多数情况下，单台机器（单卡或者多卡）可以满足需求。但是训练耗时很长，有时需要一周甚至更长的时间。为了解决这个问题，Uber 开始探索分布式训练。\nUber 团队首先使用了标准的 TensorFlow 分布式训练 API。发现两个明显的问题:\n\n用户不知道如何使用: 做深度学习的人往往不懂底层概念, 不清楚该如何修改自己的代码，Debug 也很麻烦\n可伸缩性: Uber 的服务规模过大，TensorFlow 在这种情况下的性能不佳。例如在 128 卡跑 TF 的 benchmark 时，利用率不到 50%。\n\n#3. 使用不同的算法\nUber 团队尝试了 Facebook Accurate, large minibatch SGD: Training ImageNet in 1 hour 论文中的数据并行(Data Parallel)方法。\n从概念上说，DP 方法很简单：每个训练脚本副本读取一个数据库，进行 forward 之后计算梯度，多个副本之间平均梯度，然后更新参数。\n标准的 TensorFlow 分布式训练 API 使用了 Parameter Server 方法来平均梯度。虽然能够提高性能，但是遇到两个挑战：\n\n难以确定计算节点和服务节点的比例\n增加应用程序的复杂性\n\n2017 年初百度开源了一个基于 Ring-AllReduce 方法的 TensorFlow Fork。\n在 Ring-AllReduce 方法中，每个节点与其他两个相邻的节点交换梯度，经过多次迭代后，所有节点都拥有相同的梯度。该算法能更好地利用网络带宽。\n此外，这种方法也更容易理解，用户使用 MPI API 进行编程，使用mpirun启动训练，修改也更简单。\n#4. Horovod\nUber 团队在百度的 Fork 的基础上构建了 Horovod。\n\n相关代码被放进了一个单独的 Python 包中，方便安装和使用\n使用 NCCL 替换了百度的 Ring-AllReduce 实现\n增加了对单机器多卡的支持，之前只支持单卡\n根据用户反馈进行改进，增加了全局同步的广播操作。用户只需要增加 4 个 API 即可。\n\n#5. 如何使用\nimport tensorflow as tfimport horovod.tensorflow as hvd# Initialize Horovodhvd.init()# Pin GPU to be used to process local rank(one GPU per process)config = tf.ConfigProto()config.gpu_options.visible_device_list = str(hvd.local_rank())# Build model...loss = ...opt = tf.train.AdagradOptimizer(0.01)# Add Horovod Distributed Optimizeropt = hvd.DistributedOptimizer(opt)# Add hook to broadcast variables from rank 0 to all other processes# during initialization.hooks = [hvd.BroadcastGlobalVariablesHook(0)]# Make training operationtrain_op = opt.minimize(loss)# The MonitoredTrainingSession takes care of session initialization,# restoring from a checkpoint, saving to a checkpoint, and closing# when done or an error occurs.with tf.train.MonitoredTrainingSession(checkpoint_dir=&quot;/tmp/train_logs&quot;, config=config,hooks=hooks) as mon_sess:    while not mon_sess.should_stop():        # Perform synchronous training.        mon_sess.run(train_op)\n用户可以使用\n$ mpirun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py...\n来启动训练。-np 16表示使用 16 个进程，-H表示主机列表。\n#6. Horovod Timeline\nHorovod Timeline 是一个可视化工具，可以帮助用户分析训练过程中的性能瓶颈。\n#7. Tensor 聚合\n在调用 Ring-AllReduce 之前，Horovod 可以对 Tensor 进行聚合以可以减少通信开销。\n#8. 评估\n使用 Horovod 运行 Tensorflow 的 Benchmark，训练速度能够达到 Tensorflow 的 2 倍。\n由于 MPI 和 NCCL 都支持 RDMA，作者也测试了使用 RDMA 的性能。在某些情况下能够提升。\n#FAQ\n#1. NCCL 和 MPI 是什么？\nNCCL（NVIDIA Collective Communications Library）是 NVIDIA 针对 GPU 设计的一种规约库，可以实现多 GPU 间的直接数据同步，避免内存和显存的，CPU 和 GPU 间的数据拷贝成本。\nMPI（Massage Passing Interface）是消息传递函数库的标准规范，主要是被应用在科学计算，尤其是超算领域。由于容错性一般，故在机器学习场景下使用较少。\n","categories":["mlsys"],"tags":["MLSys","horovod","tensorflow"]},{"title":"[NeurIPS'12] Large Scale Distributed Deep Networks (DistBelief) 阅读笔记","url":"/2026/02/08/large-scale-distributed-deep-networks/","content":"#0. 摘要\n近期在无监督特征学习和深度学习领域的研究表明, 能够训练大型模型可以显著提升性能.\n本文考虑了使用数万个 CPU 核心训练具有数十亿参数的深度网络的问题.\n我们开发了一个名为 DistBelief 的软件框架, 该框架能够利用包含数千台机器的计算集群来训练大型模型.\n在该框架中, 我们开发了两种用于大规模分布式训练的算法:\n\nDownpour SGD, 一种支持大量模型副本的异步 SGD 过程\nSandblaster, 一个支持多种分布式批量优化过程的框架, 包括 L-BFGS 的分布式实现.\n\nDownpour SGD 和 Sandblaster L-BFGS 均提高了深度网络训练的规模和速度.\n我们成功使用该系统训练了一个比文献中先前报道的大 30 倍的深度网络, 并在 ImageNet 上取得了最先进的性能, 这是一个包含 1600 万张图像和 21k 类别的视觉对象识别任务.\n我们证明了这些相同的技术能够显著加速对商业语音识别服务所用规模较小的深度网络的训练.\n尽管我们专注于并报告了这些方法在训练大型神经网络时的性能, 但底层算法适用于任何基于梯度的机器学习算法.\n#1. 引言\n深度学习和无监督特征学习在许多实际应用中已展现出巨大的潜力.\n在语音识别, 视觉物体识别以及文本处理等多个领域, 已报道了最先进的性能.\n人们还观察到, 增加深度学习的规模, 无论是增加训练样本数量, 模型参数数量, 还是两者同时增加, 都可以显著提高最终的分类精度.\n这些结果导致了对用于这些模型的训练和推理算法的扩展产生了浓厚的兴趣, 以及对适用优化程序的改进.\n近年来, 使用 GPU 是一项重大进步, 使得中等规模的深度网络训练变得可行.\nGPU 方法的一个已知局限性是, 当模型不适合 GPU 内存时 (通常小于 6GB), 训练加速效果很小.\n为了有效使用 GPU, 研究人员通常会减小数据或参数的大小, 以避免 CPU 到 GPU 的传输成为显著瓶颈.\n虽然数据和参数的减小对于小问题 (例如语音识别的声学建模) 效果很好, 但对于具有大量样本和维度的复杂问题 (例如高分辨率图像), 这种方法就不那么吸引人了.\n在本文中, 我们描述了一种替代方法: 利用大规模机器集群来分布式地执行深度网络的训练和推理.\n我们开发了一个名为 DistBelief 的软件框架, 该框架能够在单台机器内部实现 MP (通过多线程), 以及跨多台机器实现 MP (通过消息传递), 其中并行性, 同步和通信的细节由框架管理.\n除了支持 MP , DistBelief 框架还支持数据并行, 其中使用多个模型副本来优化单个目标.\n在该框架中, 我们设计和实现了两种用于大规模分布式训练的新方法:\n\nDownpour SGD: 一种异步 SGD 程序, 利用自适应学习率并支持大量模型副本\nSandblaster L-BFGS: 一种使用数据和 MP 的 L-BFGS 分布式实现.\n\n与更传统的 SGD 和 L-BFGS 实现相比, Downpour SGD 和 Sandblaster L-BFGS 都实现了显著的加速.\n我们的实验揭示了关于大规模非凸优化的几个令人惊讶的结果.\n首先, 异步 SGD 很少应用于非凸问题, 但在训练深度网络时表现非常好, 特别是与 Adagrad 自适应学习率结合使用时.\n其次, 我们表明, 在资源充足的情况下, L-BFGS 与许多 SGD 变体相比具有竞争力或更快.\n关于深度学习的具体应用, 我们报告了两个主要发现: 我们的分布式优化方法可以大大加速中等规模模型的训练, 并且还可以训练比以往可考虑的更大的模型.\n为了说明第一个点, 我们展示了如何使用一组机器在不到 GPU 所需时间的 1/10 来训练一个中等规模的语音模型, 达到相同的分类精度.\n为了说明第二个点, 我们训练了一个包含超过 10 亿参数的大神经网络, 并使用该网络在 ImageNet 数据集上大幅提高了最先进的性能, 这是计算机视觉中最大的数据集之一.\n#2. 前期工作\n近年来, 商业和学术机器学习数据集以前所未有的速度增长.\n为此, 许多作者通过并行化和分布式方式探索了机器学习算法的扩展.\n这些研究中的很大一部分集中在线性, 凸模型上, 其中分布式梯度计算是自然的第一步.\n在这一领域内, 一些团队放宽了同步要求, 探索了凸问题的延迟梯度更新.\n与此同时, 其他研究稀疏梯度 (即对于任何给定的训练样本, 梯度向量的坐标只有极小部分非零) 问题的团队探索了在共享内存架构 (即单机) 上的无锁异步 SGD.\n我们感兴趣的是一种能够兼顾两者的方法, 允许使用一组机器异步计算梯度, 但无需要求问题是凸的或稀疏的.\n在深度学习的背景下, 大多数工作集中于在单台机器上训练相对较小的模型 (例如 Theano).\n关于扩展深度学习的建议包括使用 GPU 农场来训练大量小模型的集合, 并随后对其预测进行平均, 或修改标准深度网络以使其本质上更具并行性.\n我们的重点是扩展深度学习技术, 朝着训练具有数十亿参数的超大型模型的方向发展, 但不对模型的形态施加限制.\n在某个层主导计算的特殊情况下, 一些作者考虑在该层中分布计算, 并在其余层中复制计算.\n但在模型的多层计算密集的一般情况下, 需要类似于[22]的完整 MP.\n然而, 要取得成功, 我们相信 MP 必须与巧妙的分布式优化技术相结合, 这些技术利用数据并行性.\n我们考虑了若干现有的用于解决我们问题的计算工具, 其中 MapReduce 和 GraphLab 是值得注意的例子.\n我们得出结论, MapReduce 是为并行数据处理而设计的, 不适合深度网络训练中固有的迭代计算;\n而 GraphLab 是为通用 (非结构化) 图计算而设计的, 不会利用深度网络中通常存在的结构化图中的计算效率.\n#3. 模型并行 (MP)\n为了促进超大型深度网络的训练, 我们开发了一个软件框架 DistBelief, 该框架支持神经网络和分层图模型的分布式计算.\n用户定义模型每层每个节点的计算过程, 以及计算的上行和下行阶段应传递的消息.\n对于大型模型, 用户可以将模型划分到多台机器上 (图 1), 以便将不同节点的计算责任分配到不同的机器.\n该框架使用所有可用的核心自动并行化每台机器上的计算, 并在训练和推理过程中管理机器间的通信, 同步和数据传输.\n\n图 1: DistBelief 中 MP 的示例. 这里展示了一个具有局部连接的五层深度神经网络, 被划分到四台机器 (蓝色矩形) 上.\n只有那些边跨越分区边界 (粗线) 的节点需要在机器之间传输其状态.\n即使一个节点有多个边跨越分区边界, 其状态也只会在该边界另一侧的机器上发送一次.\n在每个分区内部, 单个节点的计算将在所有可用的 CPU 核心上并行化.\n将深度网络分布到多台机器上的性能优势取决于模型的连接结构和算力需求.\n参数数量较多或算力需求较高的模型通常能从更多的 CPU 和内存访问中获益, 直到通信成本成为主导因素.\n\n\n当计算开销远大于通信开销时, 集中式架构更高效, 因为避免了分布式带来的网络延迟、同步和数据传输成本.\n当通信开销成为主要瓶颈 (即计算相对较轻) 时, 分布式架构更有优势, 它可以将计算任务分发到数据所在地, 减少大规模数据移动.\n\n\n我们在 DistBelief 框架中成功运行了包含高达 144 个分区的模型, 并实现了显著的加速,\n而规模较小的模型在最多 8 个或 16 个分区的情况下也表现出不错的加速效果 (参见第 5 节, 标题为 “MP 基准测试” 的实验结果).\n显然, 具有局部连接结构的模型比全连接结构更易于进行广泛分布, 因为它们的通信需求较低.\n速度提升不理想的主要原因是 不同机器之间处理时间的差异, 导致许多机器需要等待速度最慢的机器完成计算的一定阶段.\n然而, 对于我们最大的模型, 我们可以高效地使用 32 台机器, 每台机器实现 16 核的平均 CPU 利用率, 总计 512 个 CPU 核心来训练单个大型神经网络.\n当与下一节中描述的分布式优化算法结合时, 这些算法利用整个神经网络的多个副本, 可以用于训练单个模型的上万 CPU 核心, 从而显著减少整体训练时间.\n#4. 分布式优化算法\n在 DistBelief 框架内并行化计算使我们能够实例化和运行比以往报告的规模大得多的神经网络.\n但为了在合理的时间内训练此类大型模型, 我们不仅需要在模型的一个实例 内部, 还要将训练分布到 多个 模型实例上.\n在本节中, 我们描述了这种第二级的并行性, 我们采用一组 DistBelief 模型实例 (副本), 来同时解决一个单一的优化问题.\n我们比较了两种大规模分布式优化程序: Downpour SGD, 一种在线方法, 和 Sandblaster L-BFGS, 一种批量方法.\n这两种方法都利用了集中式分片 PS 的概念, 模型副本使用它来共享它们的参数.\n这两种方法都利用了 DistBelief 在每个单独副本中允许的分布式计算.\n但最重要的是, 这两种方法都被设计成能够容忍不同模型副本处理速度的差异, 甚至模型副本的整批失败, 这些副本可能会被离线或随机重启.\n从某种意义上说, 这两种优化算法实现了数据并行性的一种智能版本.\n这两种方法都允许我们在许多模型副本中的每一个同时处理不同的训练示例, 并定期结合它们的结果来优化我们的目标函数.\n#4.1. Downpour SGD\nSGD 或许是训练深度神经网络最常用的优化方法.\n不幸的是, SGD 的传统公式是串行的, 这使得它不适用于非常大的数据集, 因为在完全串行方式下遍历数据所需的时间是难以承受的.\n为了将 SGD 应用于大数据集, 我们引入了 Downpour SGD, 这是一种异步 SGD 的变体, 它使用单个 DistBelief 模型的多个副本.\n基本方法如下: 我们将训练数据划分为多个子集, 并在每个子集上运行模型的一个副本 (DP).\n这些模型通过一个中央 PS进行通信, 该服务器保存模型所有参数的当前状态, 并跨多台机器分片 (例如, 如果我们有 10 个 PS 分片, 每个分片负责存储和应用于模型参数的 1/10) (图 2).\n这种方法在两个方面是异步的: 模型副本相互独立运行, PS 分片也相互独立运行.\n\n图 2: 左: Downpour SGD. 模型副本异步获取参数 w 并将梯度 ∆w 推送到 PS .\n右: Sandblaster L-BFGS. 单个 Coordinator 向副本和 PS 发送小消息以协调批量优化.\n在最简单的实现中, 在处理每个 mini-batch 之前, 模型副本会向 PS 服务请求其模型参数的更新副本.\n由于 DistBelief 模型本身是 MP 的, 每台机器只需要与持有与其分区相关的模型参数的 PS 分片进行通信.\n在接收到其参数的更新副本后, DistBelief 模型副本处理一个 mini-batch 的数据以计算参数梯度, 并将梯度发送到 PS , PS 随后将梯度应用于模型参数的当前值.\n可以通过限制每个模型副本仅在每 n_fetch 个 step 请求一次更新参数, 且仅在每 n_push 个 step 发送一次更新梯度值, 来减少 Downpour SGD 的通信开销 (其中 n_fetch 可能不等于 n_push).\n事实上, 获取参数, 梯度推送以及训练数据处理可以在仅弱同步的三个线程中执行 (参见附录中的伪代码).\n在以下实验中, 我们为简单起见并便于与传统 SGD 进行比较, 固定 n_fetch = n_push = 1.\nDownpour SGD 比标准同步 SGD 更鲁棒于机器故障.\n对于同步 SGD, 如果一台机器宕机, 整个训练过程将被延迟; 而对于异步 SGD, 如果模型副本中的一台机器宕机, 其他模型副本会继续处理其训练数据并通过 PS 更新模型参数.\n另一方面, Downpour SGD 中的多种异步处理形式在优化过程中引入了大量额外的随机性:\n\n模型副本几乎肯定会基于一组略微过时的参数来计算其梯度, 因为在此期间, 其他模型副本很可能已在 PS 上更新了参数.\n由于 PS 分片独立工作, 无法保证在任何给定时刻 PS 每个分片上的参数都经过了相同数量的更新, 或者更新是按相同的顺序应用的.\n由于模型副本被允许在不同的线程中获取参数和推送梯度, 参数的时间戳可能存在额外的细微不一致性.\n\n对于非凸问题, 这些操作的安全性缺乏理论基础, 但在实践中我们发现放宽一致性要求非常有效.\n我们发现的能够显著提高 Downpour SGD 鲁棒性的技术之一是使用 Adagrad 自适应学习率方法.\nAdagrad 不是在 PS 上使用单一固定的学习率 (图 2 中的 η\\etaη), 而是为每个参数使用独立的自适应学习率.\n设 ηi,K\\eta_{i,K}ηi,K​ 为第 iii 个参数在迭代 KKK 时的学习率, Δwi,K\\Delta{w}_{i, K}Δwi,K​ 为其梯度,\n然后我们设置 ηi,K=γ∑j=1KΔwi,K2\\eta_{i,K} = \\frac{\\gamma}{\\sqrt{\\sum_{j=1}^{K}{ \\Delta{w}_{i, K}^2 }}}ηi,K​=∑j=1K​Δwi,K2​​γ​.\n由于这些学习率仅根据每个参数的平方梯度之和计算得出, Adagrad 可以很容易地在每个 PS 分片中本地实现.\nη\\etaη 的值, 即所有学习率的常数缩放因子, 通常比不使用 Adagrad 时使用的最佳固定学习率要大 (可能大一个数量级).\n使用 Adagrad 扩展了可以同时有效工作的模型副本的最大数量, 并且结合使用单个模型副本 “预热” 模型训练再释放其他副本的实践, 几乎消除了使用 Downpour SGD 训练深度网络时的稳定性问题 (参见第 5 节的结果).\n#4.2. Sandblaster L-BFGS\n批量方法已被证明在训练小型深度网络时效果良好.\n为了将这些方法应用于大型模型和大型数据集, 我们引入了 Sandblaster 批量优化框架, 并讨论了使用该框架实现的 L-BFGS 方法.\nSandblaster 的一个关键思想是 分布式参数存储和操作.\n优化算法的核心 (例如 L-BFGS) 位于协调器进程中 (图 2), 该进程无法直接访问模型参数.\n相反, 协调器会发出来自一小组操作 (例如点积, 缩放, 系数相加, 乘法) 的命令, 这些操作可以被每个 PS 分片独立执行, 并将结果存储在同一个分片中.\n额外的信息 (例如 L-BFGS 的历史缓存) 也存储在计算它的 PS 分片中.\n这使得可以运行大型模型 (数十亿参数), 而无需承担将所有参数和梯度发送到单个中央服务器的开销 (参见附录中的伪代码).\n在典型的 L-BFGS 并行化实现中, 数据被分发到多台机器上, 每台机器负责计算特定数据子集上的梯度.\n梯度被发送回中央服务器 (或通过树形结构聚合).\n许多此类方法等待速度最慢的机器, 因此无法很好地扩展到大型共享集群.\n为了解决这个问题, 我们采用了以下负载均衡方案: 协调器为每个 N 个模型副本分配一小部分工作, 远小于批量总大小的 1/N, 并在副本空闲时分配新的部分.\n通过这种方法, 速度更快的模型副本比速度较慢的副本承担更多的工作.\n为了进一步管理批量结束时的速度较慢的模型副本, 协调器安排多个副本处理未完成的任务, 并使用第一个完成模型副本的结果.\n该方案类似于 MapReduce 框架中使用的 “备份任务”.\n数据预取, 以及通过分配连续数据来支持数据亲和性,\n协调器安排多个副本处理未完成的任务, 并使用最先完成数据部分的模型副本结果, 将数据发送给同一工作节点, 使数据访问成为非问题.\n与需要相对高频, 高带宽参数同步到 PS 的 Downpour SGD 相比, Sandblaster 工作节点仅在每批数据的开始时 (由协调器更新后)获取参数, 并且每隔几个完成的任务部分才发送梯度 (以防止副本故障和重启).\n#5. 实验\n我们通过将优化算法应用于两个不同深度学习问题的训练模型来评估我们的优化算法: 静态图像中的目标识别和用于语音识别的声学处理.\n语音识别任务是将短音频片段中的中心区域 (或帧) 分类为数千种声学状态之一.\n我们使用了一个具有五层的深度网络: 四个具有 Sigmoid 激活函数的隐藏层, 每层有 2560 个节点, 以及一个具有 8192 个节点的 softmax 输出层.\n输入表示是 11 个连续重叠的 25 毫秒语音帧, 每帧由 40 个对数能量值表示.\n网络是逐层全连接的, 总共大约有 4200 万个模型参数.\n我们在一个包含 11 亿个弱标签样本的数据集上训练, 并在一个保留的测试集上进行评估.\n参见[27]了解类似的深度网络配置和训练过程.\n在视觉物体识别方面, 我们在 ImageNet 数据集 (包含 1600 万张图像)上训练了一个具有局部连接感受野的大规模神经网络, 并将每张图像缩放为 100x100 像素.\n该网络包含三个阶段, 每个阶段由滤波, 池化和局部对比度归一化组成, 其中滤波层中的每个节点都与下层的一个 10x10 图像块相连.\n我们的基础设施允许多个节点连接到同一个输入图像块, 我们进行了实验, 改变了相同连接的节点数量, 从 8 到 36 不等.\n输出层由 21001 个一对一逻辑分类器节点组成, 每个节点对应 ImageNet 中的一个物体类别.\n有关类似的深度网络配置和训练过程, 请参见[29].\nMP 性基准测试: 为了探索 DistBelief MP 的扩展行为 (第 3 节), 我们测量了在单个模型实例中使用不同分区 (机器) 数量时, 对单个 mini-batch 进行简单 SGD 训练的平均处理时间.\n在图 3 中, 我们通过报告平均训练加速比来量化跨 N 台机器并行化的影响: 使用单台机器所需时间与使用 N 台机器所需时间的比值.\n这些模型中的推理步骤加速比相似, 此处未显示.\n\n图 3: 四个不同深度网络的训练加速 随 分配给单个 DistBelief 模型实例的机器数量的函数.\n参数更多的模型从使用额外机器中受益更多, 而参数较少的模型则受益较少.\n中等规模的语音模型在 8 台机器上运行速度最快, 比单台机器计算速度快 2.2 倍 (模型配置为每台机器最多使用 20 个核心).\n在超过 8 台机器上分区运行模型实际上会减慢训练, 因为网络开销在完全连接的网络结构中开始占主导地位, 并且随着分区增多, 每台机器的工作量减少.\n相比之下, 规模更大且本地连接的图像模型可以从每个模型副本使用更多机器中获益.\n参数量最大的模型 (1.7 亿个参数) 获益最大, 使用 81 台机器时, 速度提升超过 12 倍.\n对于这些大型模型, 使用更多机器可以继续提高速度, 但收益递减.\n优化方法比较: 为了评估所提出的分布式优化程序, 我们以多种配置运行了上述语音模型.\n我们考虑两种基准优化程序: 使用传统 (单副本)SGD 在 8 个分区上训练 DistBelief 模型, 以及使用 CUDA 在 GPU 上训练相同模型.\n我们与这些基准方法进行比较的三种分布式优化方法是: 固定学习率的 Downpour SGD, Adagrad 学习率的 Downpour SGD 以及 Sandblaster L-BFGS.\n\n图 4: 左: 不同优化方法在训练集部分数据上的训练准确率.\n右: 保留测试集上的分类准确率随训练时间的变化.\nDownpour 和 Sandblaster 实验使用相同的约 10 小时简单 SGD 预热初始化.\n图 4 展示了这些方法在训练集和测试集上, 分类性能随训练时间的变化情况.\n我们的目标是在最短的训练时间内获得最大的测试集准确率, 而不管资源需求如何.\n传统的单副本 SGD (黑色曲线)训练速度最慢.\n带有 20 个模型副本的 Downpour SGD (蓝色曲线)显示出显著改进.\n带有 20 个副本和 Adagrad 的 Downpour SGD (橙色曲线)速度稍快.\n使用 2000 个模型副本的 Sandblaster L-BFGS (绿色曲线)则更快得多.\n然而, 最快的是带有 200 个模型副本的 Downpour SGD 加 Adagrad (红色曲线).\n如果能够获得充足的 CPU 资源, Sandblaster L-BFGS 和带有 Adagrad 的 Downpour SGD 训练模型的速度将显著快于高性能 GPU.\n尽管我们没有将上述实验限制在固定的资源预算内, 但考虑各种方法如何在性能和资源消耗之间进行权衡是很有趣的.\n我们通过任意选择一个固定的测试集准确率 (16%), 并测量每种方法达到该准确率所需的时间, 以机器数量和使用的 CPU 核心数为函数进行分析, 如图 5 所示.\n每条轨迹上的四个点中, 有一个点对应于图 4 中所示的训练配置, 其余三个点是其他配置.\n\n图 5: 不同优化策略达到固定准确率 (16%) 所需时间随机器数量 (左) 和核心数量 (右) 的变化.\n在这个图中, 越靠近原点越好, 因为它们在消耗更少资源的同时花费了更少的时间.\n在这方面, 使用 Adagrad 的 Downpour SGD 似乎是最佳权衡:\n对于任何固定的机器或核心预算, 使用 Adagrad 的 Downpour SGD 比使用固定学习率的 Downpour SGD 或 Sandblaster L-BFGS 更快地达到准确率目标.\n对于任何分配的训练时间来达到准确率目标, 使用 Adagrad 的 Downpour SGD 使用的资源更少, 而且在许多情况下, 使用固定学习率的 Downpour SGD 甚至无法在截止日期前达到目标.\nSandblaster L-BFGS 系统在方面确实显示出前景.\n与固定学习率的 Downpour SGD 或 Sandblaster L-BFGS 相比, 带有 Adagrad 的 Downpour SGD 达到准确率目标所需时间更短.\n由于其在增加核心数量时的扩展性, 这表明如果使用极其庞大的资源预算 (例如 30k 个核心), 它最终可能产生最快的训练时间.\n应用于 ImageNet: 之前的实验表明, 我们的技术能够加速具有数百万参数的神经网络的训练.\n然而, 我们基于集群的分布式优化方法更显著的优势在于其能够扩展到远超单台机器舒适容量的大型模型, 更不用说单块 GPU 了.\n作为探索超大型神经网络能力的第一步, 我们使用 Downpour SGD 在 ImageNet 目标分类任务上训练了上述 1.7 亿参数的图像模型.\n如[29]中详细所述, 该网络实现了超过 15% 的交叉验证分类准确率, 相较于我们所知的 21k 类别 ImageNet 分类任务的最佳性能, 相对提升了 60%.\n#6. 结论\n在本文中, 我们介绍了 DistBelief, 一个用于深度网络并行分布式训练的框架.\n在该框架内, 我们发现了多种有效的分布式优化策略.\n我们发现, Downpour SGD 作为 SGD 的一种高度异步变体, 在训练非凸深度学习模型方面表现异常出色.\nSandblaster L-BFGS 作为 L-BFGS 的分布式实现, 可以与 SGD 相媲美, 其更高效的网络带宽利用使其能够扩展到更多并发核心来训练单个模型.\n综上所述, 当计算预算不超过 2000 个 CPU 核心时, Downpour SGD 与 Adagrad 自适应学习率过程的组合成为明显占优的方法.\nAdagrad 原本并非设计用于与异步 SGD 一起使用, 而且这两种方法通常也不应用于非凸问题.\n因此, 它们能如此出色地协同工作, 并在高度非线性的深度网络中表现良好, 这令人惊讶.\n我们推测, Adagrad 在面对大量异步更新时能自动稳定波动参数, 并自然地根据深度网络中不同层的需求调整学习率.\n我们的实验表明, 我们新的大规模训练方法可以利用一组机器显著快于 GPU 地训练规模适中的深度网络, 并且不受 GPU 对模型最大规模的限制.\n为了展示能够训练更大模型的价值, 我们已经训练了一个超过 10 亿参数的模型, 在 ImageNet 目标识别挑战中取得了优于当前最佳性能的结果.\n","categories":["mlsys"],"tags":["MLSys","Distributed Machine Learning","System"]},{"title":"[Arxiv'19] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism 论文阅读","url":"/2025/04/05/megatron-lm-training-multi-billion-parameter/","content":"NVIDIA 出品的大模型训练方法，2019 年挂在 ArXiV 上，到现在(2025 年)有 2k+ 论文引用。\n#1. 引言\n得益于计算能力和数据集大小的增长，NLP 领域取得了飞速的进展。\n随着这些模型变得越来越大，它们超出了现代处理器的内存限制，需要额外的内存管理技术，例如 Activation Checkpointing。广泛使用的优化算法，如 Adam，需要每个参数存储额外的信息例如动量和其他优化器状态，这降低了可以训练的模型的大小。几种模型并行方法通过按比例划分模型参数来解决这个问题，使得权重和优化器状态不需要同时存在于处理器上。例如，GPipe 和 Mesh TensorFlow 提供了各种模型并行化框架。然而，这些方法需要重写模型，并且依赖于自定义的编译器和框架。\n在这项工作中，作者实现了一个简单而有效的模型并行方法，使用层内模型并行。作者利用了 Transformer 架构的内存结构来实现简单的模型并行实现，可以高效地在 PyTorch 中进行训练，无需自定义 C++代码或者编译器。这种方法于 GPipe 等方法所倡导的基于管道的模型并行方法正交。\n#2. 背景与挑战\n#2.1. 神经语言模型预训练\n预训练语言模型已经是 NLP 中不可或缺的组成部分。这些方法的进步需要高效的硬件、系统技术和框架。本文的工作旨在提供必要的工具，以便在这个趋势中迈出一步。\n#2.2 Transformer 语言模型和多头注意力机制\n由于其优越的准确性和计算效率，当前的 NLP 趋势是使用 Transformer 模型。原始的 Transformer 模型是一种机器翻译架构，分为编码器和解码器两个部分。然而，最近（2019 年）的工作，如 BERT 和 GPT-2 只使用了 Transformer 的编码器或解码器部分。\n#2.3 深度学习中的数据并行和模型并行\n有两种主要的方法来利用大量硬件加速器以扩展深度学习模型：数据并行（1990）和模型并行。数据并行方法中，每个 minibatch 被划分到多个 Worker 上。模型并行方法中，模型的内存使用和计算分布在多个 Worker 上。通过成比例增加 minibatch 大小和 Worker 数量的比例，可以观测到接近线性增长的训练数据吞吐量。然而，大的训练 batch 会引入优化过程的复杂性，可能导致准确性降低、或者收敛速度变慢，从而抵消了吞吐量增加的好处。\nParallel work 结合了数据并行和激活检查点方法来减少内存的需求。\n然而，这些技术有一个共同的缺点：模型必须能完全放入 Worker 的内存中。随着语言模型越来越大和复杂，神经网络已经接近这个限制。解决这个问题的一个方法是使用参数共享(2019)来减少模型的内存占用，但是会限制模型的整体容量。本文的方法是使用模型并行来将模型分布在多个 GPU 上，这不仅缓解了内存压力，还增加了与 minibatch 大小无关的并行性。\n在模型并行中，还有两个其他的范式：层间流水线并行，和更通用的分布式张量计算。在流水线模型并行中，在一个设备上先执行一组操作，然后输出会被传递到流水线上的下一个设备，在上面进行另一组操作。一些方法使用参数服务器和流水线并行相结合。然而会遇到不一致的问题，这种方法需要额外的处理逻辑，以及对优化器本身进行修改，而这些修改会降低效率或者影响准确性。\n分布式张量计算是一种正交而且更加通用的方法，它将一个张量操作分布在多个设备上，以加速计算或者增加模型大小。\nFlexFlow(Jia et al., 2018)提出了一种选择最优并行化策略的方法。最近，Mesh-TensorFlow(Shazeer et al., 2018)引入了一种语言来指定 TensorFlow 中的一般类分布张量计算。该语言由用户定义维度，使用适当的聚合原语编译生成的图。我们与 Mesh-TensorFlow 有类似的见解，但是我们没有实现新的框架和编译器，而是对现有的 PyTorch Transformer 实现进行了少量针对性的修改。我们的方法简单易懂，不需要任何新的编译器或者代码重写，通过插入一些简单的原语就可以完全实现，如下一节所述。\n#3. 模型并行 Transformer\n本文利用了 Transformer 的网络结构，通过添加几个同步原语来实现一个简单的模型并行。\nTransformer 层由一个 Self Attention 后跟一个 MLP 组成，我们在两个块中分别引入了模型并行性。\n首先介绍 MLP 块。MLP 块的第一部分是 GEMM，然后是 GeLU。\nY=GeLU(XA)Y=GeLU(XA)\nY=GeLU(XA)\n并行化 GEMM 的一种方法是按行拆分权重矩阵AAA，按列拆分输入矩阵XXX：\nX=[X1,X2]A=[A1A2]Y=GeLU(X1A1+X2A2)X=[X_1, X_2] \\\\\nA=\\begin{bmatrix}\nA_1 \\\\\nA_2\n\\end{bmatrix} \\\\\nY=GeLU(X_1 A_1 + X_2 A_2)\nX=[X1​,X2​]A=[A1​A2​​]Y=GeLU(X1​A1​+X2​A2​)\n因为 GeLU 是一个非线性函数，不能继续拆分成两个 GeLU 的和，这种方法需要在 GeLU 函数之前就插入同步点。\n另一种选择是按列拆分A=[A1,A2]A = [A_1, A_2]A=[A1​,A2​]。这种分区允许我们对每个分区的 GEMM 结果独立应用 GeLU 非线性：\n[Y1,Y2]=[GeLU(XA1),GeLu(XA2)][Y_1, Y_2]=[GeLU(X A_1), GeLu(X A_2)]\n[Y1​,Y2​]=[GeLU(XA1​),GeLu(XA2​)]\n这种划分有利于消除同步点。因此，我们采用了这种方法。在分块进行第二次 GEMM 之后，进行 Reduce，然后进行 Dropout 层。这种方法将 MLP 块中的两个 GEMM 都分布在 GPU 上，并且只需要 Forward 和 Backward 各一次 All-Reduce。这两个操作彼此共轭，而且可以在 PyTorch 中使用几行代码实现。例如下面的代码实现：\nclass f(torch.autograd.Function):    def forward(ctx, x):        return x    def backward(ctx, gradient):        all_reduce(gradient)        return gradient\n对于 Self Attention，我们利用了 Multihead Attention 操作中的固有并行性，按照 K, Q 和 V 划分 GEMM 操作，这样每个 Attention 头对应的矩阵乘法都在一个 GPU 上本地执行。这使得我们能够将每个 Attention 头的参数和工作负载分布在多个 GPU 上，并且不需要任何即时通信就可以完成 Self Attention 的计算。输出 linear 层之后的 GEMM 按照行并行化，并直接采用并行注意力层的输出，无需 GPU 之间的通信。这种方法融合了两组 GEMM，去掉了中间一个同步点，并带来了更好的拓展性。这使得我们能够只使用 Forward/Backward 各两次 All-Reduce 的情况下，实现简单 Transformer 层中所有的 GEMM 计算。\nTransformer 语言模型有一个 Hidden-size(HHH) x Vocabulary-size(vvv) 大小的输入 Embedding 层。由于现代语言模型的词表通常在万级别（例如，GPT-2 是 50,257 个），因此对输出 Embedding 层进行并行化是有好处的。然而，在 Transformer 语言模型中，输入和输出 Embedding 层共享权重，需要进行修改。我们按照单词维度并行化输入 Embedding 权重矩阵 EH×v=[E1,E2]E_{H\\times v} = [E_1, E_2]EH×v​=[E1​,E2​]。现在因为每个分片只包含 Embedding 表个一部分，在输入 Embedding 之后需要一次 All-Reduce。\n对于输出 Embedding，一种办法是进行并行 GEMM[Y1,Y2]=[XE1,XE2]GEMM[Y_1, Y_2] = [X E_1, X E_2]GEMM[Y1​,Y2​]=[XE1​,XE2​] 获取 logits，加上一次 All-gather 操作 Y=all−gather([Y1,Y2])Y=all-gather([Y_1, Y_2])Y=all−gather([Y1​,Y2​])，然后把结果发给 Cross-Entropy 损失函数。然而，All-gather 操作需要通信 \\text{batch_size} \\times \\text{seq_len} \\times v 个元素，由于词表大小vvv很大，总体通信也很大。为了减少通信大小，作者将并行GEMM[Y1,Y2]GEMM[Y_1, Y_2]GEMM[Y1​,Y2​]的输出与 Cross-Entropy 聚合在一起，将维度降低至 b×sb \\times sb×s。只通信标量的损失而不是 logits，极大地减少了通信量，提高了模型并行方法的效率。\n我们的模型并发方法的主要特点是减少通信并保持 GPU 计算量。我们选择在多个 GPU 之间复制计算，而不是让一个 GPU 进行一部分计算然后广播结果。具体来说，我们在每个 GPU 上维护 LayerNorm 的参数，并在把参数输出到下一部分之前，在这些 Tensor 上进行 Dropout 和残差连接。为了优化模型，我们允许每个 Worker 优化自己的参数集合。由于所有的参数要么是 GPU 局部的，要么被拷贝过，因此不需要额外的通信去更新参数值。\n简而言之，我们的方法实现简单，只需要在 Forward 和 Backward 的过程中增加一些额外的 All-Reduce 操作。不需要编辑器，而且与流水线模型并行方案正交。\n#4. 设定\n预训练语言理解模型是 NLP 和语言理解的核心任务。语言模型有几种方法。在本文中，我们关注 GPT-2，一种基于 Transformer 的自左向右的生成式语言模型，以及 BERT，一种基于语言模型掩码的双向 Transformer 模型。我们在下一部分解释这些模型的配置。\n#4.1 训练数据集\n包含 Wikipedia + CC-Stories + RealNews + OpenWebtext - WikiText103。\nBERT 数据集额外包括 BookCorpus。过滤掉长度小于 128 个 token 的文档，使用 LSH 消除相似度大于 0.7 的重复内容。最终得到 174GB 的去重文本。\n#4.2 训练优化，超参数\n采用了带 Dynamic Loss Scaling 的 Mixed Precision 训练，以更好地利用 V100 的张量核心。权重初始化 W N(0,0.02)W~N(0, 0.02)W N(0,0.02)。在残差连接之前将权重乘以 1/2N1/\\sqrt{2N}1/2N​，其中 N 是 Transformer 层的数量。对于优化器，使用带 Weight Decay（λ=0.01\\lambda=0.01λ=0.01） 的 Adam 优化器。使用了 1.0 的全局 Gradient Norm Clipping 来提升训练稳定性。\nDropout 使用 0.1。在每个 Transformer 之后使用 Activation Checkpointing。\n对于 GPT-2 模型，Batch size 设定为 512，输入序列长度是 1024 个 subword，迭代 300k 次。学习率在前 3k 次迭代是 1.5e-4，后续采用单周期余弦衰减，在达到 1e-5 之后停止衰减。\n对于 BERT 模型，使用了大小为 30522 的原始词典。使用 Sentence Order Prediction 替换了 Next Sentence Prediction 任务。使用 整词 n-gram 掩码。\nBatch size 设定为 1024，学习率为 1.0e-4，Warmup 10k 次，然后在剩下来的 2m 次迭代中线性衰减。\n#5. 实验\nTODO\n","categories":["mlsys"],"tags":["MLSys","Megatron-LM","Model Parallelism","Distributed Training"]},{"title":"[KDD'15] Petuum: A New Platform for Distributed Machine Learning 论文阅读","url":"/2025/08/31/petuum-a-new-platform/","content":"\nPetuum 是一家位于美国匹兹堡 (Pittsburgh) 的人工智能创业公司, 由卡内基梅隆大学 (CMU) 的邢波 Eric Xing 教授创立. Petuum 团队的技术实力已经获得了业内广泛的认可, 并取得了诸多的奖项, 其中包括 ACM 云计算研讨会上的最佳论文奖、 CBInsights 的全球 AI 初创公司 100 强以及 GWC 2017 年 G-Summit 峰会上的 AI 初创公司 10 强. 迄今, Petuum 的融资总额已达一亿八百万美元, 成为获投资额度最高的早期人工智能初创公司之一.\n\n#0. 摘要\n当前的现代先进机器学习 (ML) 程序的并行化策略通常采用细粒度操作, 并突破了以 MapReduce 为代表的经典批量同步处理范式, 甚至引入了依赖于 ML 程序图形化表示的专用算子. 然而, 这些多样化的方法往往使系统与算法设计朝不同方向发展, 导致难以找到一种通用平台, 以满足大规模下多种不同 ML 程序的需求.\n我们提出了一种通用框架, 系统性地应对大规模 ML 中数据并行与模型并行所面临的挑战. 这一框架充分利用了 ML 程序背后若干关键特性 – 这些特性使其有别于传统以操作为中心的程序: 即错误容忍性、动态结构以及非均匀收敛性.\n该框架能够显著提升多个知名 ML 程序的运行性能: 不仅大幅缩短执行时间, 还支持更大规模的模型训练, 同时在规模适中的计算集群上即可轻松实现.\n#1. 引言\n机器学习 (ML) 正逐渐成为从数据中提取信息的主要机制. 企业拥有海量的数据, 传统批量或分批方式处理这些数据的效率过于低下, 无法实现.\n\n图像识别系统: Billion 级别参数的深度学习模型;\n广告系统: 高达 10^6 个主题的主题模型;\n推荐系统: 依赖高维矩阵分解改善精度;\n\n这些需求无法通过单台机器满足.\n我们认为, 从可扩展执行的角度来看, 阻碍众多前沿机器学习模型和算法在大规模&quot;大数据学习&quot;场景下得到更广泛应用的主要原因, 在于这些模型和算法难以从学术界的实验环境迁移至实际生产环境 – 后者往往规模庞大、不确定性更高, 例如企业级集群或云端平台.\n而在这些环境中, 要确保原始程序的正确运行, 必须对分布式环境及资源的底层细节进行精准把控与深入掌握, 而这恰恰需要高度复杂的分布式编程技术, 绝非易事.\n\n许多平台已提供了部分解决方案, 以弥合这一从研究到生产的鸿沟:\n\nHadoop &amp; MapReduce: 简单性使其难以充分发挥机器学习的特性\nSpark: 未实现对计算与通信的精细化调度\nGraphLab 和 Pregel: 关注基于图的模型, 但是处理不了像主题建模和回归分析这样的机器学习任务\nPS 和 Piccolo: 过于底层, 没有为用户提供更高层次、通用性强的构建模块 – 例如调度机制、模型划分策略以及可控的通信方式\n\n总之, 当前支持分布式机器学习的各类系统, 各自在效率、正确性、可编程性及通用性之间展现出独特的权衡取舍.\n在本文中, 我们从效率、正确性、可编程性和通用性之间的权衡这一全新视角, 探讨了构建分布式机器学习框架的问题.\n我们注意到, 大多数 (甚至可以说是所有) 机器学习程序的一个显著特征是: 它们都由一个明确的数据目标函数定义 (例如似然、误差损失或图割), 而目标正是在模型参数及其他中间变量所限定的空间内, 实现该函数的最优值. 此外, 这些算法还具有共同的计算模式 – 即均采用迭代收敛的过程 (参见公式 1) .\n机器学习程序的真正目标是快速、高效地收敛至最优解, 而我们认为, 细粒度的容错机制与强一致性, 只是实现这一目标的一种手段, 甚至未必是最为高效的途径.\n我们提出了一种全新的分布式机器学习框架 – Petuum, 其构建基于以机器学习为核心的优化理论原则, 而非此前探索过的多种操作目标.\n为了充分利用这些特性, Petuum 提出了三个新颖的系统目标, 这些目标基于上述机器学习程序的关键属性, 旨在大规模加速其收敛过程:\n\nPetuum 通过有限的延迟保证同步参数状态, 这种机制利用了机器学习固有的容错特性, 能够确保计算结果的正确性, 同时通信开销远低于传统的每轮批量同步方式;\nPetuum 提供动态调度策略, 能够实时考虑模型参数之间不断变化的结构依赖关系, 从而最大限度地降低并行化误差和同步成本;\n由于机器学习程序中的参数收敛代价并不均衡 (即所需更新次数各不相同), Petuum 会优先对尚未收敛的模型参数进行计算, 以实现更快的全局收敛速度.\n\n#2. 前置依赖: 数据和模型并行\n我们首先提出一种基于原则的迭代收敛型机器学习程序表述, 这种表述揭示了数据与模型之间的二元对立, 为 Petuum 的并行系统架构 (§3) 、算法设计 (§4) 以及理论分析 (§5) 提供了灵感. 请考虑以下将机器学习视为由目标函数驱动的迭代收敛型程序的编程视角:\n迭代收敛的机器学习算法: 给定数据 D 和损失 L (即一种适应度函数, 如 RMS 损失、似然性或间隔),\n典型的机器学习问题可被表述为: 反复执行以下更新方程, 直至模型状态 (即参数和/或隐变量) A 达到某种停止条件:\nA(t)=F(A(t−1),ΔL(A(t−1),D))A^{(t)}=F\\left( A^{(t-1)}, \\Delta_{L}(A^{(t-1)},D) \\right)\nA(t)=F(A(t−1),ΔL​(A(t−1),D))\n其中, 上标(t)表示迭代. 更新函数 ∆L() (用于优化损失 L) 对数据 D 和模型状态 A 进行计算, 并输出中间结果, 供 F()进一步汇总. 为简化起见, 在本文后续部分, 我们省略了下标中的 L, 但需明确的是, 我们所关注的所有机器学习程序均显式定义了损失函数, 这一损失函数可用于监测收敛性和解的质量, 而不同于那些不附带此类损失函数的启发式方法或操作流程.\n在大规模机器学习中, 数据集 D 和模型 A 都可能非常庞大. 数据并行化 – 即将数据划分到多台机器上处理 – 是解决大数据问题的常用策略.\n#3. Petuum 框架\nPetuum 的核心目标是让数据并行和模型并行的机器学习算法实现起来更加简便. 为此, Petuum 为关键系统提供了 API, 以简化这一任务:\n\n一个参数服务器系统, 它允许程序员通过便捷的分布式共享内存接口, 从任意机器访问全局模型状态 A, 该接口的设计类似于单机编程; 同时, 系统采用了一种有界异步一致性模型, 既能确保数据并行的收敛性, 又免去了用户手动进行网络同步的繁琐操作;\n一个调度器, 可对模型并行更新 ∆()的并行顺序进行精细控制 – 本质上, 调度器让用户能够自定义自己的机器学习应用一致性规则.\n\n#3.1 Petuum 系统设计\n机器学习算法展现出若干可被利用的原则, 以加速分布式机器学习程序: 参数间的依赖关系结构、参数的非均匀收敛性, 以及有限的误差容忍度. Petuum 允许开发者编写数据并行和模型并行的机器学习程序, 充分挖掘这些原则的潜力, 并能轻松扩展至大数据和大模型应用领域. Petuum 系统由三个核心组件构成 (见图 3) : 调度器、工作节点和参数服务器; 而 Petuum 的机器学习程序目前采用 C++ 编写 (未来不久将支持 Java) .\n\n\n\n调度器: 调度系统通过允许用户控制哪些模型参数由工作机器更新, 实现了模型并行. 这一过程由用户自定义的调度函数schedule() (对应于S(t−1)p(())) 完成, 该函数为每个工作节点输出一组参数 – 例如, 一个简单的调度策略可能为每个工作节点随机选择一个参数, 而更复杂的调度器 (如我们稍后将展示的) 则可根据多种标准挑选参数, 比如两两独立性或与收敛点的距离. 调度器会通过调度控制通道 (见图 3) 将这些参数的标识发送给各工作节点, 而实际的参数值则由我们将很快介绍的参数服务器系统负责传输; 调度器仅需决定更新哪些参数. 在第 5 节中, 我们将讨论模型并行调度所具备的理论保证. 此外, 几种常见的调度设计模式也值得深入探讨.\n\n\n工作节点: 每个工作节点 p 从 schedule() 接收待更新的参数, 随后并行执行针对数据 D 的更新函数 push() (对应于 ∆()) . Petuum 特意未指定数据抽象, 因此可使用任何类型的数据存储系统 – 工作节点既可直接读取加载到内存中的数据, 也可从磁盘读取, 甚至可通过分布式文件系统或 HDFS 等数据库访问数据. 此外, 工作节点还可按程序员所希望的任意顺序操作数据: 在数据并行的随机算法中, 工作节点可能逐个采样数据点; 而在批处理算法中, 工作节点则可能一次性遍历所有数据点. 当 push() 正在执行时, 模型状态 A 会通过参数交换通道自动与参数服务器同步, 这一过程采用了一种便捷的分布式共享内存编程接口, 从而实现高效的协同工作.\n\n\n参数服务器: 参数服务器 (PS) 通过便捷的分布式共享内存 API, 提供对模型参数的全局访问 – 这些参数被分散存储于多台机器上, 类似于基于表或键值存储的方式. 为了充分利用机器学习算法原理, PS 实现了&quot;过时同步并行&quot; (SSP) 一致性, 这一机制在降低网络同步开销的同时, 仍能确保由 SSP 所保障的有界过时收敛性. 我们将在第 5 节中详细讨论这些保证. 与仅支持数据并行的纯 PS 系统不同, Petuum 结合了调度器与 PS 的设计, 使得数据并行和模型并行算法均可异步运行, 并享受在更多机器上可验证的速度提升保障.\n\n\n容错机制采用检查点与重启策略, 该策略适用于最多 100 台机器; 而对于数千台机器的更复杂方案, 则属于未来的工作内容. 为进一步提升网络性能, Petuum 可被配置为遵守带宽限制, 并支持逻辑网络拓扑结构 (如环形、网格或胖树型).\n#3.2 编程接口\n图 4 展示了一个基本的 Petuum 程序, 它由一个中央调度函数schedule()、一个并行更新函数push(), 以及一个中央聚合函数pull()组成. 模型变量 A 存储在参数服务器中, 用户可随时通过 PS 对象从任意函数访问这些变量. PS 对象本身也可被任何函数调用, 其包含三个功能：PS.get()用于读取参数, PS.inc()用于对参数进行累加操作, 而PS.put()则用于覆盖现有参数. 仅凭这些操作, SSP 一致性模型便能自动确保 Petuum 各组件之间的参数一致性, 无需用户额外编写代码. 最后, 我们用 DATA 来表示数据 D; 如前所述, DATA 可以是任何第三方数据结构、数据库, 或分布式文件系统.\n\n#4. PETUUM 并行算法\n#4.1 数据并行距离度量学习 (Distance Metric Learning, DML)\n#4.2 模型并行 Lasso\n#4.3 其他算法\n在 Petuum 上实现实现的其他算法:\n\nLDA\nMF\nDL: CNN+CrossEntropy 实现图像分类\n\n#5. 原则与理论\n#5.1 容错收敛\n数据并行的机器学习算法通常对中间计算中的轻微误差具有较强的鲁棒性; 因此, 即使模型参数 A 出现同步延迟 (例如, 各个工作节点仅看到过时或陈旧的参数), 只要这些延迟被严格限制, 算法仍能正确执行.\nPetuum 正是利用了这种容错特性, 通过在参数服务器系统之上实现&quot;过时同步并行&quot; (SSP) 一致性模型, 大幅降低了网络通信与同步的开销 – 该系统确保所有机器都能访问参数 A.\nSSP 一致性模型保证: 如果某个工作节点在第 c 次迭代时从参数服务器读取数据, 它将确保接收到所有工作节点在第 c − s − 1 次迭代及之前完成的更新, 其中 s 是陈旧性阈值. 如果由于某些落后的工作节点而无法实现这一点, 超过 s 次迭代后, 读者将暂停, 直到落后者追上并发送其更新为止.\n可以证明对于 SGD 算法, SSP 能够保证概率收敛性.\n#5.2 依赖结构\n#5.3 非一致收敛\n在模型并行的机器学习程序中, 经验表明, 某些参数 Aj 的收敛速度可能比其他参数快得多或慢得多. 例如, 在 Lasso 算法中就会出现这种情况, 因为该模型强制实现稀疏性, 导致大部分参数在整个算法运行过程中始终保持为零, 且极难再次变为非零值. 因此, 根据参数的大小对 Lasso 参数进行优先排序, 能够显著提升每次迭代的收敛效率, 同时避免频繁 (且浪费资源) 地更新那些本已为零的参数.\n#6. 性能评估\nPetuum 以机器学习为核心的设计架构, 能够支持多种机器学习程序, 并从以下几方面显著提升其在大数据环境下的性能:\n\nPetuum 实现的 DML 和 Lasso 算法, 其收敛速度远超基准方案 (即单机实现的 DML, 以及 Shotgun) ;\n与 Spark、GraphLab4 等其他平台相比, Petuum 的机器学习实现运行更加高效, 这得益于 Petuum 能够有效利用模型间的依赖关系、不均衡的收敛特性及容错能力;\nPetuum 的机器学习实现还能处理更大规模的模型, 原因在于 Petuum (在参数服务器和调度器上) 采用轻量化的存储方式来管理机器学习程序中的变量.\n对于尚未实现分布式版本的机器学习程序, 我们可以在 Petuum 平台上加以实现, 并展示其随着机器数量增加而表现出良好的扩展性.\n\n#附录\n\n如何评价 Eric Xing 实验室做的 Petuum 分布式机器学习平台？\nPetuum：大规模机器学习平台的变革\nGitHub: sailing-pmls/bosen\ntensorflow 和 petuum 有可能集成在一起使用吗？ - 张颖峰的回答 - 知乎\n\n","categories":["mlsys"],"tags":["MLSys","Distributed Machine Learning","System","Petuum"]},{"title":"[ICDE'22] PICASSO: Unleashing the Potential of GPU-centric Training for Wide-and-deep Recommender Systems 论文阅读","url":"/2026/02/22/picasso-unleashing-the-potential-of-gpu-centric/","content":"#0. 摘要\n个性化推荐的发展显著提高了信息匹配的准确性和电子商务平台的收入.\n近期, 该领域呈现出两大趋势:\n\n推荐系统必须及时进行训练, 以应对在线营销和社交网络中不断涌现的新产品以及不断变化的用户兴趣;\n最先进的推荐模型引入了深度神经网络 (DNN) 模块以提高预测准确率.\n\n传统的基于CPU的推荐系统已无法满足这两大趋势, 以GPU为中心的训练已成为一种主流方法.\n然而, 我们观察到在训练推荐系统时, GPU设备的利用率不足, 且无法达到其在计算机视觉 (CV) 和神经语言处理 (NLP) 领域所实现的预期吞吐量提升.\n这一问题可归因于这些推荐模型的两个特征: 首先, 它们包含多达上千个输入特征字段, 导致了碎片化和内存密集型操作; 其次, 多个组成的特征交互子模块引入了大量的小规模计算内核.\n为了消除这一阻碍推荐系统发展的障碍, 我们提出了一种名为PICASSO的新型框架, 旨在加速推荐模型在商用硬件上的训练.\n具体而言, 我们进行了系统分析, 揭示了训练推荐模型时遇到的瓶颈.\n我们利用模型结构和数据分布, 通过打包、交错和缓存优化来释放硬件潜力.\n实验表明, PICASSO在最先进基线的基础上将硬件利用率提高了一个数量级, 并为多种工业级推荐模型带来了高达6倍的吞吐量提升.\n在生产环境中使用相同的硬件预算, PICASSO平均将每日训练任务的实际耗时缩短了 7 小时, 显著减少了持续交付的延迟.\n#1. 引言\n如今 (2022 年), 推荐系统已成为社交网络和电子商务平台提高收入、用户参与度和客户留存的关键.\n为了应对数据的爆炸式增长, 推荐系统正迅速从协同过滤 (CF) 演变为深度神经网络 (DNN) 模型, 并持续提高任务准确率, 如图1所示.\n从Google的Wide&amp;Deep开始, 工业级推荐模型的创新遵循两个趋势:\n\n嵌入层变得更宽, 消耗多达数千个特征\n特征交互层通过利用不同特征子集上的多个 DNN 子模块变得更加深层.\n\n我们将这些模型称为 Wide-and-Deep Learning (WDL) 推荐模型.\n工业级 WDL 模型必须定期重新训练, 以及时、准确地反映用户兴趣漂移和新的热点.\n因此, 高训练吞吐量对于 WDL 模型追赶流式数据并降低持续交付的延迟至关重要.\n由于缺乏完成深度特征交互的计算能力, 在大规模分布式 CPU 集群上通过参数服务器 (PS) 训练最先进的 WDL 模型非常耗时.\n近期的研究工作, 以 Facebook 的 TorchRec、百度的 PaddleBox 和 NVIDIA 的 HugeCTR 为代表, 倾向于在 WDL 工作负载上采用以 GPU 为中心的同步训练框架, 因为高端 NVIDIA GPU (例如 NVIDIA Tesla V100) 的单精度浮点运算速度比 Intel CPU 高出 30 倍.\n这些工作大多通过利用 GPU 设备提高了训练吞吐量.\n\n图 1. WDL 的趋势从阿里巴巴一项业务的角度出发. 这些模型的平均 GPU 利用率通过内部优化的 Tensorflow in PS 训练策略从训练工作负载中收集. 统计数据显示, 除了性能提升外, WDL 模型的训练并未充分利用标准训练框架 (在训练同等规模的 CV 或 NLP 模型时 GPU 利用率可达 95%+) .\n然而, 如图 1 所示, 我们观察到随着特征字段和特征交互模块数量的增加, 存在严重的硬件资源利用率不足 (例如, 测得的 GPU 利用率).\n这意味着 WDL 训练工作负载远未达到硬件的峰值性能, 预期可进一步加速.\n虽然针对特定的 WDL 工作负载模式定制硬件可能是一种选择, 但存在以下顾虑:\n\n我们有多种 WDL 设计, 它们拥有显著不同的工作负载模式 (例如, 特征字段的数量、特征交互层的子模块), 且新的 WDL 模型每月都在涌现;\n对于公有云使用, 出于预算和弹性的考虑, 首选商用硬件.\n\n因此, 我们提出两个问题: 是什么导致了训练 WDL 模型时的硬件利用率不足问题?\n我们能否从软件系统的角度解决这个问题?\n我们对各种 WDL 工作负载进行了系统分析 (详见 2), 并得出如下启示:\n\n由于特征字段数量巨大 (多达数千个), WDL模型训练在嵌入层和特征交互层内包含碎片化操作, 这带来了启动操作 (例如向CUDA流发布CUDA内核) 的不可忽视的开销以及硬件利用率不足的问题.\nWDL模型的嵌入层主要由内存密集型和通信密集型操作组成 (在分布式环境中), 而特征交互和 MLP 则包含计算密集型操作.\n在处理大量嵌入参数时, 计算资源利用率不足, 从而导致脉冲式的GPU使用模式.\n\n随后, 我们提出了一个新颖的框架——包含打包、交错和缓存增强的软件系统优化 (PICASSO), 以回答上述两个问题.\n\n我们创建细粒度的嵌入特征组. 同一组内的操作被打包以减少碎片化操作的数量;\n来自不同组的操作在数据层和内核层两个层面进行交错, 以提高硬件利用率;\n我们开发了一种数据分布感知的缓存机制, 利用大容量DRAM和GPU设备内存的高带宽.\n\n评估表明, PICASSO 在训练多种工业级 WDL 模型时显著提高了 GPU 利用率, 并且与最先进的通用训练框架 (TF 1.15+PS, PyTorch+All2All, PyTorch+AllReduce) 相比, 将吞吐量提高了一个数量级.\nPICASSO 已部署在我们的内部训练集群中, 在阿里巴巴内部被称为 XDL2, 在阿里云中被称为 HybridBackend.\n将持续交付的延迟从平均 8.6 小时降低到 1.4 小时, 这在阿里巴巴内部是前所未有的, 并对社区具有指导意义.\n本文的主要贡献总结如下:\n\n我们分析了使用以 GPU 为中心的同步框架训练 WDL 模型时硬件利用率不足的问题, 并揭示了其原因.\n我们提出的 PICASSO 采用软件系统方法解决硬件利用率不足的问题, 该方法适用于通用硬件.\n我们构建了一个系统, 能够支持我们日常生产工作负载, 处理高达万亿个参数和 PB 级训练数据, 在未增加预算的情况下实现了平均6倍的训练性能提升.\nPICASSO 已发布, 可用于公共云.\n\n#2. 基于 Workload 分析的启示\n#2.1. WDL 模型架构\n\n图 2. WDL 的标准架构, 1) 嵌入层 吸收多个特征字段 (不同于只有一个 “word” 字段的 NLP 任务), 采用各种哈希或数值操作, 并从内存中查询 (不同维度的) 嵌入; 2) 特征交互层 通过多个特征交互模块处理取样的特征嵌入; 3) 模块的输出被串接并输入 MLP 以获得最终预测.\nWDL 模型具有如图2所示的典型架构:\n数据传输层 处理以类别特征身份 (ID) 以及稠密特征向量形式传输的流式训练数据.\n类别特征ID通常具有可变长度 (即多热或非表格数据), 并且在一个批次内可能达到数十或数百兆字节.\n通常, 数据存储在远程数据库中, 需要通过以太网进行传输;\n嵌入层 将稀疏类别特征的高维特征空间投影到低维嵌入特征空间.\n嵌入参数以名为特征嵌入的稠密向量表示, 并以嵌入表的形式存储在DRAM中.\n每个特征嵌入都可以通过其类别特征ID进行查询, 以便在WDL中进行训练.\n由于每个训练批次需要从DRAM中查询大量特征嵌入, 因此嵌入层主要由访存密集型操作主导.\n特征交互层 首先将来自嵌入层的特征嵌入组织成若干组.\n每组应用一个独立的特征交互模块, 例如 GRU 和 Transformer, 以从组内特征嵌入中提取有用信息.\n然后, 将组成特征交互模块的输出进行拼接, 以形成特征交互层的最终输出.\n该层中可能存在数十个组成特征交互模块, 以处理层中的主要激活特征嵌入的不同子集, 产生多达数十万次运算.\nMLP 利用一批次训练数据构建全连接层, 以提供最终预测.\nMLP还包含计算密集型架构单元, 如批归一化和残差连接.\n值得注意的是, 在许多业务场景中, 精度损失通常是不可接受的.\n因此, 通用加速策略 (例如半精度、量化剪枝、梯度陈旧性) 可能仅适用于部分WDL模型.\n#2.2. WDL 工作负载中的数据分布\n每个特征字段的类别特征ID通常呈偏斜或非均匀分布.\n我们研究了五个代表性 WDL 数据集的数据分布 (统计数据列于表 2 中) 及图 3.\n当按频率降序排列时, 平均而言 20% 的 ID 将覆盖 70% 的训练数据, 最高可达 99%.\n因此, 在训练WDL模型时缓存频繁访问的数据是有益的.\n\n图 3. 类别特征 ID 在代表性 WDL 数据集中的分布.\n#2.3. WDL 模型的分布式训练策略\n总体而言, 在以 GPU 为中心的分布式系统中训练 WDL 模型时, 通常采用三种训练策略:\n参数服务器 (PS) 策略仍是工业界实际应用的主流训练策略, 其中训练数据被划分到多个工作节点, 而模型参数则被划分到多个服务器节点.\n工作节点从服务器节点拉取 (pull) 模型参数, 并利用本地划分的训练数据进行训练;\n在每次迭代结束时, 工作节点将相应的梯度异步推回 (push) 服务器节点以更新参数.\n数据并行 (DP) 策略是 Tensorflow 和 PyTorch 等框架的默认分布式训练策略.\n训练数据被均匀划分到所有工作节点, 而模型参数则被复制到所有工作节点.\n它使用一种名为 Allreduce 的集合通信原语来聚合梯度, 从而同步更新模型参数的本地副本.\n模型并行 (MP) 策略不存在服务器节点.\n相反, 它将所有参数划分并存储在多个工作节点上.\n然后, 它使用一种名为 AllToAllv 的集合通信原语在所有工作节点之间同步交换数据.\n#2.4. WDL 工作负载的特征分析\n我们首先将WDL工作负载到底层硬件进行底层映射, 随后总结三种代表性工作负载模式.\n从算法角度来看, 每个 WDL 层由一组算子组成.\n算子在程序中通常实现为内核 (kernel), 其在训练过程中的调用被称为操作 (operation).\n操作的执行需要各种硬件资源.\n图 4 展示了在分布式系统中训练的标准 WDL 模型针对三种硬件资源的底层投影:\n\n节点内带宽 (例如 DRAM 和 PCIe 带宽)\n计算资源 (例如 GPU 流式多处理器)\n节点间带宽 (NVLink 和以太网带宽)\n\n\n图 4. WDL 模型标准架构的低层投影: 多特征字段和组成特征交互模块, 会在计算图中造成大量重复操作.\n在 WDL 模型的分布式训练中, 嵌入层主要包含以下算子:\n\nUnique: 消除冗余类别特征 ID 以减少内存访问开销\nPartition: 将类别特征 ID 划分为本地 ID 和远程 ID\nGather: 从嵌入表中查询本地 ID\nShuffle: 与远程工作节点通信以获取属于远程 ID 的特征嵌入\nStitch: 拼接本地查询的特征嵌入和远程获取的特征嵌入\nSegmentReduction: 按段对特征嵌入进行池化, 例如对同一用户的行为特征嵌入求和.\n\n嵌入层中的大多数算子受限于一种主导类型的硬件资源 (例如, Shuffle 算子受限于节点间带宽).\n特征交互层和 MLP 的算子主要受限于计算资源.\n类似地, 反向传播可以被视为前向传播的镜像.\n在同步的以 GPU 为中心的训练系统中, 这种底层投影表明硬件资源的使用是间歇性的, 这意味着在某一时刻, 训练将受限于一种类型的硬件资源, 而其他类型的资源则未被充分利用.\n与 CV 和 NLP 工作负载相比, 该投影反映了 WDL 工作负载的三个特征.\n\n嵌入层和特征交互层涉及大量小规模操作 (即, 单个特征字段的嵌入可能涉及数百次操作, 而WDL工作负载可能需要处理多达数千个特征字段), 这会带来显著的启动开销.\n不同特征字段对嵌入层中同一算子的调用会在同一时刻争用同一类型的硬件资源, 这在硬件资源相对紧张时 (例如, CPU与GPU之间的PCIe带宽) 会限制吞吐量.\n如 2.2 所述的数据倾斜分布会导致分布式系统中工作节点间的硬件资源使用不均衡, 从而在同步训练期间损害吞吐量.\n\n我们调研了阿里巴巴云上一组 NVIDIA Tesla V100 (表 1 中的 EFLOPS) 上 WDL 工作负载的统计数据 (由如所述的商用硬件设备组成).\n这些模型由 Tensorflow 实现, 采用如 2.3 介绍的PS策略或MP策略.\n从性能剖析和工作端性能分解中, 我们观察到了如图 5 所示的三种代表性模式.\n\n\nI/O 和内存密集型工作负载.\n以 W&amp;D 为代表的 WDL 模型包含大量的数据传输和嵌入查找操作, 其中I/O可能无法完全与其他过程重叠, 如图 5 所示.\n随着特征工程和可迁移预训练嵌入的蓬勃发展, I/O 和内存密集型模型应运而生, 这些模型需要处理海量特征字段才能达到最佳精度.\n即使进行了 I/O 优化, 如图 5 所示, 暴露的 I/O 和内存访问仍然占用总训练时间的约 20%.\n\n\n通信密集型工作负载.\n这类工作负载的大部分时间都花费在通信相关的操作上.\n由于获取远程高维嵌入特征产生的大量通信, 以及来自高阶交叉特征的频繁参数交换, 导致分布式WDL工作负载中产生严重的通信开销.\n我们以 CAN 为例, 它是最近从 DIN 和 DLRM 衍生而来的.\nCAN 包含大量特征字段上的特征交互模块的组合, 因此如图5所示, 它在MP模式下带来了约 60% 的通信开销, 在PS模式下带来了约 70% 的通信开销.\n\n\n计算密集型工作负载.\n由于深度和复杂的 WDL 模型受益于 CV 和 NLP 领域的进步, 部分 WDL 模型受到计算操作的制约.\nMMoE 的一种变体源自标准 DIN, 在 MLP 中拥有71个专家, 用于我们业务中场景感知的CTR预测.\n在图 5 中, MMoE 大约花费 50% 的训练时间进行算术计算.\n在实践中, 计算密集型 WDL 模型倾向于应用于多子任务场景 (例如, 多目标学习、元学习)、超复杂计算 (例如, 大量特征字段上的极深网络) 和多模态协同训练.\n\n\n\n图 5. 按 PS 和 MP 策略对三种 WDL 模型的 Worker 端拆解 (“Exposed” 表示该作阻塞所有其他模型的阶段) .\n#3. PICASSO 系统设计\n在本节中, 我们首先介绍PICASSO的整体架构, 该架构适应以GPU为中心的训练集群的硬件拓扑, 从而支持WDL模型的混合分布式训练策略.\n其次, 我们介绍通过提高硬件利用率来加速训练的三个核心思想.\n#3.1. 混合分布式训练策略\n\n图 6. PICASSO 支持 MP 和 DP 的混合策略, 嵌入参数在 PICASSO 执行器 (MP) 之间划分, 密集参数在 PICASSO 执行器 (DP) 间复制. 此外, PICASSO 也支持规范的 DP 和 MP.\n我们在图6中提出了 PICASSO 的架构, 该架构专为以GPU为中心的集群而设计.\n此类集群通常由多台配置相同的机器 (集群节点) 组成, 而每台机器则具有异构架构, 包括处理器 (如 Intel CPU) 、加速器 (如 NVIDIA GPU) 以及非核心系统 (如 DRAM).\n此外, 机器内部各组件之间存在 PCIe 和 NVLink 等互连, 且所有机器进一步通过以太网连接, 构成一个分布式系统.\n相应地, PICASSO 设置了多个 PICASSO-Executor, 它们分别映射到集群中的不同机器.\n每个 PICASSO-Executor 拥有异构硬件资源:\n\nGPU 流多处理器 (SM) 和 CPU 物理核心作为计算资源;\n由 GPU 设备内存、DRAM、Intel 持久内存和 SSD (如果可访问) 构成的分层内存子系统作为内存存储资源;\nNVLink、PCIe、InfiniBand和以太网等分层互连作为通信资源.\n\n借助该架构, PICASSO 能够针对 WDL 模型中的不同层定制混合分布式训练策略, 具体如下:\n\n嵌入层拥有海量的嵌入参数, 并采用模型并行 (MP) 策略.\n嵌入参数在所有 PICASSO-Executors 之间进行分区, 并存储在分层内存子系统中.\n参数通过 AllToAllv 集合通信原语在 PICASSO-Executors 之间进行同步交换.\n特征交互层和 MLP 的参数量远小于嵌入层.\n我们对这两层采用数据并行 (DP) 策略, 其中参数在所有 PICASSO-Executors 上进行复制, 并通过 Allreduce 原语进行聚合.\n\n#3.2. Packing\n为了解决 2.4 中所述的碎片化操作问题, 我们提出了一种打包方法, 该方法从两个方面有效减少了操作数量:\n数据打包 (D-Packing).\n当来自不同特征域的分类特征 ID 输入到嵌入层内的同一算子时, PICASSO 将分类特征 ID 组合成单个打包 ID 张量.\n因此, 我们可以启动单个操作 (称为打包操作) 来处理打包数据, 这符合 NVIDIA GPU 设备的单指令多数据 (SIMD) 编程范式.\n此外, 它显著减少了向 GPU 设备启动大量操作的开销.\n此外, 将所有类别特征ID打包到单个张量中的朴素策略可能会导致严重的吞吐量问题.\n例如, 工业级推荐系统通常使用哈希表来实现嵌入表, 以适应不断增长的特征嵌入数量.\n海量的 (百万级) 并发查询请求将受限于哈希表底层的锁机制.\n因此, 我们在嵌入表共享相同特征维度时对类别特征ID进行打包.\n从而, 我们获得的打包操作的数量与不同特征维度的数量成正比.\n然而, 由于数据分布偏斜和特征维度较大, 某些打包操作仍可能面临过多的并发查询, 从而损害哈希表的吞吐量.\n我们提出了一种通过估计打包操作内的参数量 (浮点数数量) 来评估执行成本的方法, 如公式1所示.\nCalcVParam(T)=N∑t∈T(tdim∑ID∈tIDfreq)\\mathrm{CalcVParam}(T)\n=\nN \\sum_{t \\in T}\n\\left(\nt_{\\mathrm{dim}}\n\\sum_{\\mathrm{ID} \\in t}\n\\mathbf{ID}_{\\mathrm{freq}}\n\\right)\nCalcVParam(T)=Nt∈T∑​(tdim​ID∈t∑​IDfreq​)\n其中, N 表示类别特征 ID 的总数, T 表示打包的嵌入表, t dim 表示嵌入表的特征维度, ID freq 表示类别特征 ID 的频率.\nN 和 ID freq 可从预热迭代的统计信息中获取.\n如果某个打包操作的 CalcVParam(T) 较高且高于平均水平, 我们应将其进一步均匀拆分为多个分片.\n例如, 假设我们有一个针对所有特征维度为 8 的嵌入表的打包操作, 且数据分布均匀.\n对于维度为 32 的嵌入表, 我们将创建四个打包操作分片, 每个分片包含这些嵌入表的四分之一.\n内核打包 (K-Packing).\n内核融合已是深度学习系统中广泛采用的一种优化技术.\n主要存在两种方法:\n\n手写的大型内核 (例如将整个嵌入层融合到单个 CUDA 内核中) 会错失受不同硬件资源限制的操作交错执行的机会 (详见 3.3).\n基于编译的代码生成 依赖于每个算子的静态输入和输出形状来推断生成内核的合适大小.\n然而, 分类特征ID会导致动态算子形状, 从而破坏Tensorflow XLA等编译技术的效率.\n\n相比之下, 我们的内核打包根据硬件资源利用率评估所有内核, 并将它们分为计算密集型内核, 内存密集型内核 和 通信密集型内核.\n我们仅融合来自同一内核组的内核, 并为跨内核组交错执行保留机会.\n图 7 展示了我们的打包优化过程.\n分类特征ID首先被分组在一起 (即D-Packing).\n每组分类特征ID被输入到一个名为Unique&amp;Partition的融合内核 (即K-Packing) 中, 以消除来自冗余ID的内存访问和数据通信.\n分类特征ID将从其嵌入表的本地分区中获取.\n然后, 我们开发了另一个名为Shuffle&amp;Stitch的融合内核, 以实现shuffle kernel的拼接输出, 并移除显式的stitch kernel.\n\n图 7. 打包优化示例: a) 不打包时, 从三个嵌入表查找特征嵌入需要 18 次操作; b) 通过 D-打包 和 K-打包, PICASSO 将操作总数减少为 4 个打包操作.\n#3.3. 交错\n在应用了打包优化之后, 我们进一步开发了两种类型的交错优化, 以提高不同硬件资源的利用率.\n数据交错 (D-Interleaving): 当 WDL 模型使用大 Batch Size (例如数万) 进行训练时, 操作会受到各种硬件限制的制约.\n例如, 来自中间张量 (在 TensorFlow 中称为特征图) 的 GPU 设备内存占用与数据 Batch Size 成正比.\n由于 GPU 设备内存容量受限 (例如 NVIDIA Tesla V100 为 32GB), 大 Batch Size 很可能导致内存溢出 (OOM) 问题并导致训练崩溃.\n然而, 为了在 WDL 训练中获得高精度和高吞吐量, 通常需要大 Batch Size .\n因此, PICASSO 采用了一种基于 Micro Batch 的数据交错 (D-Interleaving) 方法, 允许用户从 WDL 模型的指定层开始对工作负载进行切片和交错.\n为了解决 GPU 设备内存的 OOM 问题, 我们可以将特征交互层的输出嵌入划分为若干 Micro Batch , 并对 MLP 应用 D-Interleaving, 其中 GPU 设备内存的峰值使用量可以被摊销, 如图 8 (a) 所示.\n此外, 我们可以将类别特征 ID 划分为若干 Micro Batch , 并对训练的其余部分应用 D-Interleaving, 如图 8 (b) 所示.\n默认情况下, 我们将数据均匀划分为 Micro Batch 以实现负载均衡, 微 Batch Size 可以通过以下公式估算:\n其中 BS micro 是估计的 Micro Batch 大小, RBound op 表示算子主要硬件资源 (例如 GPU 设备内存容量) 的边界值, 而 RInstance op 表示算子主要硬件资源中每个数据实例的成本.\n由于嵌入层中算子的形状通常是动态的, 因此无法预先推导出公式 2 的解析值.\n相反, 我们通过训练的热身迭代以经验或实验方式确定其值.\n\n图 8. a) D-交错从 MLP 开始, 以消除 GPU 设备内存容量限制; b) D-交错从嵌入层开始, 以减少开销. c) 不同硬件资源上的填充和非填充嵌入作的 K-交错.\n内核交错 (K-Interleaving) 打包机制将数百个操作转换为少量打包的操作.\n然而, 来自不同嵌入表的打包操作仍会争用相同的硬件资源.\n例如, 当所有 Shuffle&amp;Stitch 操作并发启动时, 以太网带宽会限制训练吞吐量.\n我们提出了一种内核交错 (K-Interleaving) 优化, 该优化在打包操作组之间建立控制依赖关系, 如图 8 所示.\n为确保所谓的交错组不会受到各种硬件资源的限制, 我们首先根据处理的参数确定每个交错组的容量, 记为 Capacity g, 公式如下:\nCapacityg=min⁡op∈layer(RBoundopRParamop)\\mathrm{Capacity}_g\n=\n\\min_{\\mathrm{op} \\in \\mathrm{layer}}\n\\left(\n\\frac{\\mathrm{RBound}_{\\mathrm{op}}}{\\mathrm{RParam}_{\\mathrm{op}}}\n\\right)\nCapacityg​=op∈layermin​(RParamop​RBoundop​​)\n其中 RBound op 的定义与方程 2 中相同, RParam op 表示从算子的主要硬件资源训练一个参数的成本.\n在此, 我们简单地将参数量视为嵌入查找和交换中的成本.\n我们还可以通过按比例修改 PICASSO 中的Capacity g来改变交错组的数量.\n值得注意的是, 我们允许用户指定预设的排除嵌入, 其中打包操作对其他K-交错组没有控制依赖.\n例如, 当某些操作的输出 (特征嵌入) 不会与其他特征嵌入进行拼接以供下游层使用时, K-交错可以推进其下游操作.\n#3.4. 缓存\n\n图 9. 通过 HybridHash 上的缓存方法收集嵌入向量 (例如 “Emb-8002”).\n缓存是一种广泛应用的系统技术, 它利用分层内存子系统来降低内存访问延迟.\n然而, 缓存的有效性, 即缓存命中率, 取决于多种因素, 如数据分布和访问模式.\n在 2.2 中, 我们观察到仅有 20% 的分类特征 ID 被高频查询, 这促使我们提出一种名为 HybridHash 的优化方案.\n该方案旨在消除两个硬件限制:\n\nDRAM具有大容量, 但受限于内存访问带宽;\nGPU设备内存具有高带宽, 但受限于有限的容量.\n\n如图 9 所示, HybridHash 作为一个哈希表, 用于存储、获取和更新嵌入参数.\n我们将 GPU 设备内存称为热存储 (Hot-storage), 将DRAM称为冷存储 (Cold-storage).\n与其他基于 GPU 的哈希表解决方案不同, 我们将热存储视为一种昂贵的资源, 应避免浪费其容量.\n因此, 我们将哈希表这一稀疏数据结构放置在冷存储上, 并仅将热存储作为暂存器, 用于存储和更新频繁访问的嵌入.\n回顾第 2.2 节, WDL工作负载中的分类特征ID遵循一定的分布.\n在预定义的预热迭代次数内, 记录从哈希表中查询的每个ID的频率是合理的.\n随后, HybridHash 定期将前 k 个 (k 由热存储的大小决定) 频繁嵌入从冷存储加载到热存储, 以保留最热的分类特征 ID.\n在预热步骤之后, 预期大多数 ID 查询会在热存储中命中, 而未命中的查询可由冷存储处理.\n需要注意的是, 如果在预热步骤中发现热存储的容量远超嵌入表的总大小, HybridHash 会将所有数据放置在热存储上.\n此外, HybridHash可扩展为多级缓存系统, 包括Intel持久内存和SSD等设备.\nHybridHash的算法如算法1所示.\nL9-12描述了在预热迭代期间收集统计信息的步骤, L14-22介绍了获取嵌入的规则, L23-26定义了更新热存储内容的程序.\n\n#4. 实验\n在本节中, 我们进行了广泛的实验以回答以下研究问题:\n\n与经过计算优化的最先进通用框架相比, 通过释放硬件潜能, PICASSO 能否实现更高的吞吐量?\nPICASSO 中的软件系统优化如何影响每种硬件资源的利用率?\nPICASSO 在多样化的 WDL 模型架构和特征字段上的表现如何?\n\n#4.1. 实验设置\n我们从两个方面对 PICASSO 进行实验:\n\n在公共数据集上, 将 PICASSO 与最先进框架的性能进行基准测试.\n利用生产就绪的数据集和三个代表性模型评估 PICASSO 的设计.\n\n表 1 总结了我们的测试平台, 包括用于性能基准测试的来自阿里云 (Gn6e) 的公共机器, 以及用于系统设计评估的本地 Tesla-V100 (EFLOPS) 集群.\n测试模型与数据集:\n\nDLRM 是由 Facebook 提出并被 MLPerf 采用的基准模型;\nDeepFM 源自 Wide&amp;Deep 模型, 广泛应用于工业推荐系统;\nDIN 和 DIEN 是两个利用复杂特征交互模块训练多字段分类数据的模型.\n我们还利用 2 中讨论的三个代表性模型进行系统设计评估.\n\n表 1. 测试平台规格 (每节点)\n\n\n\n集群\nCPU\nGPU\nDRAM\n网络\n\n\n\n\nGn6e\nXeon 8163 (96 核)\n8× Tesla V100-SXM2 (256GB HBM2)\n724GB DDR4\n32Gbps (TCP)\n\n\nEFLOPS\nXeon 8269CY (104 核)\n1× Tesla V100S-PCIe (32GB HBM2)\n512GB DDR4\n100Gbps (RDMA)\n\n\n\n对于 基准数据集, 我们收集了:\n\nCriteo: 一个由 Kaggle 和 MLPerf 采用的点击率 (CTR) 数据集\nAlibaba: 一个开源的工业级 CTR 数据集.\n\n对于系统设计评估, 我们使用阿里巴巴内部的生产数据集, 该数据集包含大量单热或多热分类特征.\n这些数据集的统计信息见表 2.\n数据集部署在远程服务器上, 可通过网络下载.\n遵循常见的工业设置, 模型仅对整个数据集遍历一个 epoch, 并采用全精度训练以避免精度损失.\n最先进的训练框架:\n我们评估并比较了 PICASSO 与主流开源 WDL 训练框架的性能, 包括:\n\n采用异步参数服务器 (PS) 训练策略 (CPU 上运行一个 PS, GPU 上运行多个工作节点) 的 1.15 版本 的 Tensorflow-PS (简称 TF-PS). NVLink 在此训练模式下无法工作;\nFacebook 开发的、针对 WDL 模型采用混合训练策略的 1.8 版本 的 PyTorch, 该策略使用 AllToAll 通信 (基于 NCCL).\n根据大小, 面向不同特征域的嵌入表被手动放置在不同的 GPU 上;\n在 PyTorch 分布式数据并行 (DDP) 模式下采用 Allreduce 通信的 Horovod.\n\n我们在实验中使用一组评估指标进行综合测量:\n\nAUC 是评估准确性的标准 CTR 指标;\n性能 是指训练系统的吞吐量 (每节点每秒实例数 (IPS) 和训练运行时间 (GPU 核心小时数);\nGPU SM 利用率 是指在至少一个 warp 在多处理器上处于活动状态的时间比例, 对所有 SM 取平均值;\n带宽利用率 是指测得的网络 (PCIe / NVLink / RDMA) 带宽.\n\n我们使用 NVIDIA 的 DCGM 来检查测试平台上设备利用率的指标.\n表 2. 实验中数据集的统计信息\n\n\n\n数据集\n实例数\n数值特征数量\n稀疏特征字段数量 (括号内为序列特征长度)\n模型\n嵌入维度\n参数数量\n\n\n\n\nCriteo\n4B\n13\n26\nDLRM / DeepFM\n128\n6B\n\n\nAlibaba\n13M\n0\n1,207 (7 + 12×100)\nDIN / DIEN\n4\n6B\n\n\nProduct-1\nInfinite\n10\n204\nW&amp;D\n8–32\n160B\n\n\nProduct-2\nInfinite\n0\n1,834 (334 + 30×50)\nCAN\n8–200\n1T\n\n\nProduct-3\nInfinite\n0\n584 (84 + 10×50)\nMMoE\n12–128\n1T\n\n\n\n表 3. 四种训练系统训练模型的 AUC\n\n\n\n模型\nPIC AUC (步数)\nPICASSO AUC (步数)\nPyTorch AUC (步数)\nTF-PS AUC (步数)\nHorovod AUC (步数)\n\n\n\n\nDLRM\n0.8025 (42K)\n0.8025 (7K)\n0.8024 (6K)\n0.8025 (10K)\n—\n\n\nDeepFM\n0.8007 (30K)\n0.8007 (7K)\n0.8007 (7K)\n0.8007 (8K)\n—\n\n\nDIN\n0.6331 (32K)\n0.6329 (20K)\n0.6327 (16K)\n0.6329 (24K)\n—\n\n\nDIEN\n0.6345 (32K)\n0.6344 (16K)\n0.6340 (12K)\n0.6343 (24K)\n—\n\n\n\n#4.2. 基准评估 (RQ1)\n我们首先在基准测试任务上评估了PICASSO在单个Gn6e集群节点上的性能.\n我们在保持模型精度的同时, 调整各框架的 Batch Size 以实现最佳吞吐量.\n精度与吞吐量.\n各GPU设备对应的 Batch Size 及AUC值列于表 3 中.\n对于DLRM和DeepFM, PICASSO实现了与PyTorch和Horovod相同的AUC, 且优于TF-PS的异步训练.\n对于DIN和DIEN, PICASSO甚至获得了比其他框架略高的精度, 这对工业实践具有指导意义.\n就吞吐量而言, 图10记录了四个框架下模型的训练墙钟时间.\nTF-PS在四个框架中表现最差, 归因于服务器节点和工作节点之间大量的数据交换以及PCIe拥塞.\n得益于集合通信原语 (即Allreduce和AllToAll) 的使用, Horovod和PyTorch的性能相比TF-PS有了显著提升.\nPICASSO展现了最佳性能, 且在DIN和DIEN上的优势更为显著, 这是因为其工作负载模式相对复杂 (内存密集型和计算密集型层的混合; 且阿里巴巴数据集比Criteo数据集具有更高的稀疏性).\n结果表明, 与基基准框架 (TF-PS) 相比, PICASSO令人印象深刻地将训练速度至少提升了1.9倍, 最高可达10倍.\n\n图 10. 通过比较训练系统, GPU 核心小时数中四个模型的 Walltime.\n\n图 11. 四个比较系统在整个过程中 SM 对训练 DLRM 的利用情况的 CDF.\n\n图 12. 四个比较系统对训练 DLRM 的 PCIe 和 NVLink 带宽消耗.\n硬件利用率.\n接着, 我们研究了训练DLRM模型时底层硬件的运行时利用率, 并在图11和图12中以10毫秒的粒度绘制了GPU SM利用率及NVLink/PCIe带宽消耗情况.\n尽管其他框架优化了训练的某些阶段 (例如, PyTorch和Horovod呈现出间歇性的高GPU SM利用率), 但它们仍受制于某些瓶颈, 这从低GPU SM利用率的大面积累积分布函数 (CDF) 区域中可以看出.\n相比之下, PICASSO几乎不存在低GPU SM利用率的区域, 这意味着该测试平台上的瓶颈已通过软件系统优化得到有效解决.\n就带宽利用率而言, PICASSO远优于TF-PS基准, 因为它利用了集合通信原语以及通过NVLink实现的硬件一致性.\n与Horovod和PyTorch相比, PICASSO仍因交错流水线而略微提升了带宽使用率.\n硬件利用率的提升表明, 尽管通用框架已将计算效率推向峰值水平, PICASSO仍成功释放了底层硬件资源的潜力.\n#4.3. 系统设计评估 (RQ2)\n就模型架构和数据分布而言, 工业级WDL工作负载通常比基准测试工作负载复杂得多.\n我们通过 W&amp;D 研究了 PICASSO 在工业服务中的有效性,\n表 4. PICASSO 消融研究\n\n\n\n模型\n配置\nPCIe (GBps)\n通信 (Gbps)\nSM 利用率 (%)\n\n\n\n\nW&amp;D\nPICASSO\n22,825\n1.57\n32\n\n\nW&amp;D\n无打包\n17,827\n1.54\n23\n\n\nW&amp;D\n无交错\n16,218\n1.49\n21\n\n\nW&amp;D\n无缓存\n19,264\n1.51\n25\n\n\nCAN\nPICASSO\n12,218,218\n2.59\n62\n\n\nCAN\n无打包\n8,769\n2.55\n45\n\n\nCAN\n无交错\n7,957\n2.02\n43\n\n\nCAN\n无缓存\n10,829\n2.60\n51\n\n\nMMoE\nPICASSO\n2,546\n2.31\n98\n\n\nMMoE\n无打包\n2,270\n2.27\n96\n\n\nMMoE\n无交错\n1,319\n1.87\n64\n\n\nMMoE\n无缓存\n2,401\n2.28\n98\n\n\n\nCAN 和 MMoE 模型在工业数据集上的表现.\n同时, 我们通过训练吞吐量来衡量打包、交错和缓存的细粒度贡献.\n除非另有明确说明, 评估均在 EFLOPS 集群的 16 个节点上进行.\n我们使用阿里巴巴 Tensorflow 中常用的异步 PS 策略作为基线.\n我们还实现了不包含软件系统优化的 PICASSO, 记为 “PICASSO(Base)”, 这可以被视为一个纯混合并行训练策略框架.\n图 13 展示了 IPS, 我们观察到在 CAN 和 MMoE 上实现了 4 倍的加速.\n随后, 我们通过消融实验深入研究软件系统优化.\n消融实验.\n我们依次从PICASSO中移除软件系统优化, 以验证其对WDL任务的影响, 并在表IV中收集指标.\n通过使用打包方法, 特征嵌入上的零碎操作被打包在一起, 使得IPS提高了30%, 并相应地增加了PCIe、网络和GPU SM的硬件使用率.\n交错方法利用流水线通过计算密集型操作来隐藏内存访问和网络延迟.\n显然, 在这三个模型中, MMoE拥有最复杂的计算工作负载, 因此从该优化中受益最多.\n交错方法将MMoE的性能显著提高了93%.\n这与这两个模型承受繁重计算负载的分析一致, 且PICASSO有助于在整个训练过程中扩散脉冲式的GPU使用率.\n由于计算资源不足, W&amp;D 可受益于数据交错以缓解 PCIe 和网络上的拥塞.\n缓存依赖于利用输入数据的分布.\n因此, 我们运行 100 步作为预热以收集统计数据, 然后将 GPU 内存上的 Hotstorage 大小设置为 1GB, 以保持每个批次内唯一 ID 的命中率高于 20%.\nHybridHash 通过平衡 PCIe 和 GPU 的利用率将性能提高了高达 13%.\n打包的有效性.\nWDL 模型倾向于拥有针对多字段嵌入和特征交互的碎片化操作.\n我们通过 Baseline 和 PICASSO 转储了三个模型的计算图, 操作数量和打包嵌入如表 5 所示.\n统计数据显示, PICASSO 显著减少了碎片化操作, 即使交错优化补充了一定数量的操作以流水线化执行.\n表 5. 计算图中的操作数量\n\n\n\n模型\n操作数量 (基线)\n操作数量 (PICASSO)\n打包嵌入数量 (基线)\n打包嵌入数量 (PICASSO)\n\n\n\n\nW&amp;D\n100,039\n14,882 (14.9%)\n204\n16 (7.8%)\n\n\nCAN\n381,364\n67,985 (17.8%)\n364\n19 (5.2%)\n\n\nMMoE\n300,524\n75,217 (25.0%)\n94\n11 (11.7%)\n\n\n\n\n图 13. 内部集群中三个训练系统对这三个模型的训练表现.\n\n图 14. 通过使用 1 到 11 个交错组来训练表现. 这三款型号分别拥有 16、19 和 11 个填充嵌入.\n\n图 15. PICASSO 规模扩展的范围为 1 至 128 个集群 Worker.\n交错组.\n交错组的数量影响嵌入层的效率.\n图14展示了通过改变K交错组数量所得到的吞吐量.\n显然, 通信密集型工作负载, 即 W&amp;D 和 CAN, 可以从增加的交错中受益, 因为打包嵌入的组合统一了每个硬件资源的使用.\n我们还观察到, 批量交错对不同模型的贡献各不相同.\n结果表明, 利用更多 Micro Batch 将显著提高计算密集型工作负载 (即 CAN 和 MMoE) 的性能, 通过满足 GPU 的饱和度.\n这揭示了当存在充足的输入数据和硬件利用率不足时, 交错策略对 WDL 工作负载是有效的.\n热存储大小.\n在工业级 WDL 工作负载中, 无法预见嵌入表的大小, 因为模型需要不断处理新出现的类别特征 ID.\n我们在之前的评估中将热存储的大小设置为 1GB, 以确保 20% 的命中率.\n表 6 描述了通过改变热存储大小而得到的缓存命中率以及 IPS 的提升.\n更大的缓存大小可以承载更多的嵌入, 但我们发现当缓存大小达到 2GB 以上时, 命中率呈现出明显的边际效应.\n尽管大缓存命中了更多的 ID 查询, 但 GPU 内存的占用迫使训练在批次大小上做出妥协, 从而导致整体吞吐量略有下降.\n因此, 在 WDL 工作负载中, 无需通过设置过大的缓存大小来追求高缓存命中率.\n表 6. 通过改变热存储大小观察到的命中率与 IPS\n\n\n\nHot-Storage\nW&amp;D Hit Ratio\nW&amp;D IPS\nCAN Hit Ratio\nCAN IPS\nMMoE Hit Ratio\nMMoE IPS\n\n\n\n\n256MB\n9%\n-11%\n20%\n-19%\n9%\n-3%\n\n\n512MB\n18%\n-5%\n28%\n-10%\n16%\n-1%\n\n\n1GB\n24%\n+0%\n37%\n+0%\n21%\n+0%\n\n\n2GB\n28%\n+1%\n44%\n+5%\n24%\n+0%\n\n\n4GB\n31%\n-3%\n45%\n+2%\n27%\n-2%\n\n\n\n扩展.\n我们将训练集群从一个 PICASSO-Executor 扩展到 128 个 PICASSO-Executor, 并在图 15 中通过 IPS 展示了性能.\nIPS 与 PICASSO-Executor 数量之间的相关性表明, PICASSO 在 CAN 和 MMoE 上实现了近线性扩展性, 而在 W&amp;D 上达到了亚线性吞吐量.\n这一结果意味着 PICASSO 可以分摊由 PICASSO-Executor 数量增加带来的额外通信开销, 并处理大规模的 WDL 训练.\n#D. 适用性实验 (RQ3)\n变化的特征交互.\n我们进一步研究了PICASSO在具有多种特征交互模块的更大规模的工业级WDL模型上的性能.\n我们选取了12个AUC预测模型, 并调整超参数以确保模型收敛.\n这些模型经过轻微修改以适应Product-2数据集.\n为了展示PICASSO的适应性, 我们采用内部优化的XDL作为基线, 在同步PS训练模式下训练这些模型.\n这12个模型的性能列于表 7 中.\n显然, 与内部优化的 XDL 框架相比, 所提出的PICASSO显著显著提高了 GPU 利用率.\n这表明, 通过软件系统优化, PICASSO 能够感知硬件限制, 并针对各种 WDL 模型架构释放硬件潜力.\n表 7. PICASSO与内部XDL训练12个AUC预测模型的吞吐量.\n\n\n\n模型\nBatch Size\nGPU SM 利用率\nIPS\n\n\n\n\nLR\n20K → 36K (20K × 2)\n9 → 22 (+144%)\n12.0K → 25.9K (+115%)\n\n\nW&amp;D\n19K → 36K (18K × 2)\n21 → 35 (+67%)\n14.7K → 22.2K (+50%)\n\n\nTwoTowerDNN\n12K → 36K (12K × 3)\n35 → 97 (+177%)\n4.7K → 12.1K (+160%)\n\n\nDLRM\n10K → 30K (10K × 3)\n38 → 98 (+158%)\n3.8K → 10.4K (+171%)\n\n\nDCN\n14K → 36K (12K × 3)\n56 → 92 (+64%)\n9.0K → 13.7K (+52%)\n\n\nxDeepFM\n6K → 20K (5K × 4)\n45 → 98 (+117%)\n3.1K → 5.9K (+89%)\n\n\nATBRG\n3K → 6K (3K × 2)\n13 → 26 (+100%)\n0.8K → 1.4K (+82%)\n\n\nDIN\n15K → 45K (15K × 3)\n34 → 80 (+135%)\n7.5K → 16.0K (+113%)\n\n\nDIEN\n15K → 45K (15K × 3)\n29 → 75 (+159%)\n7.3K → 15.6K (+115%)\n\n\nDSIN\n9K → 27K (9K × 3)\n40 → 93 (+133%)\n4.7K → 9.8K (+111%)\n\n\nCAN\n12K → 48K (12K × 4)\n17 → 75 (+341%)\n3.9K → 12.1K (+210%)\n\n\nSTAR\n2K → 8K (2K × 4)\n32 → 98 (+206%)\n0.6K → 2.0K (+215%)\n\n\n\n表 8. CAN在合成数据集上通过改变特征字段数量的性能\n\n\n\n系统\n指标\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nPICASSO\n原始值\n12.20\n6.14\n4.13\n3.13\n2.50\n2.09\n1.82\n1.61\n\n\nPICASSO\nAP\n12.20\n6.10\n4.07\n3.05\n2.44\n2.03\n1.74\n1.53\n\n\nPICASSO\n增量\n0.0%\n+0.6%\n+1.7%\n+2.5%\n+2.6%\n+2.7%\n+4.3%\n+5.3%\n\n\nXDL\n原始值\n2.40\n1.18\n0.75\n0.56\n0.42\n0.36\n0.31\n0.25\n\n\nXDL\nAP\n2.40\n1.20\n0.80\n0.60\n0.48\n0.40\n0.34\n0.30\n\n\nXDL\n增量\n0.0%\n-1.5%\n-6.8%\n-6.1%\n-13.1%\n-9.6%\n-10.3%\n-15.3%\n\n\n\n增加特征字段.\n我们通过改变特征字段的数量来展示PICASSO的性能.\n由于目前我们不运营需要处理数千个特征字段业务, 我们通过复制Product-2的特征字段来构建一个合成数据集.\n因此, 数据源中的特征字段数量变为364的整数倍.\n相应地, 我们复制特征交互层以处理该合成数据集.\n特征交互层的输出被拼接在一起, 输入到一个共享MLP中.\n表 8 展示了PICASSO和内部优化的XDL的IPS, 以及基于等差数列 (AP) 计算的理论IPS.\n尽管底层硬件资源的需求随特征字段数量的增加而增加, 但由于对零散操作的打包, PICASSO的性能略优于AP.\n相比之下, PS基线因大量特征字段及其组成的特征交互导致海量操作, 从而相比AP表现出显著的IPS下降.\n讨论.\n评估结果表明, PICASSO成功释放了硬件资源的潜力, 其中PICASSO通过统一硬件使用来实现高整体硬件利用率 (RQ2), 并为具有不同属性的 WDL 模型提供多样化的优化 (RQ3).\n在训练中采用超大 Batch Size 需要某些辅助方法 (例如, 全局批量归一化, Lamb 优化器), 这些方法可以通过在 TensorFlow 中的实现应用于 PICASSO.\n显然, 如果我们部署定制化硬件以增强特定的硬件资源, 或者如果特定的 WDL 任务对精度损失机制导致的准确率损失表现出较高的容忍度, 那么 PICASSO 的性能肯定会进一步提高.\n表 9. PICASSO 在阿里云上的性能表现\n\n\n\n系统\n平均任务实际运行时间 (h)\nGPU SM 利用率 (%)\n带宽 (Gbps)\n\n\n\n\nXDL\n8.6\n15\n1.412 (TCP)\n\n\nPICASSO\n1.4\n75\n6.851 (TCP + RDMA)\n\n\n\n#5. 生产环境部署\n我们在Tensorflow之上实现了PICASSO.\n自2020年10月以来, 它已部署在我们的本地集群中, 用于服务线上和线下WDL任务的业务, 包括信息检索、广告竞价、推荐和搜索排序.\n自部署PICASSO以来, 训练吞吐量得到了显著提升, 并在多次重点促销活动中取得了令人瞩目的性能表现.\nPICASSO采用了内部复杂的调度和故障恢复策略以实现稳健的训练, 这些内容超出了本文的讨论范围.\n训练最先进的 WDL 设计时的算术计算开销仍然非常大.\n得益于流行的计算机视觉 (CV) 和自然语言处理 (NLP) 训练加速方法, 我们将最新的解决方案 (如 GPU 加速库 (例如 CUTLASS 和 CuDNN)、算子级图替换、编译器优化以及量化通信) 应用于 PICASSO.\n我们还实现了拓扑感知通信, 以避免来自同一节点的 GPU 设备上的 IO 任务争用有限的 NIC 资源.\n这些加速技术与 PICASSO 的优化是正交的.\n我们为用户提供了灵活的接口, 以便在调优其设计时调用这些方法.\n其他新兴技术可以通过 Tensorflow 生态系统集成到 PICASSO 中.\n我们配备了一个生产训练集群, 每个工作节点配备一块Tesla-V100 GPU, 该集群每天运行数百个WDL工作负载.\n这些工作负载由于输入特征、嵌入维度、特征交互模块和 MLP 形状的不同, 呈现出显著差异的训练强度.\n我们记录了从2021年6月1日至2021年11月15日期间成功训练任务的作业统计数据.\n为了进行比较, 我们在另一个具有可比工作负载类型的生产集群中部署了此前使用的XDL.\n表 9 中的结果显示, PICASSO平均带来了约6倍的性能加速, 并有助于提高底层硬件的利用率.\n吞吐量加速的效益将每日持续交付的延迟平均缩短了 7 小时.\n我们进一步探查了受监控集群中的几个代表性模型 (具有完全不同的模型架构和数据分布), 并展示了在 128 块 Tesla-V100 上训练一年累积的 PB 级数据所需的墙时, 如表 10 所示.\n统计数据显示, PICASSO 将千亿级参数的模型训练时间从一个月缩短到了 2 天.\n此外, 对于具有万亿级参数的 WDL 模型 (我们业务中满足实时推理吞吐量需求的当前最大模型之一), 训练在九天内完成, 而基线框架估计需要占用资源三个多月.\n这种训练加速对于提供高 WDL 业务价值的最新机器学习/深度学习趋势至关重要.\n表 10. 训练一年累积数据所需的墙时 (GPU 核心小时) (&quot;P&quot;表示预测的墙时)\n\n\n\n系统\\模型规模\n∼ 1B\n∼ 10B\n∼ 100B\n∼ 1T\n\n\n\n\nXDL\n2,072\n11,013\n88,129 (P)\n323,480 (P)\n\n\nPICASSO\n747\n2,285\n6,091\n27,256\n\n\n\n#六、相关工作\n我们将针对训练WDL作业的前沿研究方法归纳为以下三类:\n硬件定制. 对于具有高业务价值的特定WDL工作负载, 定制硬件本身以实现卓越的性能和吞吐量是具有效益的.\n\nAIBox/PaddleBox 利用非易失性内存, 将训练规模从包含数百个CPU的MPI集群大幅缩减至配备8个GPU的单机.\nHugeCTR/Merlin 是运行在NVIDIA DGX-1/DGX-2超级节点上的定制框架, 这些节点配备了名为NV-Switch的高端互连技术.\nZion 和 RecSpeed 通过增加更多网卡和RoCE来缓解I/O瓶颈, 从而针对DLRM 及其变体定制节点规格.\n\n然而, 面对WDL设计的快速演变, 硬件定制依然成本高昂, 且会造成资源浪费.\n此外, 基于定制硬件构建的训练系统难以在云端进行弹性扩展.\n子系统优化 旨在诊断特定瓶颈并提升特定工作负载的性能. 例如,\n\nBytePS 中的通信协议加速了参数服务器 (PS) 策略中的数据交换.\nKraken 开发了内存高效的表结构以存储嵌入层的参数.\nScaleFreeCTR 利用GPU加速存储在DRAM中的参数的嵌入查找.\nHet 在嵌入更新中引入了陈旧性, 这适用于具有小规模本地嵌入表的WDL设计.\n\n由于未能充分感知WDL的稀疏操作或密集计算特性, 这些优化措施往往错失系统性提升整体性能的机会;\n而诸如陈旧性等有损精度的操作则会对电商WDL模型造成损害.\n通用DNN训练优化. 目前已提出多种针对CV和NLP等领域密集模型的训练框架.\n这些框架为训练过程中的工作负载拆分和流水线处理提供了精细的策略.\n\nMegatron 加速了NLP工作负载中的Transformer模块.\nGPipe 在小批次上实现了流水线处理, 而 Pipedream 则通过权重缓存进一步填补了前向传播和反向传播之间的气泡.\nGShard 依赖于编译方法来对参数和激活值进行分片.\n\n遗憾的是, WDL模型通常对数值精度和梯度陈旧性较为敏感, 且WDL工作负载在动态形状数据上的算子数量远多于CV和NLP模型.\n因此, 这些通用DNN训练优化可能并不适用于工业规模的WDL工作负载.\n#七、结论\n在本文中, 我们介绍了PICASSO, 这是一个基于TensorFlow的深度学习训练系统, 旨在通过感知模型架构和数据分布, 加速在商用硬件上对WDL模型的训练.\n通过调研阿里巴巴云上的代表性工作负载, 我们设计了一个优势训练框架, 并提供了感知工作负载的软件优化: 1) 打包嵌入表和后续操作, 以减少对以GPU为中心的训练不友好的碎片化操作; 2) 交错嵌入层和特征交互, 以将脉冲式使用扩散到整个训练过程中; 3) 缓存频繁访问的分类ID, 以加速重复的嵌入查询.\n在阿里巴巴云的产品部署表明, 一个通过一年PB级数据训练的万亿参数WDL模型可以在27,256 GPU核心小时内高效完成训练, 将训练成本显著降低了12倍.\nPICASSO有助于将每日持续交付的延迟缩短7小时, 这对最先进的推荐系统至关重要.\n","categories":["mlsys"],"tags":["MLSys","Distributed Machine Learning","System"]},{"title":"[OSDI'14] Scaling Distributed Machine Learning with the Parameter Server 阅读笔记","url":"/2025/03/13/scaling-distributed-machine-learning/","content":"沐神投稿在 OSDI '14 的论文, 提出了第三代 Parameter Server 框架.\n沐神在 B 站有亲自讲解这篇论文的视频, 可以参考: 参数服务器 (Parameter Server)逐段精读【论文精读】.\n#0. 摘要\n本文提出了一个 Parameter Server 框架, 用于解决分布式机器学习的问题.\n数据和负载分布在多个计算节点 (Worker Node) 上, 服务节点 (Server Node) 维护全局共享的参数,\n参数表示成稠密或稀疏的向量和矩阵.\n框架管理节点之间的异步数据通信, 支持灵活的一致性模型, 弹性的可扩展性和持续的容灾.\n为了展示该框架的可扩展性, 文中使用了从稀疏 LR 到 LDA 和 Distributed Sketching 等的任务, PB 级别的真实数据、十亿级别的数据样本和参数的规模上的实验结果.\n#1. 介绍\n分布式优化和推理正在成为解决大规模机器学习问题的先决条件.\n真实世界的训练数据量可以达到 1TB 到 1PB. 这使得我们创建一个具有 10910^9109 (1B) 到 101210^121012 (100B) 参数量的强大复杂模型.\n通常这些模型被所有的计算节点共享, 计算节点还需要经常访问共享参数并进行更新.\n共享带来三个挑战:\n\n通信: 所有的计算节点都需要频繁地访问参数, 会导致大量的通信.\n性能: 很多机器学习算法是顺序的模型 (算完一个 batch 再继续算下一个), 会引入大量同步机制, 损伤性能.\n容灾: 训练任务应该在机器故障、软件错误或者由于人为原因被占用的情况下, 不会停止.\n\n为了更好地说明第三点, 文章收集了一家互联网公司数据中心 (Baidu) 最近三个月的训练作业日志. 统计结果显示, 规模越大的任务的失败率越高.\n和实验室环境相比, 工业界环境有资源竞争, 因此必须要考虑容灾问题.\n#1.1 贡献\nPS 思想最早可以源自于 Alexander Smola 自 2010 年提出的 YahooLDA 框架.\nYahooLDA 采用了一个分布式的 memcached 服务存储机器学习参数.\n第二代 Distbelief 是 Google 提出, 使用独立的 PS 参数存储巨大的深度学习模型的参数.\nPS 框架已经在学界和工业界广泛使用, 本文描述了第三代 PS 框架的开源实现, 提供了一个更加通用的设计.\n本文重点关注第三代 PS 框架的分布式接口, 对开发者来说, 第三代框架有两个好处:\n\n把通用的框架代码和特定任务相关代码分离, 保证任务相关代码的简洁性: 例如, 第三代 PS 可以同时支持稀疏 LR、LDA 和 Distributed Sketching 的一系列算法.\n提供了鲁棒、多样化且高性能的实现, 处理各种各样的算法.\n\n服务节点的管理包括节点的添加和删除.\n第三代 PS 框架的设计决策基于真实工业界系统计算负载.\n作者总结了五个关键特征:\n\n高效通信: 采用了异步的通信模型, 不会阻塞计算, 对通信数据进行压缩;\n灵活一致性模型: 核心思想是 trade-off: 舍弃掉机器学习算法部分的一些指标 (收敛速度, 精度等), 换取系统部分更好的性能;\n弹性的可扩展性: 在训练时可以动态增加和减少机器;\n容灾和持久性: 少量机器挂掉可以在 1s 内恢复, 使用向量时钟确保灾难情况下的行为;\n易用性: 当时的主流语言是 C++, 主流库是 Eigen 等, PS 框架把全局参数抽象成向量和矩阵, 支持已有的库;\n\n创新性: 巧妙地调整系统领域和机器学习领域的技术, 使之合理地结合在一起, 得到第一个通用的、能够扩容到企业规模的 ML 系统.\n#1.2 工程挑战\n解决分布式数据分析问题时, 多个计算节点需要不断地读和写全局参数.\nPS 框架提供了一个高效的机制来在多个计算节点间汇聚和同步这些参数和一些统计信息.\n由于模型的总体参数量很大, 每个服务节点 (Server Node) 都只负责其中的一部分.\n计算节点通常会先朝服务节点索要一部分参数, 进行计算, 然后再把计算结果发回服务节点.\n构建高性能的 PS 系统的关键挑战在于:\n\n\n通信: 传统的 datastore 采用 key-value 模型, 但是这种抽象往往不适用于机器学习场景: 机器学习场景的 value 通常是很小的 floats 或者 integers, 而每个更新都进行一次数据通信的开销很大.\nPS 框架的设计是, 计算节点每次只发送向量或者矩阵的一部分 (vector 的 segment, 或者 matrix 的 row).\n这样就可以批量地更新参数, 并允许高效地实现一致性跟踪.\n\n\n容灾: PS 框架采用 live replication 的方式, 将参数实时复制到多台机器上, 并且支持 hot failover.\n机器的加入和离开会被视为维修和故障.\n服务节点的管理包括节点的添加和删除.\n\n\n#1.3 相关工作\n第一代 PS 包括 YahooLDA, 利用通用的 memcached 数据库作为同步机制, 因此缺乏弹性和性能;\n第二代 PS 包括 Distbelief 和 Petuum, 引入了 Bounded delay model, 以及做了一系列工程优化.\n对比通用分布式系统, Mahout (基于 Hadoop)和 MLI (基于 Spark)采用迭代式的 MapReduce 框架, 要求同步和迭代式的通信, 不适用于大规模的机器学习任务.\nGraphLab 使用图模型, 可扩展性差; Piccolo 缺乏消息压缩、复制和灵活一致性模型.\n#2. 机器学习背景知识\n相关概念介绍: 特征提取, 目标函数, 学习过程, 损失最小化, 学习率, 特征向量, 生成模型, 主题建模, LDA,\n以分布式梯度下降任务为例: 包括任务调度器, 计算节点, 服务节点三个部分.\n\n计算节点: LoadData(), WorkerIterate()\n服务节点: ServerIterate()\n\n\n#3. PS 架构\n一个 PS 示例包含四大元素: 一个 Server Group, 若干个 Worker Group, 一个 Resource Manager 以及 Training Data.\nWorker Group 负责运行一个任务, 包含一个 Task Scheduler 和若干个 Worker Node.\nServer Group 负责存储参数, 包含一个 Server Manager 和若干个 Server Node.\n分多个组的目的是让系统可以同时执行多个任务, 例如同时训练多个模型或者同时训练和推理 (一个组用于在线服务, 另一个组周期性地更新模型). 多个任务通过命名空间隔离.\n#3.1 (Key, Value) 向量\nPS 框架中的参数可以表示成一个 (Key, Value) 向量, 其中 Key 是一个整数, Value 是整数、浮点数或向量.\n这种表示法使得用户可以使用现有的线性代数库 (例如 BLAS、LAPACK 和 ATLAS) 来提高编程效率.\n#3.2 区间 Push 和 Pull\nPS 框架允许在 Push 和 Pull 时指定一个参数的区间, 只会传输区间内的参数, 这样可以减少通信开销.\n#3.3 用户自定义函数 (UDF)\nPS 框架允许用户在服务节点上执行用户定义的函数, 例如更新参数的函数, 这样更加灵活.\n#3.4 异步任务和依赖关系\n任务通过 RPC 调用来实现.\n任务是异步执行的, 发送 RPC 出去后, 发送方不会等待接收方的回复, 而是继续执行下一个运算.\n任务的回复可能是 UDF 的返回值, 或者是被请求的 (Key, Value) 对, 或者是一个空的 ACK.\n任务之间可以有依赖关系, 例如调用方可以指定一个 execute-after-finished 依赖在任务 A 和任务 B 之间, 表示任务 A 在任务 B 完成后才能执行.\n默认情况下, 任务之间是完全并行的.\n例如多次 Iteration 可以流水线式的执行, 提高效率.\n#3.5 灵活一致性模型\n通过依赖关系的不同组合方式可以实现各种不同程度的一致性模型\n\n顺序一致性 (Sequential): 所有任务都有依赖, 即 Bulk Synchronous Processing, BSP\n最终一致性 (Eventual): 任务间完全没有依赖, 仅推荐用于算法足够 robust 的情况\n有界延迟 (Bounded delay): 第 n 个任务依赖 n−τn-\\taun−τ 之前的所有任务\n\n所有依赖关系会形成一个有向无环图(DAG).\n依赖关系可以是动态的.\n#3.6 用户自定义过滤器\n用户可以定义过滤器, 来选择性地同步部分参数数据, 达到节省通信的目的.\n例如显著修改过滤器, 只推送修改超过阈值的条目.\n#4. 实现\n本章讲解了 PS 框架的具体实现细节.\n服务节点使用一致性哈希来存储参数.\n使用链式复制进行多副本.\n在数据和向量时钟上进行了压缩.\n#4.1 向量时钟 &amp; 4.2 通信消息\n消息的格式是一个向量时钟和若干个 (Key, Value) 对.\nServer 端会缓存区间的哈希, 当发送相同区间的时候, 可以只发送 Value 部分, 节约通信开销.\n由于机器学习场景会有很多零值, PS 框架使用可以删除零值的 Snappy 库来压缩消息.\n#4.3 一致性哈希\n利用经典的 DHT 技术实现动态地增加和减少服务节点, Key 和节点 ID 都被插入到哈希环中.\n为了平衡节点间的负载, 每个服务节点会创建多个虚拟节点.\n#4.4 复制和一致性\n除了负责自己的参数以外, 每个服务节点额外为哈希环上 k 个逆时针邻居的参数做副本. 该节点称为相应这些键的从节点.\n计算节点只会和主节点通信, 对主节点的更新会被同步地复制到从节点 (即 全同步复制).\n直接复制需要 k 倍的通信放大, PS 框架允许通过对多个更新先进行一次聚合, 然后再进行复制来减少通信开销.\n#4.5 服务节点管理\n服务节点的管理包括节点的添加和删除.\n当一个新节点加入时, 会进行一个重分配过程:\n\nServer Manager 分配给新节点一段 Key Range. 这可能会导致另一个服务范围从终止的节点中分裂或被删除.\n新节点获取这段 Key Range 的数据, 还有接下来 k 个逆时针邻居的数据.\nServer Manager 广播节点更改消息. 消息的接收者可能会缩小自己的数据, 和将未完成的任务重新提交给新节点.\n\n步骤 2, 从源节点 S 获取范围 R 中的数据, 使用了一个两阶段协议.\n首先, S 预复制范围 R 内的所有 (key, value) 对及其关联的向量时钟. 这可能会导致向量时钟分裂. 如果新节点在这一阶段失败, 则 S 保持不变.\n在第二阶段, S 不再接受影响键范围 R 的消息, 并丢弃未执行和回复的消息 (开启禁写).\n同时, S 向新节点发送预复制阶段期间发生在范围 R 的所有更改.\n步骤 3, 在收到节点更改消息后, 一个节点 N 首先检查它是否维护着键范围 R.\n如果为真, 并且后面不再需要由 N 维护, 则删除 R 中的所有数据和向量时钟.\n接下来, N 扫描所有未收到回复的传出消息. 如果一个键范围与 R 相交, 则该消息将被拆分并重新发送.\n由于网络延迟、失败和丢失的确认, N 可能会发送两次消息.\n由于使用了向量时钟, 原始接收者和新节点都可以拒绝此消息, 并且不会影响正确性.\n服务节点的删除 (自愿或由于失败) 和加入过程类似. Server Manager 将新节点分配给离开节点的 Key Range.\nServer Manager 通过心跳信号检测节点故障.\n与现有集群资源管理器, 例如 Yarn 或 Mesos 的集成将留给未来的工作.\n#4.6 计算节点管理\n节点 W 加入的过程:\n\nTask Scheduler 给 W 分配一部分训练数据;\nW 从 NFS 或者其他计算节点加载这些数据. 训练数据通常是只读的, 所以可以被多个计算节点共享. 然后 W 在从服务节点获取参数;\nTask Scheduler 广播这个消息, 其他节点可能会释放一些训练数据.\n\n当删除一个计算节点时, 任务调度程序可以开始替换.\n我们给算法设计师提供控制恢复的可选项是出于两个原因:\n\n如果训练数据量很大, 则恢复一个计算节点可能比恢复服务节点更昂贵.\n在优化过程中丢失少量训练数据通常只会影响模型很小一部分. 因此, 算法设计师可能宁愿不更换失败的计算节点继续下去. 甚至终止最慢的计算节点也是可以的.\n\n#5. 评估\n#5.1 稀疏 LR (1k 台机器)\n636 TB 的广告点击预测数据集, 现在看来也非常大了.\n#5.2 LDA (Google, 6k 台机器)\n#5.3 Sketches (15 台机器)\n计算 CountMin Sketches\n#参考资料 &amp; FAQ\n#1. PS 架构和 TensorFlow, PyTorch 的关系?\nPS 架构是一种分布式机器学习的系统架构模式, 核心思想是:\n\nWorker: 负责计算 (前向、反向、梯度)\nParameter Server (PS): 负责存储和更新模型参数\nWorker 与 PS 通过网络通信 (pull 参数 / push 梯度)\n\n这是一个通用思想, 不绑定任何具体框架.\nTensorFlow 是最早大规模、原生支持 PS 架构的深度学习框架之一.\n在 TensorFlow 框架中, 我们可以定义独立的参数节点, 并且将它们放置在独立的 PS 机器上.\n在 TensorFlow 1.x (尤其是 1.0~1.15) 中:\n\nTensorFlow 的分布式执行模型天然适配 PS\n显式区分:\n\nps 节点: 放变量 (tf.Variable), 通过 tf.device(&quot;/job:ps/task:i&quot;) 放置在 PS 上\nworker 节点: 放计算图\n\n\n通信基于 gRPC\n\n在 TensorFlow 2.x 中:\n\n弱化 PS，转向 AllReduce\n官方主推 tf.distribute默认策略:\n\nMirroredStrategy\nMultiWorkerMirroredStrategy\n\n\n核心是 AllReduce (如 NCCL、Ring-AllReduce)\n\nPyTorch 框架原生强支持 Data Parallel + AllReduce 的模式, 原因是动态图 + eager execution:\n\n参数就是本地 Python 对象\nAutograd 在反向阶段 hook 梯度\n梯度 ready -&gt; 立即触发通信\n\n这使得:\n\n梯度通信可以 overlap with backward\n实现高效、直观\n\n但是对于 TB 级别 Embedding 的场景, 还是离不开 PS 架构.\n#2. 深度学习时代有什么变化?\n深度学习的模型通常是稠密的, 参数量小, 计算量大.\n因此瓶颈往往在计算上, 而不是通信上. 本文的 PS 框架主要是针对稀疏模型的.\n#3. PS 框架的缺点\n来自 Horovod 的评论:\n\n难以确定计算节点和计算节点的正确比例: 如果只有一个计算节点, 它可能会成为通信或者计算的瓶颈. 如果使用多个计算节点, 就变成了 All-to-all 通信, 可能会导致网络饱和,\n应用程序复杂性: 每个应用程序都需要启动计算节点和服务节点, 传递地址和端口号, 等等复杂操作, 占用了实际建模的时间.\n\n","categories":["mlsys"],"tags":["MLSys","Parameter Server","Distributed Machine Learning","System"]},{"title":"[Arxiv'16] TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems 阅读笔记","url":"/2025/08/31/tensorflow-large-scale-machine-learning/","content":"#0. 摘要\nTensorFlow 是一种用于表达机器学习算法的接口, 以及一种用于执行这些算法的实现.\n使用 TensorFlow 表达的计算可以在各种异构系统上以少量或无需修改的方式执行, 这些系统范围从手机和平板电脑等移动设备到由数百台机器和数千个计算设备 (如 GPU 卡) 组成的大规模分布式系统.\n该系统具有高度的灵活性, 可用于表达各种算法, 包括深度神经网络模型的训练和推理算法, 并且已被用于在计算机科学和其他领域超过十几个领域进行研究和将机器学习系统部署到生产中, 包括语音识别、计算机视觉、机器人技术、信息检索、自然语言处理、地理信息提取和计算药物发现.\n本文描述了 TensorFlow 接口以及我们在 Google 构建的该接口的实现.TensorFlow API 和参考实现2015 年 11 月作为 Apache 2.0 许可的开源软件包发布.\n可在 &lt;www.tensorflow.org&gt; 上获取.\n#1. 引言\n谷歌大脑项目始于 2011 年, 旨在探索超大规模深度神经网络在研究和谷歌产品中的应用.\n作为该项目早期工作的一部分, 我们构建了 DistBelief, 这是我们第一代可扩展的分布式训练和推理系统, 该系统为我们提供了良好的服务.\n我和谷歌的其他研究人员使用 DistBelief 进行了广泛的研究, 包括无监督学习、语言表示、图像分类和目标检测模型、视频分类、语音识别、序列预测、围棋走法选择、行人检测、强化学习以及其他领域.\n此外, 在谷歌大脑团队的紧密合作下, 谷歌及其他 Alphabet 公司的 50 多个团队已使用 DistBelief 在各种产品中部署了深度神经网络, 包括谷歌搜索、我们的广告产品、我们的语音识别系统、谷歌照片、谷歌地图和街景、谷歌翻译、YouTube 以及其他许多产品.\n基于我们在 DistBelief 上的经验以及对训练和使用神经网络所需系统属性和要求的更全面理解, 我们构建了 TensorFlow, 这是我们用于大规模机器学习模型实现和部署的第二代系统.\nTensorFlow 将使用数据流模型描述的计算映射到各种不同的硬件平台上, 范围从在 Android 和 iOS 等移动设备平台上运行推理, 到使用包含一个或多个 GPU 卡的单机进行适度规模的训练和推理系统, 再到在包含数千个 GPU 的数百台专用机器上运行的大规模训练系统.\n拥有一个能够覆盖如此广泛平台范围的单一系统, 可以显著简化机器学习系统的实际应用, 因为我们发现为大规模训练和小规模部署分别使用不同的系统会导致显著的维护负担和抽象泄漏问题.\nTensorFlow 计算以有状态数据流图的形式表示 (详见第 2 节), 我们专注于使系统既灵活, 能够快速实验新模型用于研究目的, 又具有足够的高性能和鲁棒性, 用于机器学习模型的训练和部署.\n为了将神经网络训练扩展到更大的部署, TensorFlow 允许客户端通过复制和并行执行核心模型数据流图, 轻松表达各种并行性, 许多不同的计算设备协同合作来更新一组共享参数或其他状态.\n计算描述的适度变化, 使得各种不同的并行性方法能够以较低的努力被实现和尝试.\n一些 TensorFlow 的使用允许在参数更新的一致性方面有一定的灵活性, 我们可以在一些较大的部署中轻松表达和利用这些宽松的同步需求.\n与 DistBelief 相比, TensorFlow 的编程模型更加灵活, 其性能显著更好, 并且它支持在更广泛的异构硬件平台上训练和使用更广泛的模型.\nimport tensorflow as tfb = tf.Variable(tf.zeros())                   # 100-d vector, init to zeroesW = tf.Variable(tf.random_uniform([784,100],-1,1)) # 784x100 matrix w/rnd valsx = tf.placeholder(name=&quot;x&quot;)                       # Placeholder for inputrelu = tf.nn.relu(tf.matmul(W, x) + b)             # Relu(Wx+b)C = [...]                                          # Cost computed as a function                                                   # of Relus = tf.Session()for step in xrange(0, 10):  input =...construct 100-D input array...       # Create 100-d vector for input  result = s.run(C, feed_dict=&#123;x: input&#125;)          # Fetch cost, feeding x=input  print step, result\n图 1: 示例 TensorFlow 代码片段\n\n图 2: 图 1 对应的计算图\n我们 DistBelief 的数十个内部客户已经切换到 TensorFlow.\n这些客户依赖 TensorFlow 进行研究和生产, 任务多种多样, 从在手机上运行计算机视觉模型的推理, 到使用数百台机器在数百亿个示例记录上对具有数百亿参数的深度神经网络进行大规模训练.\n尽管这些应用主要集中在机器学习和深度神经网络, 但预计 TensorFlow 的抽象化将在各种其他领域发挥作用, 包括其他类型的机器学习算法, 以及可能的其他类型的数值计算.\n我们在 2015 年 11 月以 Apache 2.0 许可证开源了 TensorFlow API 和参考实现, 可在 &lt;www.tensorflow.org&gt; 获取.\n本文的其余部分将更详细地介绍 TensorFlow.\n第 2 节描述了 TensorFlow 接口的编程模型和基本概念, 第 3 节描述了我们的单机和分布式实现.\n第 4 节描述了基本编程模型的几个扩展, 第 5 节描述了基本实现的几个优化.\n第 6 节描述了我们使用 TensorFlow 的一些经验, 第 7 节描述了在使用 TensorFlow 时我们发现的几个有用的编程模式, 第 9 节描述了我们围绕核心 TensorFlow 系统构建的几个辅助工具.\n第 10 节和第 11 节分别讨论了未来工作和相关工作, 第 12 节提供了总结性思考.\n#2. 编程模型和基本概念\nTensorFlow 计算由一个有向图描述, 该图由一组节点组成.\n该图表示数据流计算, 并扩展了一些节点类型以维护和更新持久状态, 以及类似 Naiad 的方式在图中实现分支和循环控制结构.\n客户端通常使用支持的前端语言 (C++ 或 Python) 构建计算图.\n使用 Python 前端构建并执行 TensorFlow 图的示例片段如图 1 所示, 生成的计算图如图 2 所示.\n在 TensorFlow 图中, 每个节点有零个或多个输入和零个或多个输出, 并代表一个操作的实例化.\n沿着图中的正常边 (从输出到输入) 流动的值是张量, 即任意维度的数组, 其底层元素类型在图构建时指定或推断.\n图中还可以存在一种特殊的边, 称为控制依赖关系: 数据不会沿着这种边流动, 但它们表示控制依赖关系的源节点必须完成执行, 然后控制依赖关系的目标节点才能开始执行.\n由于我们的模型包含可变状态, 客户端可以直接使用控制依赖关系来强制 “happens before” 关系.\n我们的实现有时也会插入控制依赖关系, 作为强制其他独立操作之间顺序的一种方式, 例如控制峰值内存使用.\n#操作与内核\n一个操作 (Operation) 具有名称, 表示一个抽象计算 (例如, “矩阵乘法&quot;或&quot;加法”).\n操作可以具有属性, 所有属性必须在图构建时提供或推断, 以便实例化节点来执行该操作.\n属性的一个常见用途是使操作在不同张量元素类型上具有多态性 (例如, “类型为 float 的两个张量的加法&quot;与&quot;类型为 int32 的两个张量的加法”).\n内核 (Kernel) 是一个操作的特定实现, 可以在特定类型的设备上运行 (例如, CPU 或 GPU).\nTensorFlow 二进制文件通过注册机制定义可用的操作和内核集, 该集合可以通过链接额外的操作和/或内核定义/注册来扩展.\n表 1 展示了 TensorFlow 核心库中内置的一些操作类型.\n\n\n\nCategory\nExamples\n\n\n\n\nElement-wise mathematical operations\nAdd, Sub, Mul, Div, Exp, Log, Greater, Less, Equal, …\n\n\nArray operations\nConcat, Slice, Split, Constant, Rank, Shape, Shuffle, …\n\n\nMatrix operations\nMatMul, MatrixInverse, MatrixDeterminant, …\n\n\nStateful operations\nVariable, Assign, AssignAdd, …\n\n\nNeural-net building blocks\nSoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, …\n\n\nCheckpointing operations\nSave, Restore\n\n\nQueue and synchronization operations\nEnqueue, Dequeue, MutexAcquire, MutexRelease, …\n\n\nControl flow operations\nMerge, Switch, Enter, Leave, NextIteration\n\n\n\n#会话\n客户端程序通过与 TensorFlow 系统创建 会话 (Session) 进行交互.\n要创建计算图, 会话接口支持一个 Extend 方法, 用于向会话当前管理的图中添加额外的节点和边 (会话创建时的初始图是空的).\n会话接口支持的主要操作之一是 Run, 它接受需要计算的一组输出名称, 以及一组可选的张量, 这些张量将&quot;喂入&quot;图中, 用以替代某些节点的输出.\n通过 Run 的参数, TensorFlow 实现可以计算所有必须执行的节点的传递闭包, 以便计算请求的输出, 然后可以安排以尊重其依赖关系 (如 3.1 节中更详细所述) 的顺序执行相应的节点.\n我们大多数使用 TensorFlow 的情况都是一次设置一个会话和一个图, 然后通过 Run 调用数千或数百万次执行整个图或几个不同的子图.\n#变量\n在大多数计算中, 图会被多次执行.大多数张量不会存活到图的单次执行之后.\n然而, 变量 (Variable) 是一种特殊的操作, 它返回一个持久可变张量的句柄, 该张量会跨图执行而存活.\n这些持久可变张量的句柄可以被传递给少数几个特殊操作, 例如 Assign 和 AssignAdd (相当于&quot;+=&quot;), 这些操作会修改所引用的张量.\n对于 TensorFlow 的机器学习应用, 模型的参数通常存储在变量中持有的张量里, 并在模型的训练图运行过程中被更新.\n#3. 实现\nTensorFlow 系统的主要组件包括:\n\n客户端: 使用 Session 接口与主节点通信\n主节点\n一个或多个工作进程: 每个工作进程负责仲裁对一个或多个计算设备 (如 CPU 核心或 GPU 卡) 的访问, 并按照主节点的指令在这些设备上执行图节点.\n\nTensorFlow 接口具有 本地 和 分布式 两种实现.\n本地实现用于客户端、主节点和工作进程都在单个机器的同一操作系统进程上下文中运行的情况 (例如, 如果机器安装了多个 GPU 卡, 则可能具有多个设备).\n分布式实现与本地实现共享大部分代码, 但扩展了其对客户端、主节点和工作进程可以分别位于不同机器上不同进程的环境的支持.\n在我们的分布式环境中, 这些不同的任务是由集群调度系统管理的作业中的容器.\n这两种不同的模式如图 3 所示.\n本节其余部分主要讨论两种实现中共同存在的问题, 而第 3.3 节则讨论一些仅限于分布式实现的问题.\n\n图 3: 单机与分布式系统架构\n#设备\n设备 (Device) 是 TensorFlow 的计算核心.\n每个工作节点负责一个或多个设备, 每个设备具有设备类型和名称.\n设备名称由识别设备类型、设备在工作节点中的索引以及在分布式设置中识别工作节点作业和任务的标识符 (对于本地设备的情况, 则为&quot;localhost&quot;) 组成.\n示例设备名称包括 /job:localhost/device:cpu:0 或 /job:worker/task:17/device:gpu:3.\n我们为 CPU 和 GPU 提供了 Device 接口的实现, 其他设备类型的新的设备实现可以通过注册机制提供.\n每个设备对象负责管理设备内存的分配和释放, 以及安排 TensorFlow 实现中由更高层次请求的任何内核的执行.\n#张量\n在我们的实现中, 张量 (Tensor) 是一个带类型的、多维数组.\n我们支持多种张量元素类型, 包括 8 位到 64 位范围内的有符号和无符号整数、IEEE 浮点数和双精度类型、复数类型以及字符串类型 (任意字节数组).\n张量适当大小的后端存储由特定于张量所在设备的分配器进行管理.\n张量后端存储缓冲区采用引用计数, 当没有引用时会被释放.\n#3.1 单设备执行\n让我们首先考虑最简单的执行场景: 单个工作进程和单个设备.\n图中的节点按照节点之间的依赖关系顺序执行.\n具体来说, 我们跟踪每个节点尚未执行的依赖数量.\n一旦该计数降为零, 该节点就具备执行资格, 并被添加到就绪队列中.\n就绪队列以某种未指定的顺序进行处理, 将节点的内核执行委托给设备对象.\n当一个节点执行完成后, 所有依赖于该已完成节点的节点的计数都会递减.\n#3.2 多设备执行\n当一个系统拥有多个设备时, 主要有两个复杂问题:\n\n确定图中每个节点的计算应放置在哪个设备上\n管理由这些放置决策所隐含的跨设备边界的数据通信需求\n\n本小节将讨论这两个问题.\n#3.2.1 节点放置\n给定一个计算图, TensorFlow 实现的主要职责之一是将计算映射到可用设备集上.\n这里给出了该算法的简化版本.\n有关该算法支持的扩展, 请参阅第 4.3 节.\n放置算法的一个输入是一个成本模型, 该模型包含每个图节点输入和输出张量的大小 (以字节为单位) 的估计值, 以及当节点接收到其输入张量时所需的计算时间的估计值.\n这个成本模型要么基于与不同操作类型相关的启发式方法进行静态估计, 要么基于图先前执行的实际放置决策进行测量.\n放置算法首先对图进行模拟执行.\n模拟过程如下, 最终使用贪婪启发式方法为图中的每个节点选择设备.\n该模拟生成的节点到设备的放置结果也用作实际执行的放置方案.\n放置算法从计算图的源开始, 并在执行过程中模拟系统内每个设备上的活动.\n对于在此遍历中到达的每个节点, 会考虑可行的设备集 (如果设备不提供实现特定操作的内核, 则该设备可能不可行).\n对于具有多个可行设备的节点, 放置算法使用贪婪启发式算法, 该算法检查将节点放置在每个可能设备上对节点完成时间的影响.\n该启发式算法考虑了成本模型中对该类型设备上操作的估计或测量执行时间, 并还包括为将输入从其他设备传输到考虑的设备而引入的任何通信成本.\n选择节点操作最早完成的设备作为该操作的设备, 然后继续放置过程以对图中的其他节点做出放置决策, 包括现在准备好进行其模拟执行的下游节点.\n第 4.3 节 描述了一些扩展, 允许用户提供提示和部分约束来指导放置算法.\n放置算法是系统内持续发展的一个领域.\n#3.2.2 跨设备通信\n一旦节点放置计算完成, 图被划分为一组子图, 每个设备一个.\n任何跨设备从 xxx 到 yyy 的边被移除, 并替换为从 xxx 到一个新的 Send 节点的边, 以及从相应的 Receive 节点到 yyy 的边.\n见图 4, 示例此图转换.\n\n图 4: 插入发送/接收节点前与后\n在运行时, 发送和接收节点的实现协调以跨设备传输数据.\n这使得我们能够将所有通信隔离在发送和接收实现内部, 从而简化了其余的运行时.\n当我们在其中插入发送 (Send) 和接收 (Receive) 节点时, 我们会将特定设备上某个张量的所有用户规范化为使用单个接收节点, 而不是在特定设备上为每个下游用户使用一个接收节点.\n这确保了对于所需张量的数据, 在源设备和目标设备之间只传输一次, 并且目标设备上张量的内存只分配一次, 而不是多次 (例如, 参见图 4 中的节点 b 和 c).\n通过这种方式处理通信, 我们还允许将图中各个节点的调度分散到工作节点中: 发送和接收节点在不同工作节点和设备之间提供必要的同步, 而主节点只需为每个包含图节点的每个工作节点发出一次运行请求, 而不是参与每个节点或跨设备通信的调度.\n这使得系统更具可扩展性, 并且能够比强制由主节点进行调度时实现更细粒度的节点执行.\n#3.3 分布式执行\n图的分布式执行与多设备执行非常相似.\n在设备放置之后, 每个设备都会创建一个子图.\n跨工作进程通信的发送/接收节点对使用 TCP 或 RDMA 等远程通信机制来跨越机器边界传输数据.\n#容错能力\n分布式执行中的故障可以在多种位置被检测到.\n我们主要依赖的检测方式有:\n\n发送节点和接收节点之间通信的错误\n主进程对每个工作进程进行的周期性健康检查.\n\n当检测到故障时, 整个图执行会被中止并从头开始重新启动.\n然而, 需要回忆的是, Variable 节点指向在图的不同执行中持续存在的张量.\n我们支持在重启时对此状态进行一致的检查点保存和恢复.\n具体来说, 每个 Variable 节点都连接到一个 Save 节点.\n这些 Save 节点会周期性执行, 例如每 N 次迭代执行一次, 或每 N 秒执行一次.\n当它们执行时, 变量的内容会被写入持久化存储, 例如分布式文件系统.\n类似地, 每个 Variable 都连接到一个 Restore 节点, 该节点仅在重启后的第一次迭代中启用.\n有关某些节点只能在某些图执行中启用的详细信息, 请参阅第 4.2 节.\n#4. 扩展\n在本节中, 我们描述了第 2 节介绍的基本编程模型的一些更高级的功能.\n#4.1 梯度计算\n许多优化算法, 包括随机梯度下降等常见的机器学习训练算法, 都会计算成本函数相对于一组输入的梯度.\n由于这是一个非常常见的需求, TensorFlow 提供了内置的自动梯度计算支持.\n如果在 TensorFlow 图中, 一个张量 CCC 依赖于一些张量 Xk{X_k}Xk​ (可能通过一个复杂的子图操作), 那么有一个内置函数可以返回张量 dCdXk\\frac{dC}{dX_k}dXk​dC​.\n梯度张量的计算与其他张量类似, 通过扩展 TensorFlow 图, 并使用以下步骤进行计算.\n当 TensorFlow 需要计算相对于某个依赖于 III 的张量 CCC 的梯度时, 它首先在计算图中找到从 III 到 CCC 的路径.\n然后它从 CCC 回溯到 III, 对于反向路径上的每个操作, 它向 TensorFlow 图中添加一个节点, 使用链式法则沿着反向路径组合偏导数.\n新添加的节点计算正向路径中相应操作的&quot;梯度函数&quot;.\n梯度函数可以由任何操作注册.\n该函数不仅接收已经沿反向路径计算的偏导数作为输入, 还可以选择性地接收正向操作的输入和输出.\n图 5 显示了从图 2 的示例计算出的成本的梯度.\n灰色箭头显示了梯度函数的潜在输入, 这些输入未用于所显示的特定操作.计算这些梯度需要在图 1 中添加:\n[db,dW,dx] = tf.gradients(C, [b,W,x])\n通常, 一个操作可能有多个输出, 而 CCC 可能仅依赖于其中的一部分.\n例如, 如果操作 OOO 有两个输出 y1y_1y1​ 和 y2y_2y2​, 而 CCC 仅依赖于 y2y_2y2​, 那么 OOO 的梯度函数的第一个输入被设置为 0, 因为 dCdy1=0\\frac{dC}{dy_1} = 0dy1​dC​=0.\n\n图 5: 图 2 中计算得到的梯度\n自动梯度计算使优化过程复杂化, 尤其是内存使用方面.\n在执行&quot;前向&quot;计算子图时, 即由用户显式构建的子图, 一个合理的启发式方法通过观察图构建的顺序来决定下一个要执行的节点, 从而打破平局.\n这通常意味着临时输出在构建后很快被消耗, 因此其内存可以迅速重用.\n当启发式方法无效时, 用户可以改变图构建的顺序, 或添加第 5 节中描述的控制依赖关系.\n当自动向图中添加梯度节点时, 用户控制力减弱, 启发式方法可能失效.\n特别是, 因为梯度会逆转前向计算顺序, 所以在图执行早期使用的张量在梯度计算接近结束时经常再次需要.\n这些张量会占用大量稀缺的 GPU 内存, 并无意中限制计算规模.\n我们正在积极研究内存管理的改进方法, 以更好地处理此类情况.\n选项包括使用更复杂的启发式算法来确定图执行顺序, 重新计算张量而不是将它们保留在内存中, 以及将长期存在的张量从 GPU 内存中交换到更充裕的主机 CPU 内存中.\n#4.2 部分执行\n通常, 客户希望执行整个执行图中的子图.\n为了支持这一点, 一旦客户端在会话中设置了一个计算图, 我们的 Run 方法允许他们执行整个图中的任意子图, 并在图的任何边上注入任意数据, 并检索沿图任何边流动的数据.\n图中的每个节点都有一个名称, 节点的每个输出都由源节点名称和节点上的输出端口标识, 编号从 0 开始 (例如, &quot;bar:0&quot;指的是&quot;bar&quot;节点的第 1 个输出, 而&quot;bar:1&quot;指的是第 2 个输出).\nRun 调用有两个参数, 有助于定义将要执行的计算图中的确切子图.\n\ninputs 参数, 这是一个可选的从 name:port -&gt; feed 的映射.\noutput_names 参数, 这是一个 name[:port] 列表, 指示哪些节点应该被执行, 如果名称中包含端口部分, 则 Run 调用成功完成后, 应将对应节点的特定输出张量值返回给客户端.\n\n\n图 6: 部分执行前后的图转换\n图根据输入和输出的值进行转换.\n在输入中指定的每个 name:port 被替换为 feed 节点, 该节点将从用于 Run 调用的 Rendezvous 对象中特别初始化的条目中获取提供的输入张量.\n类似地, 每个带有端口的输出名称连接到一个特殊的 fetch 节点, 该节点安排在 Run 调用完成后保存输出张量并将其返回给客户端.\n最后, 一旦通过插入这些特殊的 feed 和 fetch 节点重写了图, 可以通过从任何输出命名的节点开始, 使用图依赖关系反向遍历图来确定要执行的节点集.\n图 6 显示了左侧的原始图, 以及当 Run 被调用且 输入为 {b} 和 输出为 {f:0} 时产生的转换后的图.\n由于我们只需要计算节点 f 的输出, 因此不会执行节点 d 和 e, 因为它们对 f 的输出没有贡献.\n#4.3 设备约束\nTensorFlow 客户端可以通过为节点提供部分约束来控制节点在设备上的放置位置, 这些约束规定了节点可以在哪些设备上执行.\n例如, “仅将此节点放置在类型为 GPU 的设备上”, 或 “此节点可以放置在任何设备上 /job:worker/task:17”, 或 “将此节点与名为 variable13 的节点放置在一起”.\n在这些约束的范围内, 放置算法负责选择节点分配到设备的方式, 以实现快速计算执行, 并满足设备自身施加的各种约束, 例如限制设备上执行其子图节点所需的内存总量.\n支持这些约束需要对第 3.2.1 节中描述的放置算法进行修改.\n我们首先计算每个节点的可行设备集, 然后使用并查集在邻接约束图上计算必须一起放置的图分量.\n对于每个这样的分量, 我们计算可行设备集的交集.\n每个节点计算出的可行设备集可以轻松地放入放置算法的模拟器中.\n#4.4 控制流\n尽管没有任何显式控制流的数据流图非常具有表达能力, 但我们观察到在支持条件语句和循环的情况下, 可以导致机器学习算法的表示更加简洁和高效.\n与 Arvind 所描述的 dataflow-machine 方法类似, 我们将一组基本的控制流操作符引入 TensorFlow, 并将 TensorFlow 扩展为处理循环数据流图.\nSwitch 和 Merge 操作符允许我们根据布尔张量的值跳过整个子图的执行.\nEnter、Leave 和 NextIteration 操作符使我们能够表达迭代.\n高级编程结构, 如 if 条件语句和 while 循环, 可以轻松地使用这些控制流操作符编译成数据流图.\nTensorFlow 运行时实现了一种与 MIT Tagged-Token machine 概念上相似的标签和帧机制.\n循环的每次迭代都由一个标签唯一标识, 其执行状态由一个帧表示.\n当输入变得可用时, 它可以进入迭代；因此, 多个迭代可以并发执行.\nTensorFlow 使用分布式协调机制来执行具有控制流的图.\n通常, 一个循环可以包含分配到许多不同设备的节点.\n因此, 管理循环的状态成为一个分布式终止检测问题.\nTensorFlow 的解决方案基于图重写.\n在图划分过程中, 我们自动向每个划分添加控制节点.\n这些节点实现了一个小型状态机, 用于协调每次迭代的开始和终止, 并决定循环的终止.\n对于每次迭代, 拥有循环终止谓词的设备向每个参与设备发送一个微小的控制消息.\n如前所述, 我们通常通过梯度下降来训练机器学习模型, 并将梯度计算表示为数据流图的一部分.\n当模型包含控制流操作时, 我们必须在相应的梯度计算中考虑它们.\n例如, 具有 if 条件的模型的梯度计算需要知道条件分支中哪一条被选中, 然后将对梯度逻辑应用于该分支.\n类似地, 具有 while 循环的模型的梯度计算需要知道进行了多少次迭代, 并且还需要依赖于在这些迭代过程中计算出的中间值.\n基本技术是重写图, 以便记住梯度计算所需的值.\n我们省略了这种编码的某些复杂细节.\n#4.5 输入操作\n尽管输入数据可以通过 feed 节点提供给计算, 但训练大规模机器学习模型的另一种常见机制是在图中使用特殊的输入操作节点, 这些节点通常配置有一组文件名, 每次执行时都会生成一个包含一个或多个来自该文件集存储数据的张量.\n这允许数据直接从底层存储系统读取到将执行后续处理的数据的内存中.\n在客户端进程与工作进程分离的配置中, 如果数据被传输, 通常需要额外的网络跳数 (从存储系统到客户端, 然后从客户端到工作进程, 而不是使用输入节点时直接从存储系统到工作进程).\n#4.6 队列\n队列是我们在 TensorFlow 中添加的一个有用功能.\n它们允许图的不同部分异步执行, 可能以不同的频率, 并通过 Enqueue (入队) 和 Dequeue (出队) 操作传递数据.\nEnqueue 操作可以阻塞, 直到队列中有可用空间, 而 Dequeue 操作可以阻塞, 直到队列中有所需的最小元素数量.\n队列的一个用途是允许输入数据在机器学习模型的计算部分仍在处理前一批数据时从磁盘文件中预取.\n它们也可以用于其他类型的分组, 包括累积许多梯度以计算更大批次上更复杂的梯度组合, 或者将循环语言模型的不同输入句子分组到长度大致相同的句子组中, 然后可以更高效地处理.\n除了普通的 FIFO 队列外, 我们还实现了一个洗牌队列, 它在一个大型的内存缓冲区中随机打乱其元素.\n这种洗牌功能对于希望随机化处理示例顺序的机器学习算法非常有用.\n#4.7 容器\n容器 (Container) 是 TensorFlow 中用于管理更长时间存在的可变状态的一种机制.\n变量的后端存储存在于容器中.\n默认的容器是持续到进程终止的容器, 但我们也允许其他命名的容器.\n容器可以通过完全清除其内容来重置.\n使用容器, 即使在完全不相交的、与不同会话关联的计算图中, 也有可能共享状态.\n#5. 优化\n在本节中, 我们描述了 TensorFlow 实现中的一些优化措施, 这些措施能够提升系统性能或资源利用率.\n\n公共子表达式消除\n控制数据通信和内存使用\n异步内核\n高性能算子库\n有损压缩\n\n#6. 状态与经验\nTensorFlow 接口和参考实现已在 Apache 2.0 许可证下开源, 该系统可在 &lt;www.tensorflow.org&gt; 下载.\n该系统包括详细的文档、多个教程以及多个示例, 展示了如何使用该系统进行各种不同的机器学习任务.\n这些示例包括从 MNIST 数据集 (机器学习算法的&quot;hello world&quot;) 中分类手写数字、从 CIFAR-10 数据集中分类图像、使用循环 LSTM网络进行语言建模、训练词嵌入向量等.\n该系统包括用于在 Python 和 C++中指定 TensorFlow 计算的接口, 我们预计随着时间的推移, 将根据内部 Google 用户和更广泛的开放源社区的需求添加其他接口.\n我们在之前的 DistBelief 系统中拥有许多机器学习模型, 并将它们迁移到了 TensorFlow.\n本节其余部分将讨论我们从这些迁移中学到的一些经验教训, 这些经验教训具有普遍适用性, 可以应用于任何将机器学习模型从一个系统迁移到另一个系统的过程, 因此可能对其他人也有价值.\n特别是, 我们关注从移植一种用于图像识别的最先进的卷积神经网络 Inception中吸取的教训.\n该图像识别系统将 224 × 224 像素图像分类为 1000 个标签之一 (例如, “猎豹”、&quot;垃圾车&quot;等).\n当以 TensorFlow 图的形式表示时, 此类模型包含 1360 万个可学习参数和 36000 次操作.\n在单个图像上运行推理需要 20 亿次乘加操作.\n在 TensorFlow 中构建所有必要的数学运算后, 将 36,000 个运算组装并调试成正确的图结构被证明是一项具有挑战性的工作.\n验证正确性是一项困难的任务, 因为系统本质上具有随机性, 并且仅在期望中以一种特定方式运行——可能需要数小时的计算时间.\n在这种情况下, 我们发现将 Inception 模型移植到 TensorFlow 的以下策略至关重要:\n\n构建工具以深入了解给定模型中参数的确切数量. 这些工具展示了复杂网络架构规范中的微妙缺陷. 特别是我们能够识别由于数学运算中自动广播导致维度上的实例化不正确的操作和变量.\n从小处着手, 逐步扩展. 我们从前一系统移植的第一个卷积神经网络是一个在 CIFAR-10 数据集上使用的小型网络. 调试此类网络阐明了机器学习系统中单个操作 (例如, 最大池化) 中的微妙边缘情况, 在更复杂的模型中这些情况实际上几乎无法识别.\n始终确保在关闭学习时, 机器学习系统中的目标 (损失函数) 保持一致. 将学习率设置为零帮助我们识别了模型中随机初始化变量时出现的意外行为. 在动态训练网络中, 此类错误将难以识别.\n在调试分布式实现之前, 先在单机环境中进行匹配. 这一策略帮助我们界定了机器学习系统中训练性能的差异, 并进行了调试. 具体而言, 我们识别了由于竞态条件和错误地假设为原子操作的非原子操作所导致的错误.\n防范数值错误.数值库在处理非有限浮点数方面存在不一致性. 卷积神经网络特别容易受到数值不稳定性影响, 在实验和调试阶段经常会表现出发散行为. 通过检查非有限浮点数来防范这种行为, 可以实时检测错误, 而不是事后识别发散行为.\n分析网络中的各个部分并理解数值误差的幅度. 在两个机器学习系统上并行运行神经网络的子部分, 提供了一种精确的方法来确保数值算法在两个系统中保持一致. 鉴于这些算法以浮点精度运行, 预测和理解预期数值误差的幅度非常重要, 以便判断给定组件是否正确实现 (例如, 区分&quot;在 1e-2 范围内, 很好!“和&quot;在 1e-2 范围内: 为什么它如此不正确?!”).\n\n在具有内在随机性的系统中验证复杂的数学运算极具挑战性.\n上述策略在增强系统信心以及最终在 TensorFlow 中实现 Inception 模型方面被证明极具价值.\n这些努力的结果使训练时间比我们现有的 DistBelief 模型实现提高了 6 倍, 这种速度提升在训练新一代更大规模的图像识别模型中被证明不可或缺.\n#7. 常见编程模式\nTensorFlow 的基本数据流图模型可以用于各种机器学习应用.\n我们关注的领域之一是在大型数据集上加速计算密集型神经网络的训练.\n本节介绍了我们和其他人开发的一些技术, 以实现这一目标, 并说明了如何使用 TensorFlow 来实现这些各种方法.\n本小节中的方法假设模型正在使用随机梯度下降 (SGD) 进行训练, 并且使用相对较小的 mini-batch, 包含 100 到 1000 个示例.\n#数据并行训练\n一种简单的加速 SGD 的技术是将梯度计算并行化.\n例如, 如果我们使用 1000 个元素的 mini-batch, 我们可以使用 10 个模型副本, 每个副本计算 100 个元素的梯度, 然后合并梯度并同步更新参数, 以表现得就像我们正在使用 1000 个元素的 mini-batch 运行顺序 SGD 算法一样.\n在这种情况下, TensorFlow 图简单地包含执行模型计算大部分工作的图部分的多个副本, 单个客户端线程驱动这个大型图的整个训练循环.\n这如图 7 的上半部分所示.\n这种方法也可以实现异步处理, 其中 TensorFlow 计算图包含多个副本, 这些副本负责执行模型的主要计算部分, 并且每个副本都异步地将参数更新应用于模型参数.\n在这种配置中, 每个计算图副本都有一个客户端线程.\n这如图 7 的底部所示.\n这种异步方法也在中进行了描述.\n\n图 7: 数据并行训练: 同步 (上) 与异步 (下)\n#模型并行训练\n模型并行训练, 即将同一批示例的不同模型计算部分同时分配到不同的计算设备上进行, 在 TensorFlow 中也很容易实现.\n图 8 展示了一个用于序列到序列学习的循环深度 LSTM 模型示例 (参见), 该模型在三个不同的设备上进行了并行化处理.\n\n图 8: 模型并行训练\n#模型计算流水线并行步骤\n另一种提高深度神经网络训练利用率的方法是在同一设备内对模型计算进行流水线处理, 通过在同一设备集内运行少量并行步骤.\n这如图 9 所示.它与异步数据并行性有些相似, 但并行性发生在同一设备 (或设备组) 内部, 而不是在不同的设备上复制计算图.\n这允许&quot;填补空白&quot;, 即在一个步骤中, 单个批次的计算可能无法在任何时候完全利用所有设备的全部并行性.\n\n图 9: 并发步骤\n#8. 性能\n白皮书版本无完整的性能评估.\n#9. 工具\n本节介绍了一些我们开发并与核心 TensorFlow 图执行引擎并存的工具.\n#9.1 TensorBoard: 图结构和摘要统计的可视化\n为了帮助用户理解其计算图的结构, 以及理解机器学习模型的整体行为, 我们构建了 TensorBoard, 这是一个包含在开源发布中的 TensorFlow 配套可视化工具.\n#计算图的可视化\n深度神经网络中的许多计算图可能非常复杂.\n例如, 用于训练与 Google 的 Inception 模型类似模型的计算图——该模型是一种深度卷积神经网络, 在 ImageNet 2014 竞赛中取得了最佳分类性能——其 TensorFlow 计算图包含超过 36,000 个节点, 而一些用于语言模型的深度循环 LSTM 模型则包含超过 15,000 个节点.\n由于这些图的规模和拓扑结构, 传统的可视化技术往往会产生杂乱且令人不知所措的图表.\n为了帮助用户看清图的基本组织结构, TensorBoard 中的算法将节点合并为高级模块, 突出显示具有相同结构的组.\n系统还将具有高连接度的节点 (这些节点通常用于记账功能) 分离到屏幕的单独区域.\n这样做减少了视觉上的杂乱, 使注意力集中在计算图的核心部分.\n整个可视化是交互式的: 用户可以平移、缩放并展开分组节点以深入查看详细信息.\n深度卷积图像模型的图的可视化示例如图 10 所示.\n\n图 10: 卷积神经网络模型的 TensorBoard 图可视化\n\n图 11: TensorBoard 模型摘要统计时间序列数据的图形显示\n#摘要数据的可视化\n在训练机器学习模型时, 用户通常希望能够检查模型各个方面的状态, 以及这些状态随时间的变化情况.\n为此, TensorFlow 支持将一系列不同的摘要操作插入图中, 包括标量摘要 (例如, 用于检查模型的整体属性, 如跨多个样本的损失函数平均值, 或执行计算图所需的时间)、基于直方图的摘要 (例如, 神经网络层中权重值的分布), 或基于图像的摘要 (例如, 卷积神经网络中学习到的滤波器权重的可视化).\n通常, 计算图会设置摘要节点以监控各种有趣的值, 在执行训练图的过程中, 除了执行正常节点集之外, 摘要节点集也会被定期执行, 客户端驱动程序将摘要数据写入与模型训练相关的日志文件.\nTensorBoard 程序随后被配置为监视此日志文件以获取新的摘要记录, 并能够显示此摘要信息及其随时间的变化 (具有选择&quot;时间&quot;测量的能力, 可以是 TensorFlow 程序执行开始以来的相对墙上时间、绝对时间或&quot;步骤&quot;, 即自 TensorFlow 程序执行开始以来发生的图执行次数的数值度量).\nTensorBoard 中摘要值可视化的屏幕截图如图 11 所示.\n#9.2 性能跟踪\n我们还有一个内部工具, 称为 EEG (未包含在 2015 年 11 月的初始开源发布中), 用于收集和可视化关于 TensorFlow 图执行的确切顺序和性能特征的非常细粒度信息.\n该工具适用于我们的单机和分布式实现, 对于理解 TensorFlow 程序的计算和通信模式中的瓶颈非常有用.\n系统中的每台机器同时从多种来源收集跟踪信息, 包括 Linux 内核 ftrace、我们自己的轻量级线程跟踪工具以及 CUDA Profiling Tools Interface (CUPTI).\n通过这些日志, 我们可以以微秒级的细节重建分布式训练步骤的执行过程, 包括每个线程切换、CUDA 内核启动和 DMA 操作.\n跟踪信息在可视化服务器中组合, 该服务器设计用于快速提取指定时间范围内的事件, 并以适当的详细程度为用户界面分辨率进行总结.\n由于通信、同步或 DMA 相关停滞引起的任何显著延迟都会被识别并用箭头在可视化中突出显示.\n最初, 用户界面提供整个跟踪的概览, 仅突出显示最重要的性能伪影.\n随着用户逐步放大, 越来越精细的分辨率细节将被渲染.\n\n图 12: 多线程 CPU 操作的 EEG 可视化 (x 轴为 us 的时间).\n图 12 展示了一个在多核 CPU 平台上训练模型的 EEG 可视化示例.\n屏幕截图的上三分之一显示了根据数据流约束并行调度的 TensorFlow 操作.\n追踪记录的底部部分展示了大多数操作如何被分解为多个工作项, 这些工作项在线程池中并发执行.\n右侧的对角箭头显示了线程池中队列延迟的累积位置.\n\n图 13: Inception 训练的 EEG 可视化, 显示 CPU 和 GPU 活动.\n图 13 展示了另一种 EEG 可视化, 计算主要在 GPU 上进行.\n可以看到主机线程正在将可运行的 TensorFlow GPU 操作入队 (浅蓝色线程池), 而背景维护线程以其他颜色显示在处理器核心之间迁移.\n再次, 箭头指示线程在 GPU 到 CPU 传输中停滞的位置, 或操作经历显著排队延迟的位置.\n\n图 14: 多流 GPU 执行的时间线.\n最后, 图 14 展示了一个更详细的视图, 使我们能够考察 Tensorflow GPU 算子如何分配到多个 GPU 流.\n每当数据流图允许并行执行或数据传输时, 我们都会努力使用流和流依赖原语向 GPU 设备暴露顺序约束.\n#10. 未来工作\n我们未来工作有几个不同的方向.\n我们将继续使用 TensorFlow 开发新的、有趣的机器学习模型用于人工智能, 在这个过程中, 我们可能会发现需要扩展基本 TensorFlow 系统的途径.\n开源社区也可能为 TensorFlow 实现提出新的、有趣的方向.\n我们正在考虑的基本编程模型的一个扩展是一个函数机制, 用户可以指定 TensorFlow 计算的一个子图作为一个可重用的组件.\n在我们设计的实现中, 这些函数甚至可以在不同的 TensorFlow 前端语言之间成为可重用的组件, 以便用户可以使用 Python 前端定义一个函数, 但然后可以使用该函数作为 C++前端的基本构建块.\n我们希望这种跨语言的可重用性将启动一个充满活力的机器学习研究人员社区, 他们不仅发布他们研究的完整示例, 还发布他们工作中的小型可重用组件, 这些组件可以在其他环境中重用.\n我们还有许多具体的方向来提升 TensorFlow 的性能.\n其中一个方向是我们对即时编译器 (just-in-time compiler) 的初步研究, 该编译器可以取 TensorFlow 执行的一个子图, 可能包含一些关于张量典型大小和形状的运行时分析信息, 并为此子图生成优化的例程.\n这个编译器将理解执行语义, 并执行多种优化, 如循环融合 (loop fusion)、分块 (blocking) 和分片 (tiling) 以提升局部性、针对特定形状和大小的特化 (specialization) 等.\n我们同样设想, 未来工作的一个重要领域将在于改进用于决定不同节点将执行位置以及它们应何时开始执行的放置和节点调度算法.\n目前, 我们在这些子系统中已实现了一些启发式方法, 我们希望系统能够学习做出良好的放置决策 (或许使用深度神经网络, 结合强化学习目标函数).\n#11. 相关工作\n存在许多其他系统, 在各个方面与 TensorFlow 具有可比性.\nTheano 、Torch 、Caffe 、Chainer 以及 Computational Network Toolkit 是几个主要设计用于神经网络训练的系统.\n这些系统中的每一个都将计算映射到单台机器上, 与分布式 TensorFlow 实现不同.\n与 Theano 和 Chainer 类似, TensorFlow 支持符号微分, 从而更容易定义和操作基于梯度的优化算法.\n与 Caffe 类似, TensorFlow 的核心是用 C++编写的, 简化了在多种生产环境中部署训练模型的任务, 包括内存和计算受限的环境, 如移动设备.\nTensorFlow 系统与其前身系统 DistBelief以及具有相似设计的后续系统 (如 Project Adam和 Parameter Server 项目) 在一些设计特征上有所相似.\n与 DistBelief 和 Project Adam 类似, TensorFlow 允许计算分散到多台机器的多个计算设备上, 并允许用户使用相对高级的描述来指定机器学习模型.\n然而, 与 DistBelief 和 Project Adam 不同, TensorFlow 中的通用数据流图模型更加灵活, 更易于表达更广泛的机器学习模型和优化算法.\n它还通过允许将状态参数节点表示为变量, 以及将变量更新操作作为图中的附加节点, 实现了显著的简化；相比之下, DistBelief、Project Adam 和 Parameter Server 系统都拥有完全独立的参数服务器子系统, 专门用于参数值的通信和更新.\nHalide 系统用于表达图像处理流程, 其采用与 TensorFlow 数据流图相似的中间表示.\n然而, 与 TensorFlow 不同, Halide 系统实际上对其操作的语义有更高级别的理解, 并利用这种知识生成高度优化的代码片段, 这些代码片段结合了多个操作, 同时考虑了并行性和局部性.\nHalide 仅在单机上运行生成的计算, 而非分布式环境.\n在未来的工作中, 我们希望扩展 TensorFlow, 加入类似的跨操作动态编译框架.\n与 TensorFlow 类似, 已有多个分布式系统被开发用于在集群中执行数据流图.\nDryad 和 Flume 展示了如何将复杂的工作流表示为数据流图.\nCIEL 和 Naiad 引入了针对数据依赖控制流的一般性支持: CIEL 将迭代表示为动态展开的有向无环图 (DAG), 而 Naiad 使用带有循环的静态图来支持低延迟迭代.\nSpark 针对反复访问相同数据的计算进行了优化, 使用&quot;弹性分布式数据集&quot; (RDDs), 这些是早期计算结果的软状态缓存输出.\nDandelion 在异构设备集群 (包括 GPU) 上执行数据流图.\nTensorFlow 采用了一种混合数据流模型, 借鉴了这些系统的各个元素.\n其数据流调度器 (即选择下一个执行节点的组件) 使用与 Dryad、Flume、CIEL 和 Spark 相同的基本算法.\n其分布式架构与 Naiad 最为接近, 因为该系统使用单个优化的数据流图来表示整个计算, 并在每个设备上缓存该图的信息以最小化协调开销.\n与 Spark 和 Naiad 类似, 当集群中有足够的 RAM 来存储计算的工作集时, TensorFlow 表现最佳.\nTensorFlow 中的迭代采用混合方法: 同一数据流图的多个副本可能同时执行, 同时共享同一组变量.\n副本可以通过变量异步共享数据, 或使用图中的同步机制 (如队列) 进行同步操作.\nTensorFlow 还支持图内的迭代, 这是一种 CIEL 和 Naiad 的混合: 为简化起见, 每个节点仅在其所有输入都准备好时才触发 (类似于 CIEL) ；但为提高效率, 图被表示为静态循环数据流 (类似于 Naiad).\n#12. 结论\n我们描述了 TensorFlow, 这是一个基于数据流的灵活编程模型, 以及该编程模型在单机和分布式系统中的实现.\n该系统源于我们在谷歌产品和服务中广泛开展研究和部署超过一百个机器学习项目中的实际经验.\n我们开源了一个版本的 TensorFlow, 并希望围绕 TensorFlow 的使用形成一个充满活力的共享社区.\n我们很兴奋地看到谷歌以外的人们如何在他们的工作中使用 TensorFlow.\n#参考资料\n\nTensorFlow 详解 - 张雨石\n\n寻找合适设备是TF系统区分与之前很多系统的地方，之前的系统比如Parameter Server，是参数分离出来，运算在一起，同时使用数据切分来达到分布式。而TF是把每个op都映射到某个机器上，意味着每个op可能在不同的机器上，这是对系统的进一步剖离，因而可以达到更高的可扩展性。我个人觉得这是TF区分与Parameter Server的一个大区别，对于TF来说，计算节点是分离的，参数也是分离的，因而其PS也是分离的。每个设备上可能分配了计算节点，然后其对应的ps也在该设备上。因而，传统的PS中，计算和参数是分开的，但计算和参数他们分别是在一起的；而TF中，计算本身是分离的，参数也是分离的，而计算和参数是在一起的。\n\n\n\n","categories":["mlsys"],"tags":["MLSys","Distributed Machine Learning","System"]},{"title":"[MAPL@PLDI'19] Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations 阅读笔记","url":"/2026/02/07/triton-an-interediate-language-and-compiler-for-tiled/","content":"#1. 引言\n深度神经网络 (DNNs) 的近期复兴在很大程度上得益于可编程并行计算设备的普及.\n特别是, 多核架构 (例如 GPU) 性能的持续改进发挥了基础性作用, 使研究人员和工程师能够探索种类日益增多、规模越来越大的模型, 并使用越来越多的数据.\n这一努力得到了一系列供应商库 (cuBLAS、cuDNN) 的支持, 这些库旨在尽快将最新的硬件创新带给从业者.\n不幸的是, 这些库仅支持有限的一组 Tensor 操作, 将创新原语的实现留给了专家.\n这一观察推动了针对深度神经网络 (DNNs) 的多种领域特定语言 (DSLs) 的开发, 这些语言基于 polyhedral machinery (例如 Tensor Comprehension) 和/或循环合成技术 (例如, Halide、TVM 和 PlaidML).\n然而, 虽然这些系统在特定问题类别 (如深度可分离卷积, 例如 MobileNet) 上通常表现良好, 但在实践中它们往往比供应商库慢得多 (参见图 1),\n并且缺乏实现结构化稀疏模式的表达能力, 而这些模式不能直接使用嵌套循环中的仿射数组索引来指定.\n\n图 1. C=ABTC = AB^TC=ABT 不同实现与 Roofline 模型的性能对比 (NVIDIA GeForce GTX1070), 其中 A∈R1760×1760,B∈RN×1760A \\in \\mathbb{R}^{1760 \\times 1760}, B \\in \\mathbb{R}^{N \\times 1760}A∈R1760×1760,B∈RN×1760, N 用于控制算术强度。\n这些问题通常通过使用 Micro-Kernel (即手工编写的 Tile 级原语) 来解决–但这种方法需要大量的人工工作, 并且缺乏可移植性.\n虽然最近已经提出了几种支持 Tiling 抽象的高级编程语言 (Cutlass, TiDA), 但 底层的编译器后端仍然缺乏对 Tile 级操作和优化的支持.\n为此, 我们提出了 Triton, 一种开源的中级语言和编译器, 用于指定和编译 Tile 程序为高效的 GPU 代码.\n本文的主要贡献总结如下:\n\nTriton-C: 一种类似 C 的语言, 用于通过参数化 Tile 变量表达 Tensor 程序. 该语言旨在为现有的 DNN 转编译器 (例如 PlaidML、Tensor Comprehensions) 和熟悉 CUDA 的程序员提供稳定的接口. 列表 1 显示了与简单矩阵乘法任务相关的 Triton-C 源代码.\nTriton-IR: 一种基于 LLVM 的中级表示 (IR), 它提供了一个适合 Tile 级程序分析、转换和优化的环境. 列表 5 显示了 ReLU 函数的 Triton-IR 代码. 在这里, Triton-IR 程序在解析过程中直接从 Triton-C 构建, 但未来也可以探索从嵌入式 DSL 或更高层次的 DNN 编译器 (例如 TVM) 自动生成.\nTriton-JIT: 一种即时编译器 (JIT) 和代码生成后端, 用于将 Triton-IR 程序编译为高效的 LLVM 位码. 这包括 (1) 一套面向 tile 级别、与机器无关的优化过程, 旨在独立于任何编译目标; (2) 一套针对 Tile 级别的、与机器相关的编译过程, 用于生成高效的 GPU 准备好的 LLVM-IR; (3) 一个自动调优器, 用于优化与上述编译过程相关的任何元参数.\n数值实验: 对 Triton 的数值评估, 展示了其能力在于:\n\n在循环和 Transformer 神经网络中生成与 cuBLAS 相当且比其他 DSL 快 3 倍的矩阵乘法实现\n在不损失性能的情况下重新实现 cuDNN 的密集卷积算法\n为诸如移位卷积等新型研究思想创建高效的实现.\n\n\n\n\n图 2. Triton 概述\n表1. 在 Triton-C 中计算 C=A×BTC = A \\times B^TC=A×BT\n// Tile shapes are parametric and can be optimized by compilation backendsconst tunable int TM = &#123;16, 32, 64, 128&#125;;const tunable int TN = &#123;16, 32, 64, 128&#125;;const tunable int TK = &#123;8, 16&#125;;// C = A * B.Tkernel void matmul_nt(float* a, float* b, float* c, int M, int N, int K)&#123;    // 1D tile of indices    int rm[TM] = get_global_range(0);    int rn[TN] = get_global_range(1);    int rk[TK] = 0 ... TK;    // 2D tile of accumulators    float C[TM, TN] = 0;    // 2D tile of pointers    float* pa[TM, TK] = a + rm[:, newaxis] + rk * M;    float* pb[TN, TK] = b + rn[:, newaxis] + rk * K;    for (int k = K; k &gt;= 0; k -= TK) &#123;        bool check_k[TK] = rk &lt; k;        bool check_a[TM, TK] = (rm &lt; M)[:, newaxis] &amp;&amp; check_k;        bool check_b[TN, TK] = (rn &lt; N)[:, newaxis] &amp;&amp; check_k;        // Load tile operands        float A[TM, TK] = check_a ? *pa : 0;        float B[TN, TK] = check_b ? *pb : 0;        // Accumulate        C += dot(A, trans(B));        // Update pointers        pa = pa + TK * M;        pb = pb + TK * N;    &#125;    // Write-back accumulators    float* pc[TM, TN] = c + rm[:, newaxis] + rn * M;    bool check_c[TM, TN] = (rm &lt; M)[:, newaxis] &amp;&amp; (rn &lt; N);    @check_c *pc = C;&#125;\n#2. 相关工作\n深度学习框架和库的存在对于新型神经网络架构和算法的出现至关重要.\n然而, 尽管线性代数编译器的分析和经验启发式方法取得了进步, 这些软件仍然不可避免地依赖于手工优化的子程序 (例如 cuBLAS 和 cuDNN).\n这导致了各种用于 DNN 的 DSL 和编译器的开发, 通常基于三种不同的方法:\n\nXLA 和 Glow 使用 Tensor 级别的 IR 将 Tensor 程序转换为预定义的 LLVM-IR 和 CUDA-C 操作模板 (例如, Tensor 收缩、逐元素操作等), 通过模式匹配的方法.\nTensor Comprehension 和 Diesel 使用 Polyhedral Machinery 参数化和自动化地将一个或多个 DNN 层编译成 LLVM-IR 和 CUDA-C 程序.\nHalide 和 TVM 使用 循环合成 将 Tensor 计算转换为 loop nests, 同时支持用户微调调度 (尽管可能是参数化的).\n\n相比之下, Triton 依赖于在传统编译管道中添加基于 Tile 级别的操作和优化.\n这种方法提供了 (1) 比 XLA 和 Glow 更高的灵活性; (2) 与 TC 和 Diesel 相反, 支持非仿射 Tensor 索引; (3) 自动推断可能的执行调度, 否则必须手动指定给 Halide 或 TVM.\nTriton 的好处是以增加编程工作量为代价的–请参阅表 2, 其中包含这些 DSL 中矩阵乘法的实现.\n表2. 在TF, PlaidML, TC 和 TVM 中计算 C=A×BTC = A \\times B^TC=A×BT\nC = tf.matmul(A, tf.transpose(B))   // TFC[i, j: I, J] = +(A[i, k] * B[j, k]) ; // PlaidMLC(i, j) +=! A(i, k) * B(j, k) // TCtvm.sum(A[i, k] * B[j, k], axis=k) // TVM\n#3. Triton-C\nTriton-C 的目的是为现有/未来的 DNN 转译器, 以及熟悉低级 GPU 编程的程序员提供一个稳定的前端.\n在本节中, 我们描述了 Triton-C 的 CUDA 类似语法 (第 3.1 节)、其 Numpy 类似语义 (第 3.2 节) 及其单程序多数据流 (SPMD) 编程模型 (第 3.3 节).\n#3.1 语法\nTriton-C 的语法基于 ANSI C (更具体地说是 CUDA-C) 的语法, 但为了适应下两个小节中描述的语义和编程模型, 进行了修改和扩展 (参见列表 3).\n这些更改可分为以下几类:\nTile 声明: 我们添加了用于声明多维数组的特殊语法 (例如, int tile[16, 16]), 以强调其与 ANSI C 中嵌套数组 (例如 int tile[16][16]) 的语义差异.\nTile 形状必须是常量, 但也可以使用关键字使其参数化.\n一维整数 Tile 可以使用省略号进行初始化 (例如, int range[8] = 0...8).\n内置函数: 常见的 C 语法用于元素级数组操作 (如+、*等) 仍然保留, 但添加了多种内置函数 (如 get_global_range) 以支持 tile 语义 (第 3.2.1 节) 和 SPMD 编程模型.\n广播: N 维 Tile 可以使用 newaxis 关键字沿着任何特定轴进行广播 (例如 int broadcast[8, 8] = range[:, newaxis] 用来堆叠列).\n注意, 切片 tile 以检索标量或子数组是禁止的.\n谓词: 在 Tile 操作中的基本控制流 (第 4.3 节) 是通过使用带 @ 前缀的谓词语句实现的.\n表3. Triton-C 的语法扩展:\n// Broadcasting semanticsslice       : &#x27;:&#x27;   | &#x27;newaxis&#x27;slice_list  : slice | slice_list &#x27;,&#x27; sliceslice_expr  : postfix_expr | expr &#x27;[&#x27; slice_list &#x27;]&#x27;// Range initializationconstant_range : expr &#x27;...&#x27; expr// Intrinsicsglobal_range    : &#x27;get_global_range&#x27;  &#x27;(&#x27; constant      &#x27;)&#x27;dot             : &#x27;dot&#x27;               &#x27;(&#x27; expr &#x27;,&#x27; expr &#x27;)&#x27;trans           : &#x27;trans&#x27;             &#x27;(&#x27; expr &#x27;,&#x27; expr &#x27;)&#x27;intrinsic_expr  : global_range | dot | trans// Predicationpredicate_expr : &#x27;@&#x27; expr// Tile extensions for abstract declaratorsabstract_decl : abstract_decl | &#x27;[&#x27; constant_list &#x27;]&#x27;// Extensions of C expressionsexpr          : expr | constant_range | slice_expr | intrinsic_expr// Extensions of C specifiersstorage_spec  : storage_spec  | &#x27;kernel&#x27;type_spec     : type_spec     | &#x27;tunable&#x27;// Extensions of C statementsstatement     : statement | predicate_expr statement\n#3.2 语义\n#3.2.1 Tile 语义\nTriton-C 中内置 Tile 类型和操作 (即 Tile 语义) 的存在提供了两个主要优势.\n\n通过隐藏 Intra-Tile Memory Coalecsing、缓存管理和特化硬件利用 等重要的硬件细节来简化 Tensor 程序的结构.\n为编译器自动执行这些优化打开了大门, 如第 5 节所述.\n\n#3.2.2 广播语义\nTriton-C 中的 tile 具有强类型特性, 某些指令静态地要求其操作数必须满足严格的形状约束.\n例如, 除非先进行适当的广播, 否则标量不能与数组相加.\n广播语义 提供了一套规则来隐式执行这些转换 (示例参见列表 4):\n\n填充: 最短操操作数的形状用全 1 进行左填充, 直到两个操作数具有相同的维数.\n广播: 两个作数会根据需要被复制多次, 直到它们的形状相同; 如果无法完成, 则会发出错误.\n\n表 4. 广播语义的实际应用\nint a[16], b[32, 16], c[16, 1];// a is first reshaped to [1, 16]// and then broadcast to [32, 16]int x_1[32, 16] = a[newaxis, :] + b;// Same as above but implicitlyint x_2[32, 16] = a + b;// a is first reshaped to [1, 16]// a is broadcast to [16, 16]// c is broadcast to [16, 16]int y[16, 16] = a + c;\n#3.3 编程模型\n在 GPU 上执行 CUDA代码是由 SPMD 编程模型支持的, 其中每个 Kernel 都与 launch grid 中一个可识别的 thread-block 相关联.\nTriton 编程模型类似, 但每个 Kernel 都是单线程的 – 尽管自动并行化 – 并与一组全局范围相关联, 这些范围在不同的实例之间有所不同 (见图 3).\n这种方法导致 Kernel 更简单, 其中不包含 CUDA 类似的并发原语 (Shared Memory Synchronization、线程间通信等).\n可以使用 get_global_range(axis) 内置函数查询与 Kernel 相关联的全局范围, 以便创建 Pointer 的 Tile, 如列表 1 所示.\n\n图 3. CUDA 编程模型与 Triton 编程模型之间的差异\n#4. Triton IR\nTriton-IR 是一种基于 LLVM 的中间表示 (IR), 其目的是提供一个适合于 tile 级别程序分析、转换和优化的环境.\n在这项工作中, Triton-IR 程序是在解析期间从 Triton-C 中直接构建的, 尽管它们也可以在将来直接从更高级别的 DSL 生成.\nTriton-IR 程序和 LLVM-IR 程序具有相同的高级结构 (如第 4.1 节所述), 但前者还包括一些用于块级数据流 (第 4.2 节) 和控制流 (第 4.3 节) 分析的扩展.\n这些新颖的扩展对于执行第 5 节中概述的优化以及安全地访问任意形状的 Tensor 至关重要, 如第 6 节所示.\n#4.1 结构\n#4.1.1 模块\n在最高层次上, Triton-IR 程序由一个或多个称为模块的编译基本单元组成.\n这些模块相互独立地编译, 最终由一个链接器聚合, 其作用是解析前向声明并适当地合并全局定义.\n每个模块本身由函数、全局变量、常量和其他杂项符号组成 (例如, 元数据、函数属性).\n#4.1.2 函数\nTriton-IR 函数定义由返回类型、名称和可能为空的参数列表组成.\n如果需要, 可以添加额外的可见性、对齐和链接规范符.\n函数属性 (如内联提示) 和参数属性 (如只读、别名提示) 也可以指定, 允许编译器后端通过例如更好地利用只读内存缓存等方式执行更激进的优化.\n该头后面是一个由基本块列表组成的数据体, 这些基本块之间的相互依赖关系形成了函数的控制流图 (CFG).\n#4.1.3 基本块\n基本块是直线代码序列, 其末尾只能包含所谓的终止指令 (即分支、返回).\nTriton-IR 使用静态单赋值 (SSA) 形式, 这意味着每个基本块中的每个变量必须 (1) 只被赋值一次, 并且 (2) 在使用之前必须被定义.\n通过这种方式, 每个基本块隐式地定义了一个数据流图 (DFG), 其不同路径对应于程序 SSA 表示中的 def-use 链.\n这种形式可以直接从抽象语法树 (AST) 中创建.\n#4.2 Tile 级别 DFA 支持\n#4.2.1 类型\n多维 Tile 是 Triton-IR 中数据流分析的核心, 可以使用与 LLVM-IR 中向量声明相似的语法进行声明.\n例如, i32&lt;8, 8&gt; 表示对应于 8x8 32 位整数 Tile.\n请注意, Triton-IR 中没有 tunable 关键字, 因此必须在生成程序之前求解参数化形状值.\n在我们的案例中, 这是通过 Triton-JIT 的自动调优器 (第 5.3 节) 完成的.\n#4.2.2 指令\nTriton-IR 引入了一套 retiling 指令, 其目的是支持第 3.2.2 节中描述的广播语义:\n\nreshape 指令使用其输入参数的数据创建指定形状的 Tile. 这特别适用于通过用 1 填充其输入形状来重新解释变量为高维数组, 为隐式或显式广播做准备.\nbroadcast 指令通过在其输入参数上沿大小为 1 的维度进行必要的复制来创建指定形状的 Tile --如图 4 所示.\n\n常规的标量指令 (如 cmp, getelementptr, add, load) 被保留并扩展, 表达对 Tile 操作数的逐元素操作.\n最后, Triton-IR 还暴露了用于转置 (trans) 和矩阵乘法 (dot) 的专用算术指令.\n\n图 4. broadcast&lt;3, 3&gt; 指令\n#4.3 对 Tile 级别控制流分析的支持\nTriton-IR 中存在 Tile 级别操作所带来的一个问题是 Tile 内部发散控制流的不可表达性.\n例如, 程序可能需要部分保护 Tile 级别 Load 以防止内存访问违规, 但这是无法通过分支实现的, 因为 Tile 元素不能单独访问.\n我们提出通过使用 Predicated-SSA (PSSA) 形式和 ψ-函数 来解决这个问题.\n这需要在 Triton-IR 中添加两个指令类 (参见列表 6):\n\ncmpp 指令与普通的比较(cmp)指令类似, 不同之处在于它们返回两个相反的谓词, 而不是一个.\npsi 指令合并来自不同谓词指令流的指令.\n\n; pt[i, j], pf[i, j] = (true, false) if x[i, j] &lt; 5; pt[i, j], pf[i, j] = (false, true) if x[i, j] &gt;= 5%pt, %pf = icmpp slt %x, 5@%pt %x1 = add %y, 1@%pf %x2 = sub %y, 1; merge values from different predicates%x = psi i32 &lt;8, 8&gt; [%pt, %x1], [%pf, %x2]%z = mul i32 &lt;8, 8&gt; %x, 2\n#5. Triton-JIT 编译器\nTriton-JIT 的目标是将 Triton-IR 程序简化并编译成高效的机器码, 通过一系列与机器无关 (第 5.1 节) 和与机器相关的 (第 5.2 节) 的 passes, 并由自动调优引擎 (第 5.3 节) 支持.\n#5.1 与机器无关的 passes\n#5.1.1 预取\n循环内部的 Tile 级内存操作可能会存在问题, 因为它们可能会引入严重的延迟, 如果没有足够的独立指令则无法隐藏.\n然而, 通过检测循环并在必要时添加适当的预取代码, 可以直接在 Triton-IR 中缓解这个问题 (参见列表 7).\n表 7. 自动预取\nB0:  %p0 = getelementptr %1, %2B1:  %p = phi [%p0, B0], [%p1, B1]  %x = load %p  ; increment pointer  %p1 = getelementptr %p\nB0:  %p0 = getelementptr %1, %2  %x0 = load %p0B1:  %p = phi [%p0, B0], [%p1, B1]  %x = phi [%x0, B0], [%x1, B1]  ; increment pointer  %p1 = getelementptr %p, %3  ; prefetching  %x1 = load %p\n#5.1.2 Tile 级窥孔优化\nTriton-IR 中存在 Tile 级别的操作, 为窥视优化器提供了新的机遇.\n例如, 可以使用恒等式 X=(XT)TX = (X^T)^TX=(XT)T 简化任何 Tile X 的转置链.\n我们相信, 未来还可以利用对角 Tile 等其他代数性质做优化.\n#5.2 与机器相关的优化\n我们现在介绍一套针对遵循图 5 所示高级模型的机器的优化过程.\n具体而言, Triton-JIT 执行的优化包括: (1) Hierarchical Tiling, (2) Memory Coalecsing, (3) Shared Memory Allocation, (4) Shared Memory Synchronization.\n#5.2.1 Hierarchical Tiling\n嵌套 Tile 策略 (见图 5) 旨在将 Tile 分解为 Micro-Tile, 最终分解为 Nano-Tile, 以尽可能紧密地适应机器的计算能力和内存层次结构.\n虽然这种技术在自动调优框架中常规使用, 但 Triton-IR 的结构使其能够自动枚举和优化任何可表达程序的有效的嵌套 Tile 配置 (且无需多边形机制).\n\n图 5. Triton-IR 机器模型中的 Hierarchical Tiling\n#5.2.2 Memory Coalecsing\n当相邻线程同时访问邻近的内存位置时, 内存访问被认为是合并的.\n这很重要, 因为内存通常是从 DRAM 中以大块的形式检索的.\n由于 Triton-IR 程序是单线程的并且自动并行化的, 我们的编译器后端能够在每个微块内部内部排序线程, 以便在可能的情况下避免非合并的内存访问.\n这种策略减少了加载一个块列所需的内存事务数量 (见图 6).\n\n图 6. Uncoalesced 和 coalesced 的 DRAM 访问. 不同线程以不同颜色显示.\n#5.2.3 Shared Memory Allocation\n对于具有高算术强度的块级操作 (例如, dot), 可以将它们的操作数临时存储在快速共享内存中.\nShared Memory Allocation 阶段的目的在于确定何时以及何地将块存储到该空间中.\n如图 7 所示, 这可以通过首先计算每个感兴趣变量的活动范围, 然后使用 Algorithms for Compile-time Memory Optimization 文献中提出的线性时间存储分配算法来完成.\n\n图 7. Shared Memory Allocation\n#5.2.4 Shared Memory Synchronization\n在我们的机器模型中, 对共享内存的读取和写入是异步的.\nShared Memory Synchronization 阶段的目的是自动在生成的 GPU 源代码中插入 Barrier, 以保持程序的正确性.\n这是通过使用前向数据流分析来检测读后写 (RAW) 和写后读 (WAR) 风险, 并使用以下数据流方程式来完成的:\nins(RAW)=⋃p∈pred(s)outp(RAW)ins(WAR)=⋃p∈pred(s)outp(WAR)outs(RAW)={∅,if ins(RAW)∩read(s)≠∅ins(RAW)∪write(s),otherwiseouts(WAR)={∅,if ins(WAR)∩write(s)≠∅ins(WAR)∪read(s),otherwise\\mathrm{in}_s^{(\\mathrm{RAW})}\n=\n\\bigcup_{p \\in \\mathrm{pred}(s)}\n\\mathrm{out}_p^{(\\mathrm{RAW})}\n\\\\\n\n\\mathrm{in}_s^{(\\mathrm{WAR})}\n=\n\\bigcup_{p \\in \\mathrm{pred}(s)}\n\\mathrm{out}_p^{(\\mathrm{WAR})}\n\\\\\n\n\\mathrm{out}_s^{(\\mathrm{RAW})}\n=\n\\begin{cases}\n\\varnothing,\n    &amp; \\textbf{if } \\mathrm{in}_s^{(\\mathrm{RAW})} \\cap \\mathrm{read}(s) \\neq \\varnothing \\\\\n\\mathrm{in}_s^{(\\mathrm{RAW})} \\cup \\mathrm{write}(s),\n    &amp; \\textbf{otherwise}\n\\end{cases}\n\\\\\n\n\\mathrm{out}_s^{(\\mathrm{WAR})}\n=\n\\begin{cases}\n\\varnothing,\n    &amp; \\textbf{if } \\mathrm{in}_s^{(\\mathrm{WAR})} \\cap \\mathrm{write}(s) \\neq \\varnothing \\\\\n\\mathrm{in}_s^{(\\mathrm{WAR})} \\cup \\mathrm{read}(s),\n    &amp; \\textbf{otherwise}\n\\end{cases}\nins(RAW)​=p∈pred(s)⋃​outp(RAW)​ins(WAR)​=p∈pred(s)⋃​outp(WAR)​outs(RAW)​={∅,ins(RAW)​∪write(s),​if ins(RAW)​∩read(s)=∅otherwise​outs(WAR)​={∅,ins(WAR)​∪read(s),​if ins(WAR)​∩write(s)=∅otherwise​\n#5.3 自动调优器\n传统自动调优器通常依赖于手工编写的参数化代码模板, 以在预定义的工作负载上实现良好性能.\n相比之下, Triton-JIT 可以通过简单地将与每个优化过程相关的元参数连接起来, 直接从 Triton-IR 程序中提取优化空间.\n在本工作中, 仅考虑了 Hierarchical Tiling 过程, 导致每个 Tile 每个维度最多有 3 个 Tile 参数. 然后使用穷举搜索优化这些参数, 搜索范围为:\n\nTile 大小: 32 到 128 之间的 2 的幂次方;\nMicro-Tile 大小: 8 到 32 之间的 2 的幂次方;\nNano-Tile 大小: 1 到 4 之间的 2 的幂次方.\n\n未来可以使用更好的自动调优方法.\n#6. 数值实验\n在本节中, 我们评估了 Triton 在各种深度学习文献中的工作负载上的性能.\n我们使用 NVIDIA GeForce GTX1070, 并将我们的系统与最新的供应商库 (cuBLAS 10.0, cuDNN 7.0) 以及相关的编译器技术 (AutoTVM, TC, PlaidML) 进行了比较.\n在适用的情况下, 我们根据官方文档指南, 针对每个单独的问题大小对这些 DSL 进行自动调优.\n#6.1 矩阵乘法\n矩阵乘法任务的形式: A=D×W(D∈R,W∈R)A = D \\times W (D \\in R, W \\in R)A=D×W(D∈R,W∈R) 是神经网络计算的核心.\n这里我们考虑了从 Recurrent (DeepSpeech2) 到 Transformer 的各种任务; 我们在图 8 中报告了它们的性能.\n\nTriton 和 cuBLAS 通常性能相当, 并在某些任务上达到了设备峰值性能的 90%以上.\ncuBLAS 在较浅的 Transformer 神经网络中仍然比 Triton 更快, 这得益于使用了一种 3D 算法, 该算法将深度归约分解为独立块, 以在 M 和 N 过小时提供更多的并行性.\n否则, 现有的 DSL 比我们的解决方案慢 2-3 倍\n当输入形状是 32 的倍数时, TVM 慢 &lt;2 倍.\n\n\n图 8. 矩阵乘法的性能\n#6.2 卷积\nCNN 是一类重要的机器学习模型, 需要得到 DSLs 和编译器的良好支持.\n它们基于卷积层 (图 9a), 其作为矩阵乘法 (图 9b) 的实现是利用专用 Tensor 处理硬件的必要条件–然而现有的 DSLs 并未支持这一点.\n在这里, 我们基准测试了 Triton 对 cuDNN 的 IMPLICIT_GEMM 算法 (第 6.2.1 节) 的重新实现, 并提供了第一个用于移位卷积的融合 Kernel (第 6.2.2 节).\n我们使用指针增量查找表实现了这些例程, 如列表 8 所示.\n\n图 9. 密集和移位卷积层 (a) 被视为矩阵乘法 (b)\n#6.2.1 密集卷积\n本节考虑的卷积层来自深度学习文献, 如下表所示.\n\n\n\n\nH\nW\nC\nB\nK\nR\nS\n应用场景\n\n\n\n\nTask 1\n112\n112\n64\n4\n128\n3\n3\nResNet\n\n\nTask 2\n56\n56\n128\n4\n256\n3\n3\nResNet\n\n\nTask 3\n28\n28\n256\n4\n512\n3\n3\nResNet\n\n\nTask 4\n14\n14\n512\n4\n512\n3\n3\nResNet\n\n\nTask 5\n7\n7\n512\n4\n512\n3\n3\nResNet\n\n\nTask 6\n161\n700\n1\n8\n64\n5\n5\nDeepSpeech2\n\n\nTask 7\n79\n341\n32\n8\n32\n5\n10\nDeepSpeech2\n\n\n\n如图 10 所示, Triton 在 ResNet 的 IMPLICIT_GEMM 的 cuDNN 实现上表现更优.\n这可能是因为 cuDNN 也维护了更好的 3 x 3 卷积算法 (即 Winograd),\n对于重要性较低的核心留下的工程优化空间较小.\n当快速算法不可用时 (例如 DeepSpeech2), cuDNN 和 Triton 表现相当.\n\n图 10. 隐式矩阵乘法性能\n#6.2.2 移位卷积\n最后, 我们考虑表 1 中 Task1-5 的一种实现方式为移位卷积–这是一种针对 CNN 的新方法 (见图 9a).\n我们将 Triton 中融合的移位-卷积模块实现 (见清单 8) 与依赖手写移位核心和单独调用 cuBLAS 的简单实现进行了比较.\n我们还报告了在不移位时 (即 1×1 卷积) 可达到的最大性能.\n如图 11 所示, 我们的 Triton 实现几乎完全隐藏了移位的成本.\n\n图 11. Triton 中移位卷积的性能\n表 8. Triton-C 中的移位卷积\nconst tunable int TM = &#123;16, 32, 64, 128&#125;;const tunable int TN = &#123;16, 32, 64, 128&#125;;const tunable int TK = &#123;8&#125;;__constant__ int* delta = alloc_const int[512];for (int c = 0; c &lt; C; c++)    delta[c] = c * H * W + shift_h[c] * W + shift_w[c];void shift_conv(restrict read_only float* a,                restrict read_only float* b, float* c,                int M, int N, int K) &#123;    int rxa[TM] = get_global_range[TM](0);    int ryb[TN] = get_global_range[TN](1);    int rka[TK] = 0 ... TK;    int rkb[TK] = 0 ... TK;    float C[TM, TN] = 0;    float* pxa[TM, TK] = a + rxa[:, newaxis];    float* pb[TN, TK] = b + ryb[:, newaxis] + rkb * N;    __constant__ int* pd[TK] = delta + rka;    for (int k = K; k &gt; 0; k = k - TK) &#123;        int delta[TK] = *pd;        float* pa[TM, TK] = pxa + delta[newaxis, :];        float a[TM, TK] = *pa;        float b[TN, TK] = *pb;        C = dot(a, trans(b), C);        pb = pb + TK * N;        pd = pd + TK;    &#125;    int rxc[TM] = get_global_range[TM](0);    int ryc[TN] = get_global_range[TN](1);    float* pc[TM, TN] = c + rxc[:, newaxis] + ryc * M;    bool checkc0[TM] = rxc &lt; M;    bool checkc1[TN] = ryc &lt; N;    bool checkc[TM, TN] = checkc0[:, newaxis] &amp;&amp; checkc1;    @checkc * pc = C;&#125;\n#7. 结论\n在本文中, 我们介绍了 Triton, 一种开源的语言和编译器, 用于表达和编译 tile 化神经网络计算为高效的机器代码.\n我们展示了在 LLVM-IR 中仅添加少量数据流和控制流扩展即可启用各种 tile 级优化过程, 这些过程共同带来了与供应商库相当的性能.\n我们还提出了 Triton-C, 一种高级语言, 我们能够在其中简洁地实现针对 CNN 的新型神经网络架构的高效Kernel.\n未来工作的方向包括对 Tensor Core 的支持、量化 Kernel 的实现以及集成到更高级别的 DSL 中.\n#参考资料\n","categories":["mlsys"],"tags":["MLSys","System","Compiler","Triton","Machine Learning"]},{"title":"[OSDI'18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning 阅读笔记","url":"/2026/02/01/tvm-an-automated-end-to-end-optimizing-compiler-for-deep-learning/","content":"#0. 摘要\n将机器学习应用于广泛多样的硬件设备的需求日益增长.\n当前框架依赖于特定供应商的算子库, 并针对狭窄范围的服务器级 GPU 进行优化.\n将工作负载部署到新平台–例如手机、嵌入式设备和加速器 (如 FPGA、ASIC) --需要大量手动工作.\n我们提出了 TVM, 一种编译器, 它通过暴露图级别和算子级别的优化, 为深度学习工作负载在不同硬件后端提供性能可移植性.\nTVM 解决了深度学习特有的优化挑战, 例如高级算子融合、映射到任意硬件原语以及内存延迟隐藏.\n它还通过采用一种新颖的基于学习的成本建模方法来自动化低级程序的硬件特性优化, 以快速探索代码优化.\n实验结果表明, TVM 在低功耗 CPU、手机 GPU 和服务器级 GPU 等硬件后端上提供的性能, 与最先进的、手工调优的库具有竞争力.\n我们也展示了 TVM 针对新型加速器后端的能力, 例如基于 FPGA 的通用深度学习加速器.\n该系统已开源, 并在多家大型公司内部署使用.\n#1. 引言\n深度学习 (DL) 模型现在可以识别图像、处理自然语言, 并在具有挑战性的策略游戏中击败人类.\n目前, 存在将智能应用程序部署到广泛设备的需求, 这些设备范围从云服务器到自动驾驶汽车和嵌入式设备.\n将 DL 工作负载映射到这些设备因硬件特性的多样性而变得复杂, 包括嵌入式 CPU、GPU、FPGA 和 ASIC (例如 TPU).\n这些硬件目标在内存组织、计算功能单元等方面存在差异, 如图 1 所示.\n\n图 1: CPU、GPU 和类似 TPU 的加速器需要不同的片上内存架构 (on-chip memory architectures) 和计算原语 (compute primitives).\n在生成优化代码时必须解决这种差异.\n当前的深度学习框架, 如 TensorFlow、MXNet、Caffe 和 PyTorch, 依赖于计算图中间表示来实现优化, 例如自动微分和动态内存管理.\n然而, 图级别的优化通常过于抽象, 难以处理特定于硬件后端的算子级别转换.\n这些框架大多专注于狭隘的服务器级 GPU 设备, 并将特定目标的优化委托给高度工程化且供应商特定的算子库.\n这些算子级别库需要大量的手动调优, 因此过于专业化和不透明, 难以在不同硬件设备间轻松移植.\n目前, 为各种深度学习框架提供对多样化硬件后端的支持需要大量的工程工作.\n即使对于已支持的后端, 框架也必须在以下两者之间做出艰难的选择: (1) 避免会产生预定义算子库中不存在的算子的图优化, 以及 (2) 使用这些算子, 但是是未优化过的实现.\n为了实现对不同硬件后端的图级和算子级优化, 我们采用了一种根本不同的端到端方法.\n我们构建了 TVM, 这是一个编译器, 它从现有框架中获取深度学习程序的高级规范, 并为多种硬件后端生成低级优化代码.\n为了吸引用户, TVM 需要提供与跨不同硬件后端的手动优化算子库相媲美的性能.\n这一目标需要解决下面描述的关键挑战.\n利用特定硬件特性和抽象\n深度学习加速器引入了优化的张量计算原语, 而 GPU 和 CPU 则持续改进其处理单元.\n这给针对给定算子描述生成优化代码带来了重大挑战.\n硬件指令的输入是多维的, 具有固定或可变长度; 它们决定了不同的数据布局; 并且对内存层次结构有特殊要求.\n系统必须有效利用这些复杂原语以实现加速.\n此外, 加速器设计通常也更倾向于精简控制, 并将大部分调度复杂性卸载到编译器栈中.\n对于专用加速器, 系统现在需要生成显式控制流水线依赖关系的代码, 以隐藏内存访问延迟–这是硬件为 CPU 和 GPU 执行的任务.\n优化的大搜索空间\n另一个挑战是在不手动调整算子的情况下生成高效代码.\n内存访问、线程模式和新型硬件原语的综合选择为生成的代码 (例如, 循环 tiles 和排序、缓存、循环展开) 创建了一个巨大的配置空间, 如果我们实现黑盒自动调优, 这将导致巨大的搜索成本.\n人们可以采用预定义的成本模型来指导搜索, 但由于现代硬件复杂性的增加, 建立精确的成本模型很困难.\n此外, 这种方法将要求我们为每种硬件类型建立单独的成本模型.\nTVM 通过三个关键模块来解决这些挑战.\n\n我们引入了一种 张量表达式语言 来构建算子, 并提供程序变换原语, 这些原语能够生成具有各种优化的程序的不同版本.\n这一层扩展了 Halide 的计算/调度分离概念, 通过将目标硬件内建函数 (hardware intrinsics) 与变换原语分离, 从而支持新型加速器及其对应的新内建函数.\n此外, 我们引入了新的变换原语来解决与 GPU 相关的挑战, 并支持在专用加速器上部署.\n然后, 我们可以应用不同序列的程序变换, 为给定的算子声明形成丰富的有效程序空间.\n我们引入了一个 自动化的程序优化框架 来寻找优化的张量算子.\n优化器在基于机器学习的成本模型的指导下进行工作, 该模型会随着我们从硬件后端收集更多数据而自适应和改进.\n在自动代码生成器的基础上, 我们引入了一个 图重写器, 该重写器充分利用了高层 (high-) 和算子层 (operator-level) 的优化.\n\n通过结合这三个模块, TVM 可以从现有的深度学习框架中获取模型描述, 执行高低级联合优化, 并为后端 (例如 CPU、GPU 和基于 FPGA 的专用加速器) 生成硬件特定的优化代码.\n本文做出了以下贡献:\n\n我们识别了在为不同硬件后端上的深度学习工作负载提供性能可移植性时面临的主要优化挑战.\n我们引入了新的调度原语, 这些原语利用了跨线程内存重用、新的硬件内建函数和延迟隐藏.\n我们提出并实现了一个基于机器学习的优化系统, 用于自动探索和搜索优化的张量算子.\n我们构建了一个端到端的编译和优化栈, 允许在高级框架 (包括 TensorFlow、MXNet、PyTorch、Keras、CNTK) 中指定的深度学习工作负载部署到不同的硬件后端 (包括 CPU、服务器 GPU、移动 GPU 和基于 FPGA 的加速器).\n开源的 TVM 已经在几家大型公司中投入生产使用.\n我们在服务器级 GPU、嵌入式 GPU、嵌入式 CPU 和自定义通用基于 FPGA 的加速器上使用真实世界工作负载评估了 TVM.\n实验结果表明, TVM 在不同的后端上提供了可移植的性能, 并实现了比现有基于手工优化库的框架快 1.2x 到 3.8x 的速度提升.\n\n#2. 概述\n\n图 2: TVM 系统概述. 当前栈支持从许多深度学习框架和交换格式 (如 CoreML 和 ONNX)进行描述, 以针对主要的 CPU、GPU 和专用加速器.\n本节通过示例介绍 TVM 的各个组成部分.\n图 2 总结了 TVM 的执行步骤及其在论文中对应的章节.\n系统首先从现有框架中获取模型, 并将其转换为计算图表示.\n然后执行高级数据流重写以生成优化后的图.\n算子级优化模块必须为该图中的每个融合算子生成高效代码.\n算子通过声明式张量表达式语言指定; 执行细节不需要.\nTVM 为给定硬件目标的算子识别一系列可能的代码优化方案.\n可能的优化方案构成一个庞大的空间, 因此我们使用基于机器学习的成本模型来寻找优化后的算子.\n最后, 系统将生成的代码打包成可部署的模块.\n#终端用户示例\n只需几行代码, 用户即可从现有的深度学习框架中获取模型, 并调用 TVM API 来获取可部署的模块:\nimport tvm as t# Use keras framework as example, import modelgraph, params = t.frontend.from_keras(keras_model)target = t.target.cuda()graph, lib, params = t.compiler.build(graph, target, params)\n该编译运行时模块包含三个组件: 最终优化的计算图 (graph)、生成的算子 (lib)和模块参数 (params).\n这些组件可用于将模型部署到目标后端:\nimport tvm.runtime as tmodule = runtime.create(graph, lib, t.cuda(0))module.set_input(**params)module.run(data=data_array)output = tvm.nd.empty(out_shape, ctx=t.cuda(0))module.get_output(0, output)\nTVM 支持在 C++、Java 和 Python 等语言中的多个部署后端.\n本文的其余部分描述了 TVM 的架构以及系统程序员如何扩展它以支持新的后端.\n#3. 优化计算图\n\n图 3: 一个两层卷积神经网络的示例计算图.\n图中的每个节点代表一个操作, 该操作消耗一个或多个张量并产生一个或多个张量.\n张量操作可以通过属性进行参数化以配置其行为 (例如, 填充 padding 或步长 strides).\n计算图是表示深度学习框架中程序的一种常见方式.\n图 3 展示了一个两层卷积神经网络的计算图表示示例.\n这种高级表示与低级编译器中间表示 (如 LLVM) 的主要区别在于, 中间数据项是大型、多维的张量.\n计算图提供了操作符的全局视图, 但它们避免了指定每个操作符必须如何实现.\n与 LLVM 中间表示类似, 计算图可以转换为功能等价的图以应用优化.\n我们还利用了常见深度学习工作负载中的形状特定性来针对一组固定的输入形状进行优化.\nTVM 利用计算图表示来应用高级优化: 节点表示张量或程序输入上的操作, 边表示操作之间的数据依赖关系.\n它实现了许多图级优化, 包括: 算子融合 (operator fusion), 将多个小操作融合在一起; 常量折叠 (constant-folding), 预先计算可以静态确定的图部分, 节省执行成本; 静态内存规划阶段 (static memory planning pass), 预先分配内存来存储每个中间张量; 以及数据布局转换 (data layout transformations), 将内部数据布局转换为后端友好的形式.\n我们现在讨论算子融合和数据布局转换.\n\n图 4: 融合操作与非融合操作的性能比较. TVM 生成这两种操作. 在 NVIDIA Titan X 上测试.\n#算子融合\n算子融合将多个算子组合成一个内核, 而无需将中间结果保存在内存中.\n这种优化可以显著减少执行时间, 特别是在 GPU 和专用加速器中.\n具体来说, 我们识别了四种图算子类别: (1) 单射 (一对一映射, 例如加法), (2) 归约 (例如求和), (3) 复杂输出可融合 (可以融合元素级映射到输出, 例如 conv2d), 以及 (4) 不透明 (无法融合, 例如排序).\n我们提供了通用的融合规则, 如下所示.\n\n多个单射算子可以融合成另一个单射算子.\n一个归约算子可以与输入的单射算子融合 (例如, 融合 scale 和 sum). 像 conv2d 这样的算子是复杂输出可融合的, 我们可以将其元素级算子融合到其输出.\n\n我们可以应用这些规则将计算图转换为融合版本.\n图 4 展示了这种优化对不同工作负载的影响.\n我们发现, 融合算子通过减少内存访问, 最多可产生 1.2x 到 2x 的加速比.\n#数据布局转换\n对于一个给定的张量, 在计算图中存储的方式有多种.\n最常见的数据布局选择是列主序 (column major) 和行主序 (row major).\n在实践中, 我们可能更喜欢使用更复杂的数据布局.\n例如, 一个深度学习加速器可能会利用4x4矩阵运算, 需要将数据分块为4x4块以优化访问局部性.\n数据布局优化将计算图转换为可以在目标硬件上使用更好的内部数据布局执行的形式.\n它首先根据内存层次结构所规定的约束, 为每个算子指定首选的数据布局.\n然后, 如果生产者和消费者的首选数据布局不匹配, 我们就会在它们之间执行适当的布局转换.\n\n图 5: 示例调度转换, 优化在专用加速器上的矩阵乘法.\n虽然高级图优化可以极大地提高深度学习工作负载的效率, 但其效果仅取决于算子库所提供的内容.\n目前, 少数支持算子融合的深度学习框架需要算子库提供融合模式的实现.\n随着网络算子的不断引入, 可能的融合核的数量可能会急剧增长.\n当面向越来越多的硬件后端时, 所需的融合模式实现数量会随着必须支持的数据布局、数据类型和加速器内联函数数量的组合呈指数增长.\n手动为程序所需的各种操作以及每个后端设计算子核是不可行的.\n为此, 我们接下来提出一种代码生成方法, 该方法可以为给定模型的算子生成各种可能的实现.\n#4. 生成张量操作\nTVM 通过为每个算子生成多种有效的硬件后端实现并选择最优实现来生成高效代码.\n该过程基于 Halide 将描述与计算规则 (或调度优化) 解耦的思想 , 并将其扩展以支持新的优化 (嵌套并行、张量化和延迟隐藏) 以及广泛的硬件后端.\n我们现在重点介绍 TVM 特有的功能.\n#4.1 张量表达式与调度空间\n\n图 6: TVM 调度 lowering 和代码生成过程.\n该表列出了现有的 Halide 和新的 TVM 调度原语, 用于优化 CPU、GPU 和加速器后端的调度.\n张量化对于加速器至关重要, 但也可以用于 CPU 和 GPU.\n特殊的内存作用域能够实现 GPU 中的内存重用, 以及在加速器中对片上内存进行显式管理.\n延迟隐藏是针对类似 TPU 的加速器的.\n我们介绍一种张量表达式语言, 以支持自动代码生成.\n与高级计算图表示不同, 其中张量操作的实现是不透明的, 每个操作都在一个索引公式表达式语言中描述.\n以下代码展示了一个计算转置矩阵乘法的张量表达式示例:\nm, n, h = t.var(&#x27;m&#x27;), t.var(&#x27;n&#x27;), t.var(&#x27;h&#x27;)A = t.placeholder((m, h), name=&#x27;A&#x27;)B = t.placeholder((n, h), name=&#x27;B&#x27;)k = t.reduce_axis((0, h), name=&#x27;k&#x27;)C = t.compute((m, n), lambda y, x:t.sum(A[k, y] * B[k, x], axis=k))\n每个计算操作都指定了输出张量的形状以及描述如何计算其每个元素的算术表达式.\n我们的张量表达式语言支持常见的算术和数学运算, 涵盖了常见的深度学习算子模式.\n该语言不指定循环结构和其他许多执行细节, 并为针对不同后端添加硬件感知优化提供了灵活性.\n我们采用 Halide 中的计算/调度解耦原则, 使用调度来表示从张量表达式到低级代码的特定映射.\n有许多可能的调度可以实现这一功能.\n我们通过逐步应用保留程序逻辑等价性的基本转换 (调度原语) 来构建调度.\n图 5 展示了一个在专用加速器上调度矩阵乘法的例子.\n内部, TVM 使用一种数据结构来跟踪循环结构和其他信息, 随着我们应用调度转换.\n这些信息随后可以帮助为给定的最终调度生成低级代码.\n我们的张量表达式借鉴了 Halide、Darkroom 和 TACO 的设计.\n其主要增强功能包括支持下面讨论的新调度优化.\n为了在许多后端上实现高性能, 我们必须支持足够的调度原语, 以涵盖不同硬件后端上的各种优化.\n图 6 总结了 TVM 支持的指令代码生成过程和调度原语.\n我们从 Halide 重用了有用的原语和低级循环程序抽象语法树, 并引入了新的原语来优化 GPU 和加速器的性能.\n这些新原语对于实现最佳 GPU 性能是必要的, 对于加速器也是必不可少的.\nCPU、GPU、TPU 类加速器是深度学习的三种重要硬件类型.\n本节描述了针对 CPU、GPU 和 TPU 类加速器的新优化原语, 而第 5 节则解释了如何自动推导高效的调度.\n#4.2 基于合作的嵌套并行\n并行性是提高深度学习工作负载中计算密集型内核效率的关键.\n现代 GPU 提供了巨大的并行性, 要求我们将并行模式嵌入到调度转换中.\n大多数现有解决方案采用一种称为 嵌套并行 (nested parallelism) 的模型, 这是一种形式的 fork-join.\n该模型需要一个并行调度原语来并行化数据并行任务; 每个任务可以进一步递归地细分为子任务, 以利用目标架构的多级线程层次结构 (例如 GPU 中的线程组).\n我们称此模型为 无共享嵌套并行 (shared-nothing nested parallelism), 因为同一并行计算阶段中的一个工作线程不能查看其兄弟线程的数据.\n\n图 7: 在矩阵乘法工作负载上, 带有和未带有协同共享内存获取的 TVM 性能比较.\n在 NVIDIA Titan X 上测试.\n一种 shared-nothing 的替代方法是协同获取数据.\n具体而言, 一组线程可以协同获取它们所需的所有数据, 并将其放置到一个共享内存空间中.\n这种优化可以利用 GPU 内存层次结构, 并通过共享内存区域实现跨线程的数据重用.\nTVM 使用调度原语支持这种著名的 GPU 优化, 以实现最佳性能.\n以下 GPU 代码示例优化了矩阵乘法.\nfor thread_group (by, bx) in cross(64, 64):  for thread_item (ty, tx) in cross(2, 2):    local CL[8][8] = 0    shared AS[2][8], BS[2][8]    for k in range(1024):      # 所有线程协作式地通过不同的 pattern 加载 AS 和 BS      for i in range(4):        AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx]      for each i in 0..4:        BS[ty][i*4+tx] = B[k][bx*64+ty*8+i*4+tx]      # 编译器自动插入的 Barrier      memory_barrier_among_threads()      for yi in range(8):        for xi in range(8):          CL[yi][xi] += AS[yi] * BS[xi]      for yi in range(8):        for xi in range(8):          C[yo*8+yi][xo*8+xi] = CL[yi][xi]\n图 7 展示了这种优化的影响.\n我们将内存作用域的概念引入调度空间, 以便可以将计算阶段 (代码中的 AS 和 BS) 标记为共享.\n在没有显式内存作用域的情况下, 自动作用域推断会将计算阶段标记为线程局部.\n共享任务必须计算组中所有工作线程的依赖关系.\n此外, 必须正确插入内存同步屏障, 以确保共享加载的数据对消费者可见.\n最后, 除了对 GPU 有用外, 内存作用域还允许我们标记特殊内存缓冲区, 并在针对专用 DL 加速器时创建特殊优化规则.\n#4.3 张量化\n深度学习工作负载具有高算术强度, 通常可以分解为矩阵-矩阵乘法或 1D 卷积等张量运算.\n这些自然分解导致了近期添加张量计算原语的趋势.\n这些新原语为基于调度的编译带来了机遇和挑战; 虽然使用它们可以提高性能, 但编译框架必须无缝集成它们.\n我们将此过程称为张量化 (tensorization): 它类似于 SIMD 架构的向量化 (vectorization), 但存在显著差异.\n指令输入是多维的, 具有固定或可变长度, 并且每个输入具有不同的数据布局.\n更重要的是, 我们不能支持一组固定的原语, 因为新的加速器正在涌现, 它们具有各自不同的张量指令变体.\n因此, 我们需要一个可扩展的解决方案.\n我们通过使用张量内建函数声明机制, 将目标硬件内建函数与调度分离, 使张量化过程可扩展.\n我们使用相同的张量表达式语言来声明每个新硬件内建函数的行为及其相关的降低规则.\n以下代码展示了如何声明一个 8x8 张量硬件内建函数.\nw, x = t.placeholder((8, 8)), t.placeholder((8, 8))k = t.reduce_axis((0, 8))y = t.compute((8, 8), lambda i, j: t.sum(w[i, k] * x[j, k], axis=k))def gemm_intrin_lower(inputs, outputs):  ww_ptr = inputs[0].access_ptr(&quot;r&quot;)  xx_ptr = inputs[1].access_ptr(&quot;r&quot;)  zz_ptr = outputs[0].access_ptr(&quot;w&quot;)  compute = t.hardware_intrin(&quot;gemm8x8&quot;, ww_ptr, xx_ptr, zz_ptr)  reset = t.hardware_intrin(&quot;fill_zero&quot;, zz_ptr)  update = t.hardware_intrin(&quot;fuse_gemm8x8_add&quot;, ww_ptr, xx_ptr, zz_ptr)  return compute, reset, updategemm8x8 = t.decl_tensor_intrin(y.op, gemm_intrin_lower)\n此外, 我们引入了一种张量化调度原语, 用以将一个计算单元替换为相应的内建函数.\n编译器将计算模式与硬件声明进行匹配, 并将其转换为相应的硬件内建函数.\n张量化将调度与特定的硬件原语解耦, 使得扩展 TVM 以支持新的硬件架构变得容易.\n张量化调度生成的代码与高性能计算中的实践一致: 将复杂操作分解为一系列微内核调用.\n我们还可以使用 张量化 (tensorize) 原语来利用手工制作的微内核, 这在某些平台上可能有益.\n例如, 我们通过利用位串行矩阵向量乘法微内核, 为在宽度为一或两位的数据类型上运行的移动 CPU 实现了超低精度运算.\n该微内核将结果累积到逐渐增大的数据类型中, 以最小化内存占用.\n将微内核作为张量内置于 TVM 中, 与非张量化版本相比, 可加速高达 1.5x.\n#4.4 显式内存延迟隐藏\n延迟隐藏 (Latency hiding) 是指将内存操作与计算重叠的过程, 以最大化内存和计算资源的利用率.\n它需要根据目标硬件后端采用不同的策略.\n在 CPU 上, 内存延迟隐藏通过同时多线程或硬件预取隐式实现.\nGPU 依赖于多条线程束的快速上下文切换.\n相比之下, TPU 等专用深度学习加速器通常倾向于更精简的控制, 采用 解耦访问-执行 (decoupled access-execute, DAE) 架构, 并将细粒度同步问题卸载到软件.\n\n图 9: 硬件中的解耦访问-执行隐藏了大部分内存访问延迟, 通过允许内存和计算重叠实现.\n执行正确性通过依赖令牌入队/出队形式的低级同步来强制执行, 编译器栈必须在指令流中插入这些操作.\n图 9 展示了一个能够减少运行时延迟的 DAE 硬件流水线.\n与单体式硬件设计相比, 该流水线能够隐藏大部分内存访问开销, 并几乎完全利用计算资源.\n为了实现更高的利用率, 指令流必须增加细粒度的同步操作.\n如果没有这些操作, 依赖关系无法得到保证, 从而导致执行错误.\n因此, DAE 硬件流水线需要在流水线阶段之间进行细粒度的依赖入队/出队操作, 以确保正确执行, 如图 9 中的指令流所示.\n\n图 8: TVM 虚拟线程 lowering 将虚拟线程并行程序转换为单指令流; 该流包含硬件可解释的显式 low-level 同步, 以恢复隐藏内存访问延迟所需的流水线并行性.\n为 DAE 加速器编程需要显式的低级同步, 这很困难.\n为了减轻编程负担, 我们引入了一种虚拟线程调度原语, 允许程序员像指定硬件后端一样指定高级数据并行程序, 并支持多线程.\nTVM 随后自动将程序降低为具有低级显式同步的单指令流, 如图 8 所示.\n该算法从高级多线程程序调度开始, 然后插入必要的低级同步操作, 以保证每个线程内的正确执行.\n接下来, 它将所有虚拟线程的操作交错到一个单指令流中.\n最后, 硬件从指令流中的低级同步操作中恢复可用的流水线并行性.\n#延迟隐藏的硬件评估\n\n图 10: 基于 FPGA 的深度学习加速器运行 ResNet 推理时的屋顶线.\n通过 TVM 启用的延迟隐藏功能, 基准测试的性能更接近屋顶线, 展示了更高的计算和内存带宽效率.\n我们现在展示延迟隐藏在基于定制 FPGA 的加速器设计上的有效性, 该设计在 6.4 小节中进行了详细描述.\n我们在加速器上运行了 ResNet 的每一层, 并使用 TVM 生成了两个调度方案: 一个带有延迟隐藏, 一个没有.\n带有延迟隐藏的调度方案通过虚拟线程并行化程序, 以暴露流水线并行性并因此隐藏内存访问延迟.\n结果如图 10 所示, 以屋顶线图的形式呈现; 屋顶线性能图提供了关于给定系统如何利用计算和内存资源为不同基准测试提供见解.\n总体而言, 延迟隐藏提高了所有 ResNet 层的性能.\n峰值计算利用率从没有延迟隐藏时的 70%增加到带有延迟隐藏时的 88%.\n#5. 自动化优化\n鉴于丰富的调度原语, 我们剩余的问题是为深度学习模型的每一层找到最优算子实现.\n在此, TVM 为与每一层相关的特定输入形状和布局创建专用算子.\n这种专用化提供了显著的性能优势 (与针对形状和布局多样性较小的手工代码相比), 但也带来了自动化挑战.\n系统需要选择调度优化–例如修改循环顺序或针对内存层次结构进行优化–以及调度特定参数, 如tiles大小和循环展开因子.\n此类组合选择为每个硬件后端创造了大量的算子实现搜索空间.\n为应对这一挑战, 我们构建了一个自动调度优化器, 包含两个主要组件:\n\n一个 提出 有前景新配置的调度探索器\n一个 预测 给定配置性能的机器学习成本模型\n\n本节将介绍这些组件以及 TVM 的自动优化流程 (图 11).\n#5.1 调度空间规范\n我们构建了一个调度模板规范 API, 允许开发者在调度空间中声明调节器.\n模板规范在指定可能的调度时, 允许开发者根据需要融入其领域特定知识.\n我们还为每个硬件后端创建了一个通用主模板, 该模板能够根据使用张量表达式语言表达的计算描述自动提取可能的调节器.\n从高层次来看, 我们希望尽可能多地考虑配置, 并让优化器管理选择负担.\n因此, 优化器必须针对我们实验中使用的真实世界深度学习工作负载搜索数十亿种可能的配置.\n#5.2 基于机器学习的成本模型\n一种从大型配置空间中寻找最佳调度方案的方法是通过黑盒优化, 即自动调优.\n该方法用于调优高性能计算库.\n然而, 自动调优需要大量实验来识别良好配置.\n另一种方法是构建预定义的成本模型来指导特定硬件后端的搜索, 而不是运行所有可能性并测量其性能.\n理想情况下, 完美的成本模型考虑了所有影响性能的因素: 内存访问模式、数据重用、流水线依赖关系和线程模式等.\n不幸的是, 由于现代硬件复杂性的增加, 这种方法负担沉重.\n此外, 每个新的硬件目标都需要一个新的 (预定义的)成本模型.\n\n图 11: 自动化优化框架概述.\n调度探索器使用基于机器学习的成本模型检查调度空间, 并通过 RPC 选择在分布式设备集群上运行的实验.\n为了提高其预测能力, 机器学习模型使用数据库中记录的收集数据定期更新.\n\n\n\n方法类别\n数据成本\n模型偏差\n需要硬件信息\n从历史中学习\n\n\n\n\n黑盒自动调优 Blackbox auto-tuning\n高\n无\n否\n否\n\n\n预定义成本模型 Predefined cost model\n无\n高\n是\n不\n\n\n基于机器学习的成本模型 ML based cost model\n低\n低\n不\n是\n\n\n\n表 1: 自动化方法的比较.\n模型偏差是指由于建模导致的 inaccuracy.\n我们采用统计方法来解决这个问题.\n在这种方法中, 一个调度探索器提出可能提高算子性能的配置.\n对于每个调度配置, 我们使用一个机器学习模型, 该模型以降级后的循环程序为输入, 并预测其在给定硬件后端的运行时间.\n该模型使用在探索过程中收集的运行时测量数据训练, 不需要用户输入详细的硬件信息.\n在优化过程中, 我们随着探索更多配置而定期更新模型, 这提高了与其他相关工作负载的准确性.\n通过这种方式, 随着更多实验试验的增加, 机器学习模型的质量也随之提高.\n表 1 总结了自动化方法之间的主要差异.\n基于机器学习的成本模型在自动调优和预定义成本建模之间取得了平衡, 并且可以从相关工作负载的历史性能数据中受益.\n#机器学习模型设计选择\n\n图 12: 在 TITAN X 上对 ResNet-18 中 conv2d 算子不同自动化方法的比较.\n基于 ML 的模型从无训练数据开始, 并使用收集到的数据来提升自身性能.\nY 轴表示相对于 cuDNN 的速度提升.\n我们观察到其他工作负载也呈现出类似的趋势.\n在选择调度探索器将使用的机器学习模型时, 我们必须考虑两个关键因素: 质量和速度.\n调度探索器频繁查询成本模型, 这会导致由于模型预测时间和模型重新拟合时间而产生的开销.\n为了有效, 这些开销必须小于在真实硬件上测量性能所需的时间, 该时间可能因具体的工作负载/硬件目标而异, 通常为秒级.\n这一速度要求将我们的问题与传统的超参数调整问题区分开来, 在后者中, 测量成本相对于模型开销非常高, 可以使用更昂贵的模型.\n除了模型的选择外, 我们还需要选择一个用于训练模型的目标函数, 例如配置预测运行时间的误差.\n然而, 由于探索器仅根据预测的相对顺序选择最佳候选 (A 比 B 运行更快), 因此我们无需直接预测绝对执行时间.\n相反, 我们使用排序目标来预测运行成本相对顺序.\n\n图 13: ML 成本模型的示例工作流程.\nXGBoost 根据循环程序特征预测成本.\nTreeRNN 直接总结 AST.\n我们在我们的机器学习优化器中实现了多种类型的模型.\n我们采用了一种梯度树提升模型 (基于 XGBoost), 该模型基于从循环程序中提取的特征进行预测; 这些特征包括每个循环级别中每个内存缓冲区的内存访问次数和重用率, 以及循环注解 (如&quot;向量化&quot;、“循环展开” 和&quot;并行&quot;) 的独热编码.\n我们还评估了一种使用 TreeRNN 对循环程序的抽象语法树 (AST) 进行总结的神经网络模型, 而无需进行特征工程.\n图 13 总结了成本模型的流程图.\n我们发现树提升和 TreeRNN 具有相似的预测质量.\n然而, 前者预测速度是后者的两倍, 且训练时间成本更低.\n因此, 在实验中我们选择了梯度树提升作为默认的成本模型.\n尽管如此, 我们相信这两种方法都具有价值, 并期待未来对此问题的更多研究.\n平均而言, 树提升模型进行预测需要 0.67 毫秒, 比运行真实测量快数千倍.\n图 12 比较了基于机器学习的优化器与黑盒自动调优方法; 前者比后者更快地找到更好的配置.\n#5.3 调度探索\n一旦我们选择了一个成本模型, 就可以使用它来选择有潜力的配置, 并在这些配置上迭代运行实际测量.\n在每次迭代中, 探索器使用机器学习模型的预测来选择一批候选者进行测量.\n收集到的数据随后被用作训练数据来更新模型.\n如果不存在初始训练数据, 探索器会随机选择候选者进行测量.\n最简单的探索算法通过成本模型枚举并运行每种配置, 选择预测表现最佳的配置.\n然而, 这种策略在大搜索空间中变得难以处理.\n相反, 我们运行一个并行模拟退火算法.\n探索器从随机配置开始, 在每一步随机走到附近的一个配置.\n如果成本按照成本模型的预测降低, 则该转换成功.\n如果目标配置的成本更高, 则很可能失败 (被拒绝).\n随机游走倾向于收敛到成本模型预测成本较低的配置.\n探索状态在成本模型更新时持续存在; 在这些更新后, 我们从最后一个配置继续.\n#5.4 分布式设备池和远程过程调用\n分布式设备池扩展了硬件上的试验运行, 并支持多个优化任务之间的细粒度资源共享.\nTVM 实现了一个基于 RPC 的定制化分布式设备池, 允许客户端在特定类型的设备上运行程序.\n我们可以使用此接口在主机编译器上编译程序、请求远程设备、远程运行函数, 并在主机上的同一脚本中访问结果.\nTVM 的 RPC 支持动态上传, 并能运行使用其运行时约定的交叉编译模块和函数.\n因此, 相同的底层架构可以执行单个工作负载优化和端到端图推理.\n我们的方法自动化了跨多个设备的编译、运行和分析步骤.\n该基础设施对于嵌入式设备尤为重要, 传统上嵌入式设备需要繁琐的手动交叉编译、代码部署和测量工作.\n#7. 相关工作\n深度学习框架为用户提供便捷的接口, 以表达深度学习工作负载, 并轻松地在不同的硬件后端上部署它们.\n尽管现有的框架目前依赖于特定供应商的张量运算库来执行其工作负载, 但它们可以利用 TVM 的栈为更多硬件设备生成优化代码.\n高级计算图 DSL 是表示和执行高级优化的典型方式.\nTensorflow 的 XLA 和最近引入的 DLVM 都属于这一类别.\n这些工作中的计算图表示是相似的, 本文也使用了高级计算图 DSL.\n虽然图级表示非常适合高级优化, 但它们对于在多样化的硬件后端下优化张量运算来说过于高级.\n先前的工作依赖于特定的降低规则来直接生成低级 LLVM, 或者采用供应商定制的库.\n这些方法需要对每个硬件后端和运算变体组合投入大量的工程工作.\n\n图 21: 我们将 ResNet 工作负载中的卷积卸载到基于 FPGA 的加速器上.\n灰色的条形对应那些无法被 FPGA 加速且因此必须在 CPU 上运行的层.\n与 Cortex A9 相比, FPGA 为卸载的卷积层提供了 40x 的加速.\nHalide 引入了计算与调度分离的思想.\n我们借鉴了 Halide 的见解, 并在我们的编译器中重用了其现有的有用调度原语.\n我们的张量算子调度也与 GPU 的 DSL 相关工作 以及基于多边形的循环变换 有关.\nTACO 引入了一种生成 CPU 上稀疏张量算子的通用方法.\nWeld 是一种用于数据处理任务的 DSL.\n我们特别关注解决 GPU 和专用加速器上 DL 工作负载的新调度挑战.\n我们的新原语有可能被这些工作中的优化管道所采用.\n高性能库如 ATLAS 和 FFTW 采用自动调优以获得最佳性能.\nTensor comprehension 结合了黑盒自动调优和多面体优化来优化 CUDA 内核.\nOpenTuner 和现有的超参数调优算法采用领域无关的搜索.\nHalide 使用预定义的成本模型自动调度图像处理管道.\nTVM 的机器学习模型采用有效的领域感知成本建模, 考虑程序结构.\n基于分布式调度的优化器可扩展到更大的搜索空间, 并能在支持的后端范围内找到最先进的内核.\n更重要的是, 我们提供了一个端到端的栈, 可以直接从深度学习框架获取描述, 并与图级栈联合优化.\n尽管深度学习加速器正逐渐流行, 但如何构建编译栈以有效针对这些设备仍不明确.\n我们在评估中使用的 VDLA 设计提供了一种通用的方法来总结类似 TPU 加速器的特性, 并支持对如何为加速器编译代码的具体案例研究.\n我们的方法也可能惠及现有的深度学习编译至 FPGA 的系统.\n本文通过张量化和编译器驱动的延迟隐藏, 提供了一种通用的解决方案, 以有效针对加速器.\n#8. 结论\n我们提出了一种端到端的编译栈, 以解决深度学习在不同硬件后端上的基本优化挑战.\n我们的系统包括自动端到端优化, 这在历史上是一项劳动密集且高度专业化的任务.\n我们希望这项工作能够鼓励对端到端编译方法的研究, 并为深度学习系统软硬件协同设计技术开辟新的机遇.\n","categories":["mlsys"],"tags":["MLSys","Distributed Machine Learning","System","Compiler"]},{"title":"RPC 框架对比","url":"/2025/12/28/rpc/","content":"#常见的 RPC 框架\n#Thrift\nThrift 是一个轻量级、跨语言的 RPC 框架，主要用于各个服务之间的 RPC 通信，最初由 Facebook 于 2007 年开发，2008 年进入 Apache 开源项目。它通过自身的 thrift (.thrift) 中间语言, 并借助代码生成引擎生成各种主流语言的 RPC 服务端/客户端模板代码。Thrift 支持多种不同的编程语言，包括 C++, Java, Python,PHP,Ruby, Erlang, Haskell, C#, Cocoa, Javascript, Node.js, Smalltalk, OCaml, Golang 等。\n自开源之后，Facebook 基于 Thrift 又进行了大量的性能优化 (主要是针对其中的 cpp 后端)，并在 2014 年将其再次开源，取名 FBThrift。FBThrift 在 Thrift 的基础上进行了多项优化，提升了性能和可扩展性，广泛应用于 Facebook 内部的分布式系统中。\nThrift IDL 提供了丰富的数据类型支持，包括基本数据类型（如整数、浮点数、布尔值等）、复合数据类型（如结构体、枚举、集合等）以及容器类型（如列表、集合、映射等）。此外，Thrift 还支持异常处理和服务继承等高级特性，使得开发者能够更灵活地定义和实现 RPC 服务。\nThrift 支持多种传输协议和数据序列化格式，常见的传输协议包括 TCP 套接字、HTTP 和内存传输等，而数据序列化格式则包括二进制格式、压缩格式和 JSON 格式等。开发者可以根据具体的应用需求选择合适的传输协议和序列化格式，以实现最佳的性能和兼容性。\n#gRPC\ngRPC 是由 Google 开发的一个高性能、开源和通用的 RPC 框架，基于 HTTP/2 协议和 Protocol Buffers（protobuf）序列化机制。gRPC 支持多种编程语言，包括 C++, Java, Python, Go, Ruby, C#, Node.js 等。gRPC 提供了强大的功能，如双向流式传输、负载均衡、认证和超时控制等，适用于构建高效的分布式系统和微服务架构。\ngRPC 使用 Protocol Buffers (.proto) 作为接口定义语言（IDL），允许开发者定义服务和消息结构。通过 gRPC 工具链，开发者可以自动生成客户端和服务器端的代码，从而简化了 RPC 服务的开发过程。gRPC 的设计目标是提供高性能和低延迟的通信，适用于需要高吞吐量和实时响应的应用场景。\ngRPC 使用 HTTP/2 作为底层传输协议，支持多路复用、流控和头压缩等特性，从而提高了网络通信的效率。此外，gRPC 支持多种认证机制，如 SSL/TLS 和基于令牌的认证，确保了通信的安全性。gRPC 还提供了丰富的工具和生态系统，包括负载均衡器、监控工具和服务发现机制，帮助开发者构建健壮的分布式应用。\n","categories":["programming"],"tags":["programming","rpc framework"]},{"title":"[ArXiv'25] RankMixer: Scaling Up Ranking Models in Industrial Recommenders","url":"/2025/12/16/rankmixer-scaling-up-ranking-models/","content":"\n工业界在推荐领域实现Scaling Law，目前出现了两条清晰的路径:\n\nGR\n判别式 + Transformer\n\n本文 RankMixer 是路线2的代表, 源自Google 2021年CV领域的MLP-Mixer，但做了关键适配\n\n#摘要\n近年来, 大型语言模型 (LLM) 的进展激发了人们对 Scaling Up 推荐系统的兴趣, 但仍存在两个实际障碍.\n\n首先, 工业级推荐系统的 training 和 serving 成本必须满足严格的延迟上限和高 QPS 要求.\n其次, 排序模型中大多数人为设计的特征交叉模块都沿用自 CPU 时代, 无法充分利用现代 GPU, 导致模型浮点运算利用率 (MFU) 低, 可扩展性差.\n\n我们提出了 RankMixer, 一种面向硬件的模型设计, 旨在构建统一且可扩展的特征交叉架构.\nRankMixer 保留了 Transformer 的高并行性, 同时用 Multi-Head Token Mixing 模块取代了二次 Self Attention 机制, 从而提高了效率.\n此外, RankMixer 利用 Per-Token Feed-Forward Networks (PFFN) 同时实现了对不同特征子空间的建模和跨特征空间交叉.\n我们进一步将其扩展到 1B 参数, 并提出了稀疏专家混合 (Sparse-MoE) 变体, 以获得更高的 ROI.\n我们还采用了一种动态路由策略来解决专家训练的不足和不平衡问题.\n实验表明, RankMixer 在万亿级生产数据集上展现出卓越的扩展能力.\n通过将之前分散的手工构建的低平均使用率 (MFU) 模块替换为 RankMixer, 我们将模型 MFU 从 4.5% 提升至 45%, 并在保持大致相同推理延迟的情况下, 将在线排序模型的参数规模扩大了两个数量级.\n我们通过在两个核心应用场景 (推荐和广告) 中进行在线 A/B 测试, 验证了 RankMixer 的通用性.\n最后, 我们推出了 1B 稠密参数 RankMixer, 可在不增加服务成本的情况下实现全流量服务.\n这使得用户活跃天数提高了 0.3%, 应用内总使用时长提高了 1.08%.\n#1. 引言\n推荐系统在信息分发过程中至关重要.\n作为一项重要的机器学习应用, 推荐系统基于大量的多字段特征数据预测用户对物品的行为, 这些特征包括:\n\n数值特征 (例如各种统计数据)\n类别特征 (例如用户和物品 ID)\n用户行为特征以及内容特征\n\n目前 SOTA 的推荐方法基于深度学习推荐模型 (Deep Learning Recommendation Models, DLRM), 该模型灵活地利用神经网络在输入特征 Embedding 之上构建稠密交叉层, 从而捕捉特征间的交叉.\nDLRM 中的稠密交叉层对推荐系统的性能至关重要, 并且已经提出了多种模型结构.\n随着大语言模型 (LLM) 的发展, 以及参数增加带来的性能提升, 扩展 DLRM 以充分利用海量数据已成为迫切需求.\n以往的研究已在 DLRM 扩展方面取得了诸多成果, 但早期研究仅仅扩展或堆叠特征交叉层, 而没有改变模型结构.\n这种方法带来的收益有限, 有时甚至会产生负面影响.\n随后, 诸如 DHEN 和 Wukong 等后续研究致力于设计创新的 DNN 结构以提升其扩展性能.\n然而, 利用模型规模来提升推荐系统的性能面临着独特的实际挑战.\n与 NLP 或 CV 任务不同, 工业级推荐系统必须严格遵守严格的延迟约束, 并支持极高的 QPS.\n因此, 核心挑战在于找到模型有效性和计算效率之间的最佳平衡点.\n从历史上看, 推荐系统中排序模型的架构深受 CPU 时代设计原则的影响.\n这些模型通常依赖于组合异构的、手工设计的跨特征模块来提取特征交叉作用, 但其许多核心算子在现代 GPU 上属于 Memory-bound 而非 Compute-bound, 导致 GPU 并行性差, 模型浮点运算利用率 (Model Flops Utilization, MFU) 极低, 通常只有个位数百分比.\n此外, 由于 CPU 时代模型的计算成本与参数数量大致成正比, 因此, 根据 Scaling Laws 进行激进扩展所带来的潜在 ROI 在实践中难以落地.\n总之, 对 DLRM Scaling Laws 的研究必须克服以下问题:\n\n模型架构应对齐硬件特点, 最大限度地提高现代 GPU 上的 MFU 和计算吞吐量.\n模型设计必须利用推荐系统数据的特点, 例如异构特征空间和数百个字段之间的个性化跨特征交叉.\n\n为了应对这些挑战, 我们提出了一种硬件感知的模型设计方法 - RankMixer.\nRankMixer 的核心设计基于两个可扩展的组件:\n\nMulti-head Token Mixing: 仅通过无参数算子即可实现跨 Token 特征交叉. 该策略在性能和计算效率方面均优于 Self Attention.\nPer-token Feed-Forward Networks (PFFN): 显著扩展了模型容量, 并通过为不同的特征子空间建模分配独立参数来解决特征空间支配 (inter-feature-space domination) 问题. 这些 FFN 也与推荐数据模式高度契合, 从而实现了更好的扩展性. 为了进一步提升大规模模型的 ROI, 我们将 Per-Token FFN 模块扩展为稀疏专家混合 (MoE) 结构. 通过针对不同数据动态激活每个 Token 的专家子集, 我们可以在计算成本增加极小的情况下显著提升模型容量.\n\nRankMixer 采用类似于 Transformer 的高度并行架构, 但克服了基于 Self Attention 的特征交叉的几个关键局限: 训练效率低、建模跨空间 ID 相似性时组合爆炸、以及注意力权重矩阵导致的严重 Memory-Bound. 同时, 与原版 Transformer 相比, RankMixer 在相同的 FLOPs 下提供了更大的模型容量和学习能力.\n在抖音推荐系统的生产部署中, 我们验证了在推理延迟比基线模型更短的情况下, 模型参数规模可以扩大 100x 以上.\n这得益于 RankMixer 架构能够将参数增长与浮点运算 (FLOPs) 解耦, 并通过高 MFU 和工程优化将 FLOPs 增长与实际成本解耦.\n主要贡献可概括如下:\n\n提出了一种名为 RankMixer 的新型架构, 它遵循模型设计对齐硬件的理念. 我们设计了 Multi-head Token Mixing 和 Per-Token FFN 策略, 以高效地捕获异构特征交叉, 并使用动态路由策略来提高 RankMixer 中 SparseMoE 的可扩展性.\n利用高 MFU 和性能优化的优势, 我们在不增加推理成本的情况下, 将模型参数放大 70x , 包括改进 MFU 和量化.\n我们进行了大量的线上和线下实验, 并在万亿级工业推荐数据集上研究了该模型的扩展规律. RankMixer 模型已成功部署在抖音信息流推荐排名中, 实现了全流量服务, 活跃天数和应用使用时长分别提升了 0.3% 和 1.08%.\n\n#2. 相关工作\n现代推荐系统基于深度学习推荐模型 (DLRM), 如何有效地对特征交叉进行建模是 DLRM 的关键因素.\n\nWide &amp; Deep 是最早的研究成果之一, 它结合了逻辑回归 (Wide 部分) 和 DNN (Deep 部分), 分别用于捕捉低阶和高阶特征交叉.\nDeepFM 是另一项成果, 它集成了因子分解机 (FM) 和 DNN.\n此外, DeepCross 是残差网络的扩展, 旨在隐式地学习自动特征交叉. 然而, 仅仅依靠 DNN 来学习高阶特征交叉已被证明极具挑战性.\n\n显式交叉方法设计了不同的算子来显式地捕获高阶特征交叉, 例如:\n\nPNN、DCN 及其后续版本 DCNv2 、xDeepFM 、FGCNN 和 FiGNN.\nAutoInt 和 Hiformer 采用带有残差连接的注意力机制来学习复杂的交叉.\nDHEN 提出将多个交叉算子组合在一起.\n\n尽管提高了准确率, 这些新架构增加了延迟和内存消耗, 而且模型规模也相对较小.\nScaling Law 已成为深度学习的一个基本主题, 也是过去十年中众多突破的关键催化剂, 尤其是在 NLP、CV 和多模态建模等领域.\n它描述了模型性能与缩放因子 (例如模型大小、数据量和计算能力) 之间的幂律相关性.\n近年来, 推荐系统中的 Scaling Law 问题引起了研究人员的广泛关注.\n相关研究探索了 用户活动序列预训练、通用用户表示 以及 在线检索 的 Scaling Law 策略.\nWukong 通过堆叠 FM 和 LCB 来学习特征交叉.\n与之正交的方面, Zhang (2024) 将序列推荐模型扩展到了 0.8B 的参数.\nHSTU 增强了生成式推荐器 (GR) 的扩展效果, 而 GR 更关注序列部分.\n#3. 方法\n\n图 1. RankMixer 模块的架构. 一个 RankMixer 块由两个模块组成:\n\nMulti-head Token Mixing 模块: Token 混合模块将每个 Token 的 Embedding 分割成 HHH 个更小的部分 (head), 然后将这些部分在不同 Token 之间重新组合, 从而创建新的混合 Token. 这使得来自不同特征的信息能够相互交叉.\n基于 SMoE 的 Per-Token FFN 模块\n\n#3.1 总体架构\nRankMixer 的整体架构由 TTT 个输入 Token 组成, 这些 Token 经由 LLL 个连续的 RankMixer 模块处理, 最后由一个输出池化算子进行处理.\n每个 RankMixer 模块包含两个主要组件:\n\nMulti-Head Token Mixing 层\nPer-Token Feed-Forward Network (PFFN) 层\n\n首先, 输入向量 einput\\mathbf{e}_{\\text{input}}einput​ 被分词为 TTT 个特征 Token x1,x2,…,xT\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_Tx1​,x2​,…,xT​, 每个特征 Token 代表一个连贯的特征向量. RankMixer 模块通过以下方式迭代地细化第 LLL 层的 Token 表示:\nSn−1=LN ( TokenMixing ( Xn−1 )+Xn−1)\\mathbf{S}_{n-1} = \\text{LN}~(~\\text{TokenMixing}~(~\\mathbf{X}_{n-1}~) + \\mathbf{X}_{n-1})\nSn−1​=LN ( TokenMixing ( Xn−1​ )+Xn−1​)\nXn=LN ( PFFN ( Sn−1 )+Sn−1)\\mathbf{X}_n = \\text{LN}~(~\\text{PFFN}~(~\\mathbf{S}_{n-1}~) + \\mathbf{S}_{n-1})\nXn​=LN ( PFFN ( Sn−1​ )+Sn−1​)\n其中:\n\nLN(⋅)\\text{LN}(\\cdot)LN(⋅) 为层归一化函数.\nTokenMixing(⋅)\\text{TokenMixing}(\\cdot)TokenMixing(⋅) 和 PFFN(⋅)\\text{PFFN}(\\cdot)PFFN(⋅) 分别为 Multi-Head Token Mixing 模块和 PFFN 模块.\nXn∈RT×D\\mathbf{X}_n \\in \\mathbb{R}^{T \\times D}Xn​∈RT×D 为第 nnn 个 RankMixer 模块的输出, X0∈RT×D\\mathbf{X}_0 \\in \\mathbb{R}^{T \\times D}X0​∈RT×D 由 x1,x2,…,xT\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_Tx1​,x2​,…,xT​ 堆叠而成\nDDD 为模型的隐藏维度.\n输出表示 ooutput\\mathbf{o}_{\\text{output}}ooutput​ 由最终层表示 XL\\mathbf{X}_LXL​ 的均值池化得到, 用于计算不同任务的预测结果.\n\n#3.2 输入层和特征分词\n构建大规模推荐模型的第一步是准备包含丰富信息的输入数据, 例如\n\n用户特征 (包括用户 ID 和其他的用户信息)\n候选特征 (包括视频 ID, 作者 ID 等)\n序列特征 (通过序列模块处理以捕捉时间兴趣, 产出 es\\mathbf{e}_ses​)\n交叉特征 (用户与候选对象之间的交叉特征)\n\n所有特征最终将被转换为不同维度的 Embedding.\n为了在后续阶段实现高效的并行计算, 不同维度的 Embedding 必须转换为维度对齐的向量, 称为特征 Token (feature-token).\n我们将这种 Embedding 对齐过程称为分词.\n最简单的策略是为每个特征分配一个 Embedding, 但考虑到通常有数百个特征, 这会带来诸多挑战.\n数百个 Token 不可避免地会将每个 Token 的参数和计算量减少到很小的片段, 导致对重要特征的建模不足, 而且 GPU 利用率低.\n相反, Token 数量过少(例如只有一个 Token)会将模型结构简化为简单的 DNN, 无法清晰地表示不同的特征空间, 从而导致主导特征掩盖其他特征.\n\n将入图 Embeddings 转换成类似 LLM 场景下的 Token.\n\n为了克服这些问题, 我们提出了一种基于语义的分词方法, 该方法利用领域知识将特征分组为若干语义连贯的簇.\n这些簇依次拼接成一个大的 Embedding 向量 einput=[e1;e2;…;eN]\\mathbf{e}_{\\text{input}} = [\\mathbf{e}_1; \\mathbf{e}_2; \\ldots; \\mathbf{e}_N]einput​=[e1​;e2​;…;eN​],\n随后再被分割成 T 个定长的特征 Token.\n每个特征 Token xi∈RD\\mathbf{x}_i \\in \\mathbb{R}^Dxi​∈RD 捕获一组 Embedding, 表示相似的语义方面.\n\n特征本来就有语义, 所以这里是把 N 组输入特征拼接到一起\n\nxi=Proj(einput[d⋅(i−1):d⋅i]),i=1,…,T.\\mathbf{x}_i = \\text{Proj}\\big( \\mathbf{e}_{\\text{input}}[ d \\cdot (i - 1) : d \\cdot i ] \\big), \\quad i = 1, \\ldots, T.\nxi​=Proj(einput​[d⋅(i−1):d⋅i]),i=1,…,T.\n其中 einput\\mathbf{e}_{\\text{input}}einput​ 是拼接后的 Embedding, ddd 是每个 Token 的固定维度, NNN 是特征组的数量, TTT 是结果 Token 计数, Proj\\text{Proj}Proj 函数将分割的嵌入映射到 DDD 维度.\n#3.3 RankMixer 模块\n#3.3.1 Multi-head Token Mixing\n为了促进跨 Token 的有效信息交换 (这对特征交叉和全局信息建模非常重要), 我们引入了 Multi-head Token Mixing 模块.\n每个 Token 均分为 HHH 个头, Token xt\\mathbf{x}_txt​ 的第 hhh 个头表示为 xth\\mathbf{x}_t^hxth​:\n[xt(1) ∥ xt(2) ∥ … ∥ xt(H)]=SplitHead(xt)\\big[ \\mathbf{x}_t^{(1)} \\,\\|\\, \\mathbf{x}_t^{(2)} \\,\\|\\, \\ldots \\,\\|\\, \\mathbf{x}_t^{(H)} \\big]\n=\n\\text{SplitHead}\\big( \\mathbf{x}_t \\big)\n[xt(1)​∥xt(2)​∥…∥xt(H)​]=SplitHead(xt​)\n这些头可以看作是 Token xt\\mathbf{x}_txt​ 在低维特征子空间中的投影, 因为推荐任务需要考虑不同的视角.\nToken Mixing 用于聚合这些子空间向量, 以实现全局特征交叉.\n形式上, Multi-head Token Mixing 混合后, 对应于第 hhh 个头部的第 hhh 个 Token sh\\mathbf{s}_hsh​ 如下:\nsh=Concat(x1h,x2h,…,xTh)\\mathbf{s}_h\n=\n\\text{Concat}\\big(\n\\mathbf{x}_1^{h},\n\\mathbf{x}_2^{h},\n\\ldots,\n\\mathbf{x}_T^{h}\n\\big)\nsh​=Concat(x1h​,x2h​,…,xTh​)\nMulti-head Token Mixing 模块的输出为 S∈RH×T×DH\\mathbf{S} \\in \\mathbb{R}^{H \\times T \\times \\frac{D}{H}}S∈RH×T×HD​, 它由所有打乱顺序的 Token s1,s2,…,sH\\mathbf{s}_1, \\mathbf{s}_2, \\ldots, \\mathbf{s}_Hs1​,s2​,…,sH​ 堆叠而成.\n本文中, 我们设置 H=TH = TH=T, 以在 Multi-head Token Mixing 后保持剩余连接的 Token 数量不变.\n经过残余连接和归一化模块后, 我们可以生成:\ns1,s2,…,sT=LN(TokenMixing(x1,x2,…,xT)+(x1,x2,…,xT)).\\mathbf{s}_1, \\mathbf{s}_2, \\ldots, \\mathbf{s}_T\n=\n\\text{LN}\\big(\n\\text{TokenMixing}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T)\n+\n(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T)\n\\big).\ns1​,s2​,…,sT​=LN(TokenMixing(x1​,x2​,…,xT​)+(x1​,x2​,…,xT​)).\n尽管 Self Attention 机制在大规模语言模型中已被证明非常有效, 但我们发现它对于推荐系统而言并非最优.\n在 Self Attention 机制中, 注意力权重是通过 Token 的内积来计算的.\n这种方法在 NLP 中效果良好, 因为所有 Token 共享一个统一的嵌入空间.\n然而, 在推荐任务中, 特征空间本质上是异构的.\n计算两个异构语义空间之间的内积相似度是出了名的困难, 尤其是在推荐系统中, 用户和物品端的特征 ID 空间可能包含数亿个元素.\n因此, 将 Self Attention 机制应用于如此多样化的输入, 其性能并不优于无参数的 Multi-head Token Mixing 方法, 并且会消耗更多的计算资源, 内存 I/O 操作和 GPU 内存.\n#3.3.2 Per-token FFN\n以往的 DLRM 和 DHEN 模型倾向于将来自许多不同语义空间的特征混合在一个交叉模块中, 这可能导致高频字段占主导地位, 淹没低频或长尾信号, 最终损害整体推荐质量.\n我们提出了一种参数隔离 (parameter-isolated) 的前馈网络架构, 称为 Per-token FFN.\n在传统设计中, FFN 的参数在所有 Token 之间共享, 但我们的方法使用专用的转换来处理每个 Token, 从而隔离每个 Token 的参数.\n对于第 ttt 个 Token st\\mathbf{s}_tst​, PFFN 可以表示为:\nvt=fpffnt,2(Gelu(fpffnt,1(st)))\\mathbf{v}_t = f_{\\text{pffn}}^{t,2}(\n  \\text{Gelu}(\n    f_{\\text{pffn}}^{t,1}(\\mathbf{s}_t)\n  )\n)\nvt​=fpffnt,2​(Gelu(fpffnt,1​(st​)))\n其中,\nfpffnt,i(x)=xWpffnt,i+bpffnt,if_{\\text{pffn}}^{t,i}(\\mathbf{x}) = \\mathbf{x} \\mathbf{W}_{\\text{pffn}}^{t,i} + \\mathbf{b}_{\\text{pffn}}^{t,i}\nfpffnt,i​(x)=xWpffnt,i​+bpffnt,i​\n是 PFFN 的第 iii 层 MLP.\nWpffnt,1∈RD×kD\\mathbf{W}_{\\text{pffn}}^{t,1} \\in \\mathbb{R}^{D \\times kD}Wpffnt,1​∈RD×kD, bpffnt,1∈RkD\\mathbf{b}_{\\text{pffn}}^{t,1} \\in \\mathbb{R}^{kD}bpffnt,1​∈RkD, Wpffnt,2∈RkD×D\\mathbf{W}_{\\text{pffn}}^{t,2} \\in \\mathbb{R}^{kD \\times D}Wpffnt,2​∈RkD×D, bpffnt,2∈RD\\mathbf{b}_{\\text{pffn}}^{t,2} \\in \\mathbb{R}^{D}bpffnt,2​∈RD.\nkkk 是一个超参数, 用于调整每个 Token 的 FFN 的隐藏维度.\nGelu(⋅)\\text{Gelu}(\\cdot)Gelu(⋅) 是 Gelu 激活函数.\nst∈RD\\mathbf{s}_t \\in \\mathbb{R}^Dst​∈RD 是第 ttt 个 Token.\n我们将 PFFN 模块总结如下:\nv1,v2,…,vT=PFFN(s1,s2,…,sT).=fpffnt,2 ⁣(Gelu ⁣(fpffnt,1(s1,s2,…,sT))).\\begin{align}\n\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_T\n&amp;=\n\\text{PFFN}\\big( \\mathbf{s}_1, \\mathbf{s}_2, \\ldots, \\mathbf{s}_T \\big). \\\\\n&amp;=\nf_{\\text{pffn}}^{t,2}\\!\\left(\n\\text{Gelu}\\!\\left(\nf_{\\text{pffn}}^{t,1}\\big( \\mathbf{s}_1, \\mathbf{s}_2, \\ldots, \\mathbf{s}_T \\big)\n\\right)\n\\right).\n\\end{align}\nv1​,v2​,…,vT​​=PFFN(s1​,s2​,…,sT​).=fpffnt,2​(Gelu(fpffnt,1​(s1​,s2​,…,sT​))).​​\n与参数全部共享的 FFN 相比, PFFN 通过引入更多参数来增强建模能力, 同时保持计算复杂度不变.\n值得强调的是, PFFN 与 MMoE 专家的区别在于, 每个 PFFN 处理的是不同的 Token 输入, 而 MMoE 中的所有专家组共享相同的输入.\nMMoE 模型中是多个专家处理相同的输入,\nTransformer 模型中不同输入共享一个 FFN,\nRankMixer 同时分割输入和参数, 这有利于学习不同特征子空间的多样性.\n#3.4 RankMixer 中的稀疏 MoE\n为了进一步提高 ROI, 我们可以把每个 Token 的稠密 FFN 结构替换为 Sparse-MoE (Sparse Mixture of Experts) 块, 从而在计算成本基本不变的情况下提升模型容量.\n然而, 在 RankMixer 中使用原始的 Sparse-MoE 会导致性能下降, 原因如下:\n\n均匀的 kkk-expert 路由. Top-k 选择会平等对待所有特征 Token, 将预算浪费在低信息 Token 上, 而高信息 Token 则被忽略, 这阻碍了模型捕捉 Token 之间的差异.\n专家训练不足. PFFN 已经将参数乘以 Token 数; 添加非共享专家会导致专家数量大大增加, 结果会导致路由严重不平衡和专家训练不足.\n\n我们结合两种互补的训练策略来解决上述问题.\nReLU 路由:\n为了赋予 Token 灵活的专家数量并保持可区分性, 我们替换了常见的 Top k + softmax 为一个 ReLU 门以及自适应 ℓ1 惩罚 (Wang, 2024a).\n给定令牌 si∈Rdh\\mathbf{s}_i \\in \\mathbb{R}^{d_h}si​∈Rdh​ 和路由器 h(⋅)h(\\cdot)h(⋅) 的第 jjj 个专家 ei,j(⋅)e_{i,j}(\\cdot)ei,j​(⋅).\nGi,j=ReLU(h(si)),vi=∑j=1NeGi,j ei,j(si).G_{i,j} = \\text{ReLU}\\big( h(\\mathbf{s}_i) \\big), \\quad\n\\mathbf{v}_i = \\sum_{j=1}^{N_e} G_{i,j} \\, e_{i,j}(\\mathbf{s}_i).\nGi,j​=ReLU(h(si​)),vi​=j=1∑Ne​​Gi,j​ei,j​(si​).\n其中 NeN_eNe​ 表示每个令牌的专家数量, NtN_tNt​ 表示令牌的数量.\nReLU 路由将激活更多专家来处理高信息量的令牌, 并提高参数效率.\n稀疏性受以下因素控制 Lreg\\mathcal{L}_{reg}Lreg​, 系数为 λ\\lambdaλ, 使平均活跃专家比率接近预算.\nL=Ltask+λ Lreg,Lreg=∑i=1Nt∑j=1NeGi,j.\\mathcal{L} = \\mathcal{L}_{\\text{task}} + \\lambda \\, \\mathcal{L}_{\\text{reg}}, \\quad\n\\mathcal{L}_{\\text{reg}} = \\sum_{i=1}^{N_t} \\sum_{j=1}^{N_e} G_{i,j}.\nL=Ltask​+λLreg​,Lreg​=i=1∑Nt​​j=1∑Ne​​Gi,j​.\n**稠密训练 + 稀疏推理 (DTSI-MoE) **:\n受 Pan(2024)的启发, 本文采用了两个路由器 htrainh_{\\text{train}}htrain​ 和 hinferh_{\\text{infer}}hinfer​, 而 Lreg\\mathcal{L}_{\\text{reg}}Lreg​ 仅应用于 hinferh_{\\text{infer}}hinfer​.\n训练过程中会更新 htrainh_{\\text{train}}htrain​ 和 hinferh_{\\text{infer}}hinfer​, 而推理过程中仅使用 hinferh_{\\text{infer}}hinfer​.\n结果表明, DS-MoE 能够避免专家模型训练不足的问题, 同时降低推理成本.\n#3.5 Scaling Up 的方向\nRankMixer 本质上是一个高度并行且可扩展的架构.\n其参数数量和计算成本可以沿四个正交轴展开: Token 计数 TTT, 模型宽度 DDD, 层数 LLL 和专家数量 EEE.\n对于全稠密激活版本, 单个样本的参数规模和前向浮点运算次数可以计算如下.\n#Param≈2kLTD2,FLOPs≈4kLTD2.\\#\\text{Param} \\approx 2 k L T D^2, \\quad\n\\text{FLOPs} \\approx 4 k L T D^2.\n#Param≈2kLTD2,FLOPs≈4kLTD2.\nkkk 是用于调整 FFN 隐藏层维度的缩放比例.\n在 Sparse-MoE 版本中, 有效参数量和每个 token 的计算量会进一步按照稀疏度比例进行缩放, 其中\ns=#Activated_Param#Total_Params = \\frac{\\#\\text{Activated\\_Param}}{\\#\\text{Total\\_Param}}s=#Total_Param#Activated_Param​.\n#4. 实验\n#4.1 实验设置\n#4.1.1 数据集和环境\n离线实验使用了抖音推荐系统的训练数据.\n这些数据来源于抖音的在线日志和用户反馈标签.\n训练数据集包含 300 多个特征, 例如数值特征, ID 特征, 交叉特征和序列特征, 涉及数十亿个用户 ID 和数亿个视频 ID, 所有这些特征都被转换为 Embedding.\n数据每天包含数万亿条记录, 实验所用数据收集周期为两周.\n#4.1.2 评估指标\n为了评估模型性能, 我们使用 AUC(曲线下面积) 和 UAUC(用户级 AUC) 作为主要性能指标, 并使用参数数量, FLOPs 和 MFU 作为效率指标, 具体如下.\n完成/跳过 AUC/UAUC:\nfinish=1/0 或 skip=1/0 标签表示用户是否在短时间内看完视频或切换到下一个视频.\n我们评估此完成标签的 AUC 和 UAUC.\nAUC 增加 0.0001 (万1) 可视为显著改进.\n稠密参数:\nDense 部分中的参数数量, 不包括 Sparse 嵌入参数.\n训练 FLOPs/Batch:\n运行一个包含 512 个数据的 Batch 所需的浮点运算 (FLOPs) 次数, 代表训练的计算成本.\nMFU:\nMFU(模型 FLOPs 利用率)是一个衡量模型如何有效地利用硬件提供的浮点运算的指标, 计算方法是将模型的实际 FLOPs 消耗量除以硬件的理论 FLOPs 容量.\n#4.1.3 基线\n我们与以下公认的 SOTA 基线模型进行比较:\nDLRM-MLP:\n原版 MLP.\nDCNv2, RDCN:\n特征交叉模型的 SOTA 模型.\nMoE:\n通过使用多个并行专家来扩展规模.\nAutoInt, Hiformer:\n结合了异构 Self-Attention 层和 LoRA 计算.\nDHEN:\n结合了不同类型的特征交叉块 (DCN/Self-Attention/FM/LR), 并堆叠了多层.\nWukong:\n基于 DHEN 的架构研究了特征交叉的缩放规律, 并结合了因子分解机块 (Factorization Machine Block, FMB) 和线性压缩块 (Linear Compress Block, LCB).\n所有实验均在数百个 GPU 上进行, 采用混合分布式训练框架, 其中 Sparse 部分异步更新, 而 Dense 部分同步更新.\n所有模型的优化器超参数保持一致.\n对于 Dense 部分, 我们使用学习率为 0.01 的 RMSProp 优化器; 对于 Sparse 部分, 我们使用 Adagrad 优化器.\n#4.2 与 SOTA 方法的比较\n\n为了探索如何 Scaling Up 模型, 我们比较了参数规模在 100M 左右的相似模型, 以确定在相同的计算成本下哪种模型结构性能最佳.\n表 1 总结了我们方法和基线方法的主要性能结果.\n我们可以看到, RankMixer 在多个目标和指标上都显著优于其他 SOTA 的模型.\n我们仔细分析了每个模型.\n\n简单地将 DLRM 模型扩展到 100M 参数只能带来有限的性能提升, 这凸显了设计针对推荐数据特征量身定制的模型以获得更佳扩展性能的必要性.\n所有模型都存在模型参数和计算成本不平衡的问题.\n\n即使参数规模相对较小, 这些模型的 FLOPs 也已经很高, 表明其设计存在缺陷, 限制了表格中的结果.\n而 RankMixer 模型虽然性能最佳, 但在扩展到 100M 参数时, 其 FLOPs 在所有模型中仍然保持相对适中, 这体现了其在模型容量和计算负载之间取得平衡的方法.\n我们还将 RankMixer 与几个常用的 SOTA 的 Scaling Up 模型(如 Hiformer 和 Wukong)进行了比较, 发现, 在类似的参数设置下, RankMixer 不仅性能更好, 而且计算需求也更低.\n#4.3 不同模型的 Scaling Laws\n\n图 2. 不同模型的最终 AUC-gain 与 Params/FLOPs 之间的 Scaling Law. x 轴采用对数刻度.\n图 2 展示了从参数大小和浮点运算次数(FLOPs)两个方面观察到的 Scaling Law 曲线.\nRankMixer 模型在参数大小和浮点运算次数方面都展现出最陡峭的 Scaling Law.\nRankMixer 模型始终优于其他模型.\n尽管 Wukong 模型的参数曲线相对陡峭, 但其计算成本增长速度更快; 因此, 在 AUC 与 FLOPs 曲线上, 它与 RankMixer 和 Hiformer 之间的差距更大.\n此外, Hiformer 的性能略逊于 RankMixer, 这反映出其对特征级 Token 分割和注意力机制的依赖影响了其效率.\nDHEN 的扩展性并不理想, 反映了其交叉结构的扩展性有限.\n此外, MoE 通过增加专家来 Scaling Up 模型的策略在维持专家平衡方面带来了挑战, 导致其扩展性能欠佳.\n具体来说, 我们可以通过增加宽度 (DDD)、特征 Token 数 (TTT) 和层数 (LLL) 来扩展 RankMixer 模型.\n在我们的实验中, 我们观察到与 LLM Scaling Law 一致的结论: 模型质量主要与参数总数相关, 不同的扩展方向 (深度 LLL, 宽度 DDD, Token 数 TTT) 几乎产生相同的性能.\n从计算效率的角度来看, 更大的隐藏层维度会产生更大的矩阵乘法形状, 因此比堆叠更多层能获得更高的 MFU.\n因此, 100M 和 1B 的最终配置分别设置为 (D=768D = 768D=768, T=16T = 16T=16, L=2L = 2L=2) 和 (D=1536D = 1536D=1536, T=32T = 32T=32, L=2L = 2L=2).\n#4.4 消融实验\n\n在 RankMixer-100M 模型中, 我们对残差连接和 Multi-Head Token-Mixing 进行了消融实验.\n从表 2 可以看出, 移除这些组件会显著降低模型的性能.\n移除多头标记混合会丢失全局信息, 因为每个 FFN 仅对部分特征进行建模, 彼此之间没有交互.\n移除残差连接和层归一化(LayerNorm)也会降低性能, 减少训练稳定性, 并增加梯度爆炸或梯度消失问题的可能性.\n我们进一步分析了 Token 混合策略, 即表 3 中从特征 Token 到 FFN 的路由策略.\n与 Multi-Head Token Mixing 相比, 这些路由策略包括以下几种.\n\nAll-Concat-MLP: 将所有 Token 连接起来, 并通过一个大型多层感知器(MLP)进行处理, 然后再将其分割成相同数量的 Token .\n性能下降表明学习大型矩阵和削弱局部信息学习的挑战.\nAll-Share: 不进行分割, 整个输入向量被共享并输入到每个 Token 对应的 FFN 中, 类似于 MoE.\n性能显著下降, 这表明特征子空间分割和独立建模相对于全共享输入的重要性.\nSelf-Attention: 在 Token 之间应用 Self-Attention 进行路由.\n其性能略逊于 Multi-Head Token Mixing, 且计算成本较高, 这表明在数百个不同的特征子空间中学习相似性的难度很大.\n\n#4.5 Sparse-MoE 的可扩展性和专家平衡\n\n图 3. RankMixer 变体在激活率逐渐降低(1, 1/2, 1/4, 1/8 位专家)下的 AUC 性能.\n密集训练 + ReLU 路由的 SMoE 几乎保留了 1B 密集模型的所有准确率.\n可扩展性:\n图 3 绘制了 SMoE 的离线 AUC 增益与稀疏度的关系.\n将 Dense-Training-Sparse-Inference (DTSI) 与 ReLU 路由相结合, 对于在高稀疏度下保持准确率至关重要.\n这使得 RankMixer 能够在参数容量(和内存占用)扩展 8x 以上的情况下, AUC 几乎没有损失, 并且推理时间显著缩短 (吞吐量提升 50%).\n原始 SMoE 的性能随着激活的专家数量减少而单调下降, 这说明了我们发现的专家不平衡和欠训练问题.\n添加负载均衡损失可以减少相对于原始 SMoE 的性能下降, 但仍然不如 DTSI + ReLU 版本, 因为问题主要出在专家训练上, 而不是路由上.\n这验证了 Sparse-MoE 是将 RankMixer 从当前的 10 亿参数规模扩展到未来 100 亿规模部署且不超出成本预算的有效途径.\n\n图 4. RankMixer 中不同标记的激活专家比例.\n表 4. 抖音和抖音 Lite app 中信息流推荐场景的在线 A/B 测试结果显示, 所有改进均具有统计学意义. 根据长期反向 A/B 测试和长期反向实验的持续观察, RankMixer-1B 带来的提升尚未收敛, 此处展示的结果仍在不断改进中.\n\n\n专家团队的平衡性和多样性:\n传统的稀疏学习模型 (MoE) 经常面临专家不平衡的问题, 这会导致部分专家训练不足, 最终导致“死亡专家” (几乎从未被激活的专家) , 只有少数固定专家始终处于激活状态.\n图 4 显示, 将 DTSI (密集训练-稀疏推理) 与 ReLU 路由相结合可以有效解决这个问题: 密集训练保证了大多数专家都能获得足够的梯度更新, 从而避免专家饥饿; ReLU 路由使得激活率在不同 Token 之间动态变化 – 图中所示的激活比例会根据 Token 的信息量自适应地变化, 这与推荐数据的多样性和高度动态分布非常吻合.\n#4.6 在线服务成本\n如何防止推理延迟随着参数数量增加两个数量级而急剧上升?\n在实际系统中, 延迟与吞吐量成反比, 与服务机器资源的成本成正比.\n与我们之前完全部署的 16M 参数模型 (结构集成了 DLRM 和 DCN) 相比, 我们的 RankMixer 模型将参数规模扩大了约 70x , 达到 10 亿倍.\n尽管参数数量显著增加, 但由于我们采用了与硬件相匹配的模型设计和优化策略, 最终的推理延迟仍然保持稳定.\n当大幅增加模型参数时, 延迟可以分解为以下公式:\nLatency=#Param×(FLOPs/Param ratio)MFU×(Theoretical Hardware FLOPs)\\text{Latency}\n=\n\\frac{\\#\\text{Param} \\times (\\text{FLOPs/Param ratio})}\n{\\text{MFU} \\times (\\text{Theoretical Hardware FLOPs})}\nLatency=MFU×(Theoretical Hardware FLOPs)#Param×(FLOPs/Param ratio)​\n如表 6 所示, 参数增加两个数量级逐渐被 FLOPs/Param 比率降低 3.6x 、MFU 增加 10x 以及基于量化的硬件 FLOPs 改进 2x 所抵消.\n\n\nFLOPs/Param (G/M):\n表 6 的第三行报告了每个参数所需的浮点运算次数(FLOPs).\n得益于模型设计, RankMixer 在 FLOPs 仅增加约 20x 的情况下, 参数数量增加了 70x , 其 FLOPs/参数比率仅为基线的三分之一, 效率提升了 3.6x.\n换句话说, 在相同的 FLOPs 预算下, RankMixer 可以容纳比基线多三倍的参数.\n\n\n模型浮点运算利用率 (MFU):\n如表 6 所示, MFU 表示机器计算的利用率.\n通过使用大矩阵 GEMM, 良好的并行拓扑结构 (将并行的 PFFN 算子融合到一个 Kernel 中), 降低内存带宽成本和开销, RankMixer 将 MFU 提高了近 10x, 使模型从内存密集型转变为计算密集型.\n\n\n量化:\nfp16 推理将使 GPU 的理论峰值硬件浮点运算能力提高 2x: RankMixer 中的主要计算包含多个大型矩阵乘法, 如前所述, 这些乘法非常适合半精度(fp16)推理.\n\n\n#4.7 在线表现\n为了验证 RankMixer 作为可扩展推荐模型框架的普适性, 我们在个性化排名的两大核心应用场景 – 信息流推荐和广告 – 中进行了在线实验, 涵盖了个性化排名的主要用例.\n针对每个场景, 我们监测了以下关键性能指标:\n\n信息流推荐. 活跃天数是指实验期间每位用户的平均活跃天数, 可替代 DAU 增长率; 时长是指用户在应用上的累计停留时间; 完成/点赞/评论次数: 用户完成的播放次数、点赞次数和评论次数.\n广告. 我们监测模型质量指标 (ΔAUC) 和收入指标 ADVV (广告商价值).\n\n在这些场景中, 之前的基线模型是结合 DLRM 和 DCN 的 16M 参数的模型.\n我们将 Dense 部分替换为 RankMixer-1B 模型, AUC 提高了 0.7%.\n我们对信息流推荐和广告进行了在线 A/B 测试实验.\n表 4 展示了信息流推荐 A/B 测试 8 个月的长期结果.\n广告效果如表 5 所示.\nRankMixer 已部署并评估于三个个性化排名应用中, 包括信息流推荐 (RankMixer-1B) 和广告 (RankMixer-1B).\nRankMixer 在所有关键业务指标上均实现了显著提升.\n我们还可以从表 4 中观察到, 与其他活跃度用户群体相比, 低活跃用户的提升最为显著, 活跃天数提升超过 1.7412%, 这表明该模型具有强大的泛化能力.\n这些结果表明, RankMixer 作为统一的骨干网络, 能够可靠地泛化到不同的应用场景.\n#5 结论\n本文介绍了我们最新开发的 RankMixer 模型, 该模型已全面部署于抖音 Feed 排名.\n它融合了异构特征交叉的模型设计和高度并行化的架构, 从而显著提升了服务效率.\n实验结果表明, 该模型性能优异, 且具有极强的扩展性.\n全面部署于抖音 App 后, 其活跃天数和 App 使用时长分别提升了 0.3% 和 1%.\n#参考资料\n\n「阿里/美团/字节」 轻量级实现Scaling Law|从RankMixer-MTGR到OneTrans-HHFT\n特征交叉-RankerMixing(大模型时代的特征交叉)\nRankMixer：推荐系统的“算力与精度“双突破之道\n\n","categories":["recommender-system"],"tags":["机器学习","推荐系统","深度学习"]},{"title":"转发：推荐系统再思考：从机器学习到深度学习","url":"/2025/08/17/thinking-on-rec-sys/","content":"\n本文转发自 推荐系统再思考：从机器学习到深度学习，作者 lessmore，时间：2023-03-12\n\n再次回到推荐系统方向的工作， 相比几年前 2018/19 很多认知出现了变化，对我来说很有意义，写出来作为记录也分享给大家。\n曾经的主流 ctr 模型是 deep+wide model，出自 google 的经典论文。 从业界的演进路线来说， 最开始是大规模 LR，模型很简单，f(x) = sigmoid(w*x+b), 主要的提升性能的方式就是特征工程和工程上做到极致，前者因为模型能力有限，手动提特征，越多越好，尤其是交叉类特征，因为 LR 不具备特征交叉/组合的能力，表达能力极为有限，工程上的问题是大规模稀疏 id 类特征的训练/预估，在线学习的系统架构，包括实时样本拼接，实时训练和实时取最新权重做预估。接着是 FM/FFM 模型大放异彩，解决了交叉特征过于稀疏的问题， 算是 LR 的改进版。之后类似 deep+wide，concatenate(FFM, LR) 接几层全连接层，通常 2/3 层，因为更深没有测出效果， 而且预估的机器成本增加很多。\n我个人背景是再深度学习大火之后自学的相关知识，属于野路子，看《统计学习方法》和使用 scikit-learn 入门，缺乏对机器学习/深度学习整体脉络的理解，认知比较肤浅，完全没有意识到深度学习这时再 CV/NLP 带来的巨大进步，end2end 无需特征工程，更深的网络更强的建模能力，比如 residual network 有一百多层，更没有看到 pre train + fine tune，unsupervised learning 这些新范式的强大能力，最近几个月 chatgpt 大火，看李沐大神的视频才体会到这一切。\n再回到推荐系统业界的做法，其实 19 年阿里的文章 DIN 之类的深度学习网络已经出现，在我的视角还没有理解，从组内 senior 同事得到的经验是更深的全连接层试过没什么用，这个结论应该有非常强的局限性，教训多思考为什么。现在想来有一些原因，公司使用的训练框架是自研的，对 LR/FFM 有非常多的调优，比如 ftrl 算法，bias/vec 的初始化/优化器调优，对于神经网络却非常少投入，简单尝试多层 MLP 很有可能比不过充分调优的 LR/FFM，后者做了充分的特征工程，调参，从现在的视角来看，如果没有合适的优化器，一些 tricky，比如 warnup， decay，batch size/learning rate 调参，模型结构多做调整，没有说随便做做神经网络就比 LR/FFM 强。这时公司推动的全面转向 tensorflow 现在后知后觉才意识到是大佬们已经发现了问题所在，深度学习才是未来。\n回到认知的变化，\n\nid 类 embedding 特征不用太长，FFM 会导致 id 类特征 embedding 有几千维，巨大的资源消耗，更好的模型结构足够的泛化能力\n更多 fancy 的模型结构引入都做出了收益，比如 MMOE, knowledge transfer, sequence model, pair wise -&gt; list wise\ngpu 的资源收益，曾经的自研框架很难像 tensorflow 一样同时 cpu，gpu 实现\n\n","categories":["recommender-system"],"tags":["机器学习","推荐系统","深度学习"]},{"title":"EROFS 镜像布局分析","url":"/2024/05/15/erofs-image-layout/","content":"以下参考 Core on-disk format\nEROFS 硬盘布局由Super block + inode区 + data区三部分组成。\n由于是只读文件系统，所有存储的文件大小都是已知的，因此相对于其他文件系统省去了inode bitmap和data bitmap的开销，节约空间。\n#Super Block\nSuperblock 大小为 128 字节，位于分区开始的第 1024 字节处，兼容以前的 boot sector。Superblock 里面保存根目录的inode_number。\n#Inode\nInode 按照固定位置紧密存放，一个inode的物理地址计算为 inode_start + inode_number * 32；\nInode 有两种大小：\n\nCompact 版本 32 字节；\nExtended 版本 64 字节；\n\nInode 有三种数据布局：\n\nFLAT_PLAIN：内容全部保存在i_u所指向的 block 中；\nFLAT_INLINE：整块的内容全部保存在i_u所指向的 block 中，剩余的部分保存在 inode metadata 后面；\nCHUNK_BASED：数据被分成多个chunks，每个chunk按照PLAIN方式布局数据；\n\n","categories":["storage-system"],"tags":["操作系统","文件系统"]},{"title":"EROFS: A Compression-friendly Readonly\rFile System for Resource-scarce Devices","url":"/2024/05/09/erofs-paper-reading/","content":"#作者\nGao Xiang @ 华为，MIngkai Dong @ IPADS @ SJTU\nUSENIX ATC 2019\n#背景\n安卓系统本身对设备存储的占用越来越大。\n\n这些系统数据大部分都是 Read-only 的。Read-only 为压缩存储提供了巨大的优化空间。\n#Ext4\nExt4 允许只读挂载文件系统，但是可以通过重新挂载绕过这种限制，不够安全。\n#Btrfs\n支持压缩，文件会分成 128KB 的块\n是 general-purposed 的文件系统，不能只为了压缩只读做很多优化\n#SquashFs\nSquashfs 广泛使用，支持多种压缩算法，可选的块大小 4KB~1MB。\n元数据可以被压缩，inode 和目录紧密存储，文件内容逐个块压缩，在 inode 中用一个 list 保存每个块压缩后的大小。\n#SquashFs 的问题\n在低端设备上使用 SquashFs 会导致数秒的巨大延迟（使用 FIO 测试，文件 enwik9）。\n#IO 放大和计算浪费：\nSquashFs 按输入固定大小分块压缩，压缩后的数据是变长的。然后变长的数据被 compact 存储。\nSquashfs 固定要对原始数据先按 fixed-size 分块，分别压缩，再拼接到一起，即使只改了一个字节也要重复整个过程；\n读数据需要先把整个 block 读进来，整个解压缩后才能定位到需要读的数据；\n工程实现问题：压缩过的块和磁盘不强制对齐，最坏情况需要多读和解压两个块；\n可以把块大小减小到 4KB，然而会降低压缩率\n\n#内存放大和数据拷贝\nSquashFs 在解压数据的时候需要大量的临时内存。\n读写数据的时候需要多次分配内存用于 page cache/保存解压缩后的数据等；\n在小内存机器上会额外加剧 swapping 开销；\n额外参考：Android 为何不使用 SquashFS\n#EROFS：增强的压缩文件系统\n#Fixed-sized output compression\n为了缓解读放大的问题，EROFS 对压缩后的数据（而非压缩前）分块，这样的好处有：\n\n压缩比更大，因为一次压缩输入的数据可能更多；\n解压的时候只需要取包含了需要数据的块，没有额外的块读取和解压开销；\n允许进行就地解压，内存开销能够稍微小一些；\n\nEROFS 采用了滑动窗口来压缩数据，在 输入大小到达 1MB 或者 输出大小到达 4k 时输出一个新的 block。\n#缓存层优化\n对于压缩型文件系统，为了保存解压后的数据，需要额外分配内存，这部分对内存的需求很大。\nEROFS 对于是否要读完整的块，采用了两种不同策略，以优化读性能：\n\nCached I/O：正常使用内核的 Page Cache；\nIn-place I/O：绕过 Page Cache 机制；\n\n对于只需要读一部分数据（部分解压）的块，使用 Cached I/O。EROFS 使用了一个特别的inode的 page cache 保存 物理块号-&gt;压缩块 之间的映射关系，在读取时可以直接定位到需要的块。\n对于需要读全部数据（完全解压）的块，使用 In-place I/O。每次读文件会主动从 Page Cache 层分配页去保存文件数据，然后就地解压数据；空闲下来的页会被复用，以减小频繁内存分配。\n这里的想法是一个已经解压过的块不太可能被再次使用（因为已经有了解压后的原始数据），占用的 page cache 就可以回收利用；如果仍然 cache 的话就没有必要，而且会增加内存抖动。\n#解压缩优化\nEROFS 对解压做了很多细致的优化。\n#vmap 解压\n\n在 EROFS 中，每次解压一个块可能会得到多块数据，如果只需要读其中一部分，那么解压出来的其他数据块就没必要保存。为此，EROFS 利用了虚拟内存映射机制，将不需要的块映射到临时页。\n实现上，每个压缩的硬盘块中会保存原始数据的元数据（如长度），在解压时会先准备好足够的 Buffer，然后使用 vmap 手动映射成连续的虚拟地址空间， 以调用解压缩算法。\n在解压时，对于需要的部分，会vmap映射一段 page cache 空间，对于不需要的部分，只会映射到临时页。\n#Per-CPU 解压\n上述的方式还有两个问题：\n\n还是需要动态分配内存页，不好；\n每次解压都要调用vmap和vunmap，不好；\n\n为此，EROFS 静态分配了若干个页专门用于解压，具体来说，每个 CPU 会持有kkk个页。当解压出来的数据不超过kkk页的时候，就用这些页装解压后的数据，然后再 memcpy 到目标 page cache 位置。\n论文中采用的kkk为 4。\n\n#滚动解压\n由于 LZ4 算法需要当前位置之前最多 64KB 的数据才能解压。EROFS 会给每个 CPU 预先分配若干个物理页，作为 vmap 解压时存储数据的基本 buffer。这些物理页会循环使用。\n#实现\nEROFS 已经合并进内核。使用内核的 4KB 块大小，LZ4 (v1.8.3) 压缩算法，文件元数据没有被压缩。\natc19-erofs 是合并进 5.3-rc1 的分支。\natc19-mkfs是论文版本的 utils，最新版本在erofs-utils。\nkmod-erofs (WIP)是 out-of-tree 版本的 erofs 模块，还没完成。\n#EROFS 镜像布局\n\n和其他文件系统一样，EROFS 镜像的开头也是Super block；然后是混合存储的元数据和数据区。\nEROFS 允许元数据（inode，目录）和数据混合存储，即 inline data，目的是提高局部性，减少 I/O 操作。\nTODO\n#解压策略\n商业版：少于 4 页时采用 per-CPU 解压，否则采用 vmap 解压；\n最新版：增加了 in-place 解压的优化。\n#优化\n#索引内存优化\nTODO\n#调度优化\nEROFS 没有单独的解压线程，而是由读数据的线程解压数据。\n一旦硬盘块（硬件）准备好就会立即开始解压。\n#协同解压\n对同一个数据的多个请求会去重，避免重复解压相同的数据。\n#镜像补丁\n在镜像末尾放置更新数据，解压之后用新数据覆盖。\n#评估\n#相关工作\n","categories":["storage-system"],"tags":["操作系统","文件系统","论文阅读","EROFS"]},{"title":"EROFS 相关资料整理","url":"/2024/05/14/erofs-resource/","content":"#代码仓库\n\nEROFS 内核驱动仓库: https://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs.git\nerofs-utils 仓库: https://github.com/erofs/erofs-utils\n其他相关项目: https://github.com/erofs\n\n#文档\n\nATC '19 论文主页: https://www.usenix.org/conference/atc19/presentation/gao\n内核 EROFS 文档: https://docs.kernel.org/filesystems/erofs.html\nEROFS 项目主页: https://erofs.docs.kernel.org/en/latest/#\nAn introduction to EROFS (from LWN): https://lwn.net/Articles/934047/\nEROFS 项目 mannual: https://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs-utils.git/about/\nEROFS Introducing Mail: https://marc.info/?l=linux-fsdevel&amp;m=152776480425624\n\n#Slide\n\nATC '19 汇报 Slides: https://www.usenix.org/sites/default/files/conference/protected-files/atc19_slides_gao.pdf\nEROFS Everywhere: An Image-Based Kernel Approach for Various Use Cases@KubeCon China '23: https://static.sched.com/hosted_files/kccncosschn2023/13/OSS-China-2023-EROFS.pdf\nImage-based read-only filesystems (EROFS) 龙蜥社区: https://docs.google.com/presentation/d/16LwD-F0IKy8okkC8qB94v2YgDcWBBE26rmMn_s1Mqxc/edit\n\n#其他资源\n\nLinux-erofs Mail List: https://lists.ozlabs.org/listinfo/linux-erofs\nEROFS 文件系统这几年（一） —— 缘起: https://mp.weixin.qq.com/s/RU9JcpjIHargA_-oF9tTcg\nEROFS 文件系统生态进展（-2024 年 1 月）: https://mp.weixin.qq.com/s/3QpbamujQSo378vXr7PlAA\n\n#第三方\n\n论文阅读记录（by @SToPire 大佬）: https://stopire.github.io/2023/05/07/EROFS-A-Compression-friendly-Readonly-File-System-for-Resource-scarce-Devices/\nEROFS 压缩文件格式和读取流程分析（by @SToPire 大佬）: https://stopire.github.io/2023/07/05/EROFS压缩文件格式和读取流程分析/\n论文阅读（by @mwish）: https://zhuanlan.zhihu.com/p/663782594\n论文阅读（by elecfans）: https://bbs.elecfans.com/jishu_2296700_1_1.html\nEROFS on-disk compact index 生成分析: https://tjtech.me/erofs-compacted-index-generation.html\nEROFS ztailpacking 特性实现详解: https://mp.weixin.qq.com/s/ZgKU68Jbgsk8bAK5eRR8EQ\nEROFS 源码阅读笔记: https://blog.csdn.net/m0_37637511/article/details/124330158\n浅谈华为 EROFS 文件系统的两个主要技术: https://blog.csdn.net/shuangguo121/article/details/90901131\n用 EROFS 进行存储与备份: https://zhuanlan.zhihu.com/p/646322364\n\n#Links\n\n我的论文阅读记录: 论文阅读\n\n","categories":["storage-system"],"tags":["操作系统","文件系统","EROFS"]},{"title":"EROFS 源码阅读","url":"/2024/05/12/erofs-source-reading/","content":"#erofs in-tree 驱动\nin-tree 的代码也是模块的形式，以 module_init 开始\nmodule_init(erofs_module_init);module_exit(erofs_module_exit);// in erofs_module_init()err = register_filesystem(&amp;erofs_fs_type);static struct file_system_type erofs_fs_type = &#123;    .owner          = THIS_MODULE,    .name           = &quot;erofs&quot;,    .init_fs_context = erofs_init_fs_context, // init -&gt;ops and -&gt;fs_private    .kill_sb        = erofs_kill_sb, // shutdown a fs    .fs_flags       = FS_REQUIRES_DEV | FS_ALLOW_IDMAP,&#125;;MODULE_ALIAS_FS(&quot;erofs&quot;);\nKconfig 中包含的一些选项：\n\nEROFS_FS_DEBUG DEBUG 支持\nEROFS_FS_XATTR 拓展属性支持\nEROFS_FS_POSIX_ACL ACL 支持\nEROFS_FS_SECURITY 安全标签支持\nEROFS_FS_ZIP 压缩支持，默认只支持读取 LZ4 压缩的数据\n\nEROFS_FS_ZIP_LZMA 支持读取 microLZMA 压缩的数据\nEROFS_FS_ZIP_DEFLATE 支持读取 DEFLATE 压缩的数据\nEROFS_FS_PCPU_KTHREAD 用 per-cpu 的线程池去作异步数据解压缩\n\nEROFS_FS_PCPU_KTHREAD_HIPRI 把这些线程设为高优先级\n\n\n\n\nEROFS_FS_ONDEMAND 利用 fscache 实现 on-demand 读\n\n#初始化流程\nstatic const struct fs_context_operations erofs_context_ops = &#123;    .parse_param\t= erofs_fc_parse_param, // add a fs param    .get_tree       = erofs_fc_get_tree, // ~mount a fs    .reconfigure    = erofs_fc_reconfigure, // sb reconfiguration    .free\t\t= erofs_fc_free, // clean up&#125;;\n#内核的 fs_context 结构\nstruct fs_context &#123;    const struct fs_context_operations *ops;    struct file_system_type *fs_type;    void\t\t\t*fs_private;    struct dentry\t\t*root;  // the root of fs tree    struct user_namespace\t*user_ns;    struct net\t\t*net_ns;    const struct cred\t*cred;    char\t\t\t*source;  // /dev/sda1 or host:/path, etc.    char\t\t\t*subtype;    void\t\t\t*security;    void\t\t\t*s_fs_info;  // user data -&gt; erofs_sb_info    unsigned int\t\tsb_flags;    unsigned int\t\tsb_flags_mask;    unsigned int\t\ts_iflags;    enum fs_context_purpose\tpurpose:8;    ...&#125;;\n#EROFS 的元数据 erofs_sb_info\nstruct erofs_sb_info &#123;    struct erofs_mount_opts opt;\t/* options */#ifdef CONFIG_EROFS_FS_ZIP    /* list for all registered superblocks, mainly for shrinker */    struct list_head list;    struct mutex umount_mutex;    /* managed XArray arranged in physical block number */    struct xarray managed_pslots;    unsigned int shrinker_run_no;    u16 available_compr_algs;    /* pseudo inode to manage cached pages */    struct inode *managed_cache;    struct erofs_sb_lz4_info lz4;#endif    /* CONFIG_EROFS_FS_ZIP */    struct inode *packed_inode;    struct erofs_dev_context *devs;    struct dax_device *dax_dev;    u64 dax_part_off;    u64 total_blocks;    u32 primarydevice_blocks;    u32 meta_blkaddr;#ifdef CONFIG_EROFS_FS_XATTR    u32 xattr_blkaddr;    u32 xattr_prefix_start;    u8 xattr_prefix_count;    struct erofs_xattr_prefix_item *xattr_prefixes;    unsigned int xattr_filter_reserved;#endif    u16 device_id_mask;\t/* valid bits of device id to be used */    unsigned char islotbits;\t/* inode slot unit size in bit shift */    unsigned char blkszbits;\t/* filesystem block size in bit shift */    u32 sb_size;\t\t\t/* total superblock size */    u32 build_time_nsec;    u64 build_time;    /* what we really care is nid, rather than ino.. */    erofs_nid_t root_nid;    erofs_nid_t packed_nid;    /* used for statfs, f_files - f_favail */    u64 inos;    u8 uuid[16];                    /* 128-bit uuid for volume */    u8 volume_name[16];             /* volume name */    u32 feature_compat;    u32 feature_incompat;    /* sysfs support */    struct kobject s_kobj;\t\t/* /sys/fs/erofs/&lt;devname&gt; */    struct completion s_kobj_unregister;    /* fscache support */    struct fscache_volume *volume;    struct erofs_fscache *s_fscache;    struct erofs_domain *domain;    char *fsid;    char *domain_id;&#125;;\nbdev.c 中负责硬盘块操作\n申请 super_block\nerofs_fc_fill_super 其实是从硬盘上读取数据，初始化 user 的 sb 信息\n#inode 操作\nconst struct inode_operations erofs_generic_iops = &#123;    .getattr = erofs_getattr,    .listxattr = erofs_listxattr,    .get_inode_acl = erofs_get_acl,    .fiemap = erofs_fiemap,&#125;;const struct inode_operations erofs_symlink_iops = &#123;    .get_link = page_get_link,    .getattr = erofs_getattr,    .listxattr = erofs_listxattr,    .get_inode_acl = erofs_get_acl,&#125;;const struct inode_operations erofs_fast_symlink_iops = &#123;    .get_link = simple_get_link,    .getattr = erofs_getattr,    .listxattr = erofs_listxattr,    .get_inode_acl = erofs_get_acl,&#125;;\n#erofs-utils\n\nmkfs.erofs 创建一个 erofs 文件系统；\nfsck.erofs 检查 erofs 镜像的完整性；\ndump.erofs 导出 erofs 镜像中的相关细节；\nerofsfuse 用户态\n\n#mkfs\n#fsck\n#dump\n#erofsfuse\n","categories":["storage-system"],"tags":["操作系统","文件系统"]},{"title":"io_uring 入门","url":"/2025/07/19/io-uring/","content":"io_uring 是 Linux 内核 5.1 版本引入的异步 I/O 框架，\n#原理\n#代码组织\n完整的 io-uring 由内核 syscall、liburing 库、用户态程序三部分组成。\n\n内核提供了 io_uring_register, io_uring_setup, io_uring_enter 三个 syscall 接口。\nliburing 库提供了 io_uring_queue_init, io_uring_get_sqe, io_uring_wait_cqe 等函数。\n用户态程序通过 liburing 库调用内核 syscall 接口，实现 I/O 请求的提交和完成。\n\n#核心结构\n每个 io_uring 实例由一个 struct io_uring 结构体表示。\nstruct io_uring &#123;    struct io_uring_sq sq;    struct io_uring_cq cq;    unsigned flags;    int ring_fd;    unsigned features;    int enter_ring_fd;    __u8 int_flags;    __u8 pad[3];    unsigned pad2;&#125;;\n每个 io_uring 实例包含两个环形队列：\n\nsubmission queue (SQ)：提交队列，用于提交 I/O 请求。\ncompletion queue (CQ)：完成队列，用于接收 I/O 请求的完成结果。\n\n#使用方式\n应用层通过 io_uring_queue_init 初始化一个 io_uring 实例。\nstruct io_uring ring;struct io_uring_params params;memset(&amp;params, 0, sizeof(params));params.flags |= IORING_SETUP_SQPOLL; // 开启 SQPOLL 模式params.sq_thread_idle = 2000; // 2000ms 内没有 I/O 请求，内核线程会睡眠io_uring_queue_init(1024, &amp;ring, &amp;params);\n应用层通过 io_uring_get_sqe 创建 SQE(Submission Queue Entry) 结构体，并设置 I/O 请求的参数: 操作类型、文件描述符、缓冲区、偏移量等。可以创建多个 SQE 结构体，并设置不同的私有数据。\nstruct io_uring_sqe *sqe;sqe = io_uring_get_sqe(&amp;ring);io_uring_prep_read(sqe, fd, buf, len, offset); // 提交一个 read 请求io_uring_sqe_set_data(sqe, (void *)1); // 设置 SQE 的私有数据struct io_uring_sqe *sqe2;sqe2 = io_uring_get_sqe(&amp;ring);io_uring_prep_fsync(sqe2, 1, 0); // 提交一个 fsync 请求io_uring_sqe_set_data(sqe2, (void *)2); // 设置 SQE 的私有数据io_uring_submit(&amp;ring);\n提交后，应用层通过 io_uring_wait_cqe 等待已完成的 I/O 请求，并获取完成结果。\nstruct io_uring_cqe *cqe;int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);if (ret &lt; 0) &#123;    printf(&quot;io_uring_wait_cqe failed: %s\\n&quot;, strerror(-ret));    return -1;&#125;io_uring_cqe_get_data(cqe); // 获取完成结果io_uring_cqe_seen(&amp;ring, cqe); // 标记完成结果已处理\n#工作模式\n\n中断模式 (默认): 在 io_uring_enter 时，采用中断模式，等待 CQ 中出现完成的 IO 请求。\nIORING_SETUP_IOPOLL 在 io_uring_enter 时，采用 polling 而非中断模式，等待 CQ 中出现完成的 IO 请求。\nIORING_SETUP_SQPOLL 创建一个专用的内核线程 polling SQ 并处理 IO。\nIORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL 睡眠等待 CQE 完成。\n\n","categories":["storage-system"],"tags":["io_uring","Storage"]},{"title":"SPDK 入门","url":"/2025/04/10/spdk-intro/","content":"SPDK 是 Intel 提出的一套用户态 NVMe SSD I/O 软件栈，其核心是使用用户态的 SSD 驱动绕过上下文切换的开销。\n因为驱动程序放在用户态，自然也没法使用 Linux 的 FS 接口，配套地，SPDK 提供了 Blobstore​​ 和 ​​BlobFS​​ 接口。支持对接 MySQL、RocksDB、Ceph 等。\n","categories":["storage-system"],"tags":["Storage","SPDK"]},{"title":"Tair 存储分析","url":"/2024/05/14/nosql/","content":"","categories":["database-system","nosql"],"tags":["Tair"]},{"title":"2020年度总结","url":"/2021/01/05/2020-summary/","content":"#进步\n#装备升级\n\n\n新增显示器\n\n\n新增一个 Pad\n\n\n#技术进步\n\n\n新开了 Blog!\n\n\n开始刷算法题, 并已经刷了一些\n\n\n前端技术更加熟练了\n\n\nVue/React 双线操作\n\n\n简单页面能够快速开发\n\n\n\n\nPython 更加熟练了\n\n\n爬虫技术\n\n\n数据处理技术\n\n\nAI/DL\n\n\n\n\nJava 语言入门\n\nSpring 框架\n\n\n\niOS 应用开发入门\n\n\nSwift 语言入门\n\n\nSpriteKit 游戏 * 1\n\n\n\n\n#综合能力\n\n\n表达能力略微提升\n\n\n管理能力略微提升\n\n\n#人际关系\n\n认识了很多新朋友!\n\n#不足\n#任务调度\n\n时间利用率不高, 为完成项目曾多次通宵\n\n#管理, 组织能力\n\n小组合作时, 作为组长, 任务分配还有待优化\n\n#心态\n\n有时候会比较焦虑\n\n","categories":["misc","daily"]},{"title":"2021目标计划","url":"/2021/01/05/2021-wishes/","content":"#2021 年目标\n#学习上\n\n先把期末考试完美解决\n\n#技术上\n\n\n想深入学习一下 C++!\n\n\n多刷算法题目!\n\n\n多写 Blog!\n\n\n学习 Android 开发\n\n\n实现大的后端架构\n\n\n学习新语言: Go/Julia/Ruby/…\n\n\n","categories":["misc","daily"]},{"title":"2023 年终总结","url":"/2024/01/01/2023-summary/","content":"#2023 回顾\n\n1 月: 开源投毒\n1-3 月: Rust 教材\n4-5 月: 加入误报消减团队, 投 ASE/ICSE 论文\n3,5-10 月: 相似代码片段但是有漏洞情况不同 自己搜索, 研究 PDG, Call Graph, LLVM Pass, 函数切片, 错过 FSE\n7-11 月: 参加 GLCC 开源活动，参与进了一直以来都很崇拜的 Kata Containers 项目\n8-12 月: 搞浦发银行的项目\n\n#近况\n个人: 研二在读, 人在杭州, 一篇一作论文在投, 每天搬砖, 没有确定的方向, 毕业有点迷茫\n实验室: 宁波实验室人丁兴旺, hw 稀稀拉拉, 反倒宁波更好一些\n#体会\n研究生比本科忙多了, 没有时间去做自己想做的事情\n甚至不知道 2023 年都做了些什么(笑), 只能看周报回忆\n#2024 希望\n\n多读书\n多运动\n找到好工作\n学习唱歌\n\n","categories":["misc","daily"]},{"title":"每日小结","url":"/2024/05/12/2024-05-13/","content":"#软链接更新\nln有个-f,--force更新链接，再也不用rm再重建了。\n#Conda = Root-less apt\n升级gcc依赖库：conda install libgcc libgcc-ng\n升级libstdc++：conda install -c conda-forge libstdcxx-ng\n","categories":["misc","daily"],"tags":["Linux"]},{"title":"2024 年终总结 -- 记学生时代的终结","url":"/2025/03/15/2024-summary/","content":"不知不觉 2025 年已经过去了两个多月，感觉再不写这篇总结就要忘记了。\n那么事不宜迟，现在让我们回顾一下 2024 年。\n#2024 回顾\n\n1 月: 似乎没什么事情\n2 月:\n\nKata Containers 的项目收尾，Good Job！\n和妈妈在杭州旅游，然后回老家过年，开心\n\n\n2,6-8,11 月: 科研工作论文\n\n2 月: 过年在家编 Rebuttal\n6-8 月: Rebuttal 见效，给了个 Major Revision，终于拨云见日…断断续续改了 好几个月\n11 月: 二轮终于接收，如释重负 😭\n\n\n5 月: 实验室 kg 拿到了 ICSE Distinguished Paper，Good Job！\n3-5 月：暑期实习疯狂投简历\n\n被字节简历挂了 😡，原因不明，只能解释为投太晚了\n靠 Rust 和安全方面的经验拿到了隐私计算的 Offer，但感觉和自己的方向不太符合，最终放弃了\n由于换 HR，最后的 Offer 拖了 1.5 个月才拿到，人快麻了\n\n\n3-8 月: 辅助实验室学长的漏洞课题\n5-9 月: 去某公有云大厂开始暑期实习，做 Redis &amp; Tair Persistent Memory &amp; PDB 相关 😄\n8-11 月: 准备秋招\n\n9 月暑期实习转正顺利\n投+Offer 了 8 家大厂，拉扯了 3 个月，整体过程是一波三折，好在最终的结果还算满意\n\n\n11-12 月: 参加 Rust 数据结构与算法学习赛，赢到了一个 iPad，意外惊喜 😲\n4,10-12 月: 写研究生的毕业论文\n\n#关于实习\n实习对云计算产业有了很多新的认识。\n对公有云的认识：本质上还是 toC 的业务（因此对于客户服务要求极高），只不过卖的产品是云资源。\n对工区的感受：“大概是把演唱会的观众席变成办公桌，观众变成程序员的感觉”\n实习同学还有 cvpr oral 二作哥，厉害。\n遇到了很多 nb 的师兄，不管是人际关系方面还是技术方面都有一些收获。\n#关于实验室\nhw 实验室从 z3 三楼搬到四楼（23.12.20），然后又搬到了 z9（25.1.2），真够折腾的。\n宁波和 hw 实验室都来了很多新朋友，不过作为老人社交方面有点摆烂了。\n(25.1) kg 考完雅思就去了 SG，拜到了 SE 的大师 🙏。\n#关于 AI\n难以置信，24 年看来还是不可能实现的任务，过了一年就有了新的突破。\n#读的书\n\n房思琪的初恋乐园\n雪国\n\n培养了使用微信读书的习惯。\n#有趣的事情\n和实验室友友们 Cooking 了好多次，好好吃 😋\n自学硬件，画了很多板子，焊接了很多电路，感谢嘉立创 🙏\n#未来的祝福\n工作顺利！\n","categories":["misc","daily"]},{"title":"电子电路基础","url":"/2024/09/12/basic/","content":"#基本概念\n\n电荷 Q 单位库伦 C\n电场\n电流\n\n1A = 1C/s\n电流方向\n\n直流电流 DC\n交流电流 AC\n混合电流: 直流分量=平均值, 交流分量=偏离值\n\n\n\n\n基尔霍夫电流定律 KCL\n电压定律\n电动势 单位伏特 V\n\n描述其他能量形式转化为电能的能力\n\n\n电源 提供电动势\n\n恒压源\n恒流源\n信号源\n\n\n电压 单位伏特 V\n\n描述两点之间的电位差\n描述电能转化为其他能量形式的能力\n1V = 1J/C\n电位=对地电压\n\n参考地 G 电位为 0\n\n\n\n\n功率 P\n\n电路释放能量的速率\n1W = 1J/s = 1V*A\nP&lt;0 表示电路从外界吸收能量 释放电压和电流\n\n例如 电容 电感\n\n\n\n\n信号\n\n带有信号的随时间空间变化的电压或电流\n确定性信号\n随机信号\n伪随机信号\n\n\n\n#电容\n\n单位\n\n法拉 F\n1F = 1C/V\n物理意义: 电容器两极板上的电荷量与电压之比\n\n\n时间常数\n\n电容器充放电时间常数 τ=R∗C\\tau = R*Cτ=R∗C\n物理意义: 电容器充电到额定电压的 63.2% 所需时间\n一般取 5τ\\tauτ 为充放电时间\n\n\n注意事项\n\n高耐压电容充满电后 不能上手触摸 有触电危险\n泄放电阻(bleeder resistor): 并联在电容器上的大电阻 用于释放电容器上的电荷\n使用前应先放电\n\n\n\n#半导体\n\n本征半导体\n掺杂半导体\n\nN 型半导体\nP 型半导体\n\n\n\n#二极管\n\n特性\n\n正向低电压：不导通，死区\n正向高电压：导通\n反向低电压：不导通\n反向高电压：击穿\n\n\n主要参数\n\n阈值电压: 硅 0.5V 锗 0.1V\n正向电压降 Vf: 硅 0.7V 锗 0.2V\n最大整流电流 Ir: 可稳定工作的最大正向电流\n反向击穿电压 Vr\n反向峰值电压 VRM: 可稳定工作的最大反向电压，一般为 Vr 的 1/2\n反向峰值电流 IR: 反向峰值电压时的电流\n\n\n理想二极管模型\n\n正向导通 电压降为 0\n反向不导通 电流为 0\n\n\n温度特性\n应用\n\n限幅\n钳位电路: 结合电容\n\n\n稳压二极管\n\n特点: 反向击穿后, 不管反向电流怎么增加, 电压基本不变\n用法: 反向并联在负载上\n作用: 保护电路不受过高电压的影响\n主要参数\n\n稳定电压 Vz\n稳定电流 Iz\n动态电阻 rz\n\n\n\n\n\n#三极管 BJT\n\n结构\n\n集电极 Collector\n发射极 Emitter\n基极 Base\n\n\n类型\n\nNPN\nPNP\n\n\n特性\n\n输入特性\n\niB=f(vBE)i_B = f(v_{BE})iB​=f(vBE​)\n\n\n输出特性\n\n\n应用\n\n作为开关\n作为放大器\n\n\n\n#电路系统\n\n系统属性\n\n线性/非线性: 线性系统满足叠加性和均匀性\n\n叠加性 f(x1+x2) = f(x1) + f(x2)\n均匀性 f(ax) = af(x)\n\n\n时变/时不变: 时变系统的输出取决于输入信号的时间\n\nf(x(t)) = y(t) != f(x(t-t0)) = y(t-t0)\n\n\n记忆/无记忆: 记忆系统的输出取决于输入信号的历史\n\n记忆: 电容\n无记忆: 电阻\n\n\n线性时不变(Linear Time-Invariant, LTI)系统\n\n\n黑箱、端口与网络\n\n端点: 从电路网络中引出的一个点 用于测量或者构建更大的网络\n端口: 电流始终相反的一对端点 (其中的部分当成一个黑箱)\n端口条件: 一个电路系统只有能够定义出端口，才能够套用电路模型分析; 否则只能用电磁场方程求解\n多端口网络: 一个网络中有多个端口\n\n一般有一个接地端点\n\n\n电路功能一般是通过端口电量关系方程描述的\n\n流控网络: 电压随电流按固定规律变化 因此通过改变电流控制电路\n压控网络: 电流随电压按固定规律变化 因此通过改变电压控制电路\n\n\n\n\n端口连接\n\n串联: 电流相同 电压相加\n并联: 电压相同 电流相加\n对接: 电压电流都相同\n\n\n有源(active)网络: 能够自端口向外提供电能量\n\n太阳能二极管\n初始 v&gt;0 的电容\n初始 i&gt;0 的电感\n否则是 无源(passive)网络\n\n\n理想电源\n\n理想电压源: 电压不随电流变化 v(t) = V\n理想电流源: 电流不随电压变化 i(t) = I\n理想电阻 i(t) = v(t)/R\n电导 G = 1/R\n\n\n线性内阻电源 真实电源的简化模型\n\n元件约束条件 i(t)IS0+v(t)VS0=1\\frac{i(t)}{I_{S0}} + \\frac{v(t)}{V_{S0}} = 1IS0​i(t)​+VS0​v(t)​=1\n正弦波电源 i(t)ISp+v(t)VSp=cos(ωt)\\frac{i(t)}{I_{Sp}} + \\frac{v(t)}{V_{Sp}} = cos(\\omega t)ISp​i(t)​+VSp​v(t)​=cos(ωt)\n电压源(戴维南形式) v(t)=VS0+R∗i(t)v(t) = V_{S0} + R*i(t)v(t)=VS0​+R∗i(t)\n电流源(诺顿形式) i(t)=IS0−v(t)Ri(t) = I_{S0} - \\frac{v(t)}{R}i(t)=IS0​−Rv(t)​\n等效电路\n\n\n等效电阻\n\n短路 v(t)=0\n开路 i(t)=0\n开关 关闭时相当于短路，开启时相当于开路\n受控开关 有两个端口 端口 1 控制端口 2 的导通\n\n逆变器 直流电源转交流电源\n整流器 交流电源转直流电源\n\n\n二极管\n\n正向 i 随 v 接近指数增长\n简化模型相当于受控开关 或者两段折线\n分析方法 图解法\n\n\n负阻二极管\n\n隧道二极管\n肖克利二极管\n\n\n晶体管\n\n非线性三端元件: 栅极 Gate 漏级 Drain 源级 Source\n压控电阻特性 G 段电压控制 D-S 电阻\n反相电路\n\n\n\n\n电源\n\n发电机 恒压源\n直流电池 近似恒压源\n太阳能电池 恒流源\n光电二极管 信号源\n接收天线 信号源\n信号发生器\n噪声源\n\n电阻热噪声\n\n\n\n\n\n#电路基本定律和定理\n\n基尔霍夫定律\n\n电流 节点流入=流出 ∑k=1nik=0\\sum_{k=1}^{n}i_k = 0∑k=1n​ik​=0\n电压 环路电压总和=0 ∑k=1nvk=0\\sum_{k=1}^{n}v_k = 0∑k=1n​vk​=0\n\n\n戴维南定理 包含独立电源的单端口线性电路 等效于 一个恒压源串联一个电阻\n诺顿定理 包含独立电源的单端口线性电路 等效于 一个恒流源并联一个电阻\n\n#电路元件\n\n电阻\n\nu(t)=R∗i(t)u(t) = R*i(t)u(t)=R∗i(t)\n\n\n二极管\n\nPN 结 阳极 阴极\n正向 i(t)=Is(ev(t)/VT−1)i(t) = I_s(e^{v(t)/V_T}-1)i(t)=Is​(ev(t)/VT​−1)\n\n电压超过门槛值时导通 导通后电流随电压指数增长\n\n\n反向几乎不导通 超出反向击穿电压后导通\n\n作为稳压二极管\n\n\n\n\n电容\n\nq(t)=C∗v(t)q(t) = C*v(t)q(t)=C∗v(t)\n\n\n\n#模拟电路\n\n按照关系\n\n线性电路\n非线性电路\n\n\n按照功能\n\n放大\n滤波\n比较\n混频\n电源管理\n信号处理\n\n\n\n#放大电路\n\n功能: 放大输入信号的幅度以便后续处理\n应用: 构成滤波、振荡、稳压等功能电路的基本单元电路\n分类\n\n线性/非线性放大电路\n电压/电流/…放大电路\n\n\n电路图标识: 三角符号\n\n信号输入\n信号输出\n工作电源\n\n\n基本模型\n\n电压放大\n电流放大\n互阻放大\n互导放大\n\n\n分析方法\n\n戴维南模型\n诺顿模型\n\n\n性能指标\n\n输入电阻\n输出电阻\n增益 电压增益 电流增益 单位分贝\n频率响应\n\n幅频响应 相频响应\n通频带\n线性失真: 幅度失真 相位失真\n\n\n\n\n多级放大电路\n\n电路分析\n\n\n集成运放: 高增益 低输入输出阻抗\n\n电压传输特性\n负反馈电路: 降低增益 提高稳定性\n\n输出处理后反馈到输入端\n负反馈: 实现振荡器\n\n\n内部结构\n\n差分放大电路\n共模抑制: 抑制共模信号(一般是噪声或者零点漂移)\n功率放大电路\n静态分析\n\n\n\n\n理想运放\n应用\n\n比例放大\n加减运算\n积分微分\n电压比较器\n波形发生器\n\n\n\n#振荡电路\n\n多谐振荡器: 用来产生在两种状态间变化的系统\n\n","categories":["misc","electronic"],"tags":["电子电路"]},{"title":"插头插座相关中国国家标准整理","url":"/2024/12/09/socket/","content":"现行插座国家标准一共 9 本，分别为：\n\nGB2099.1 家用和类似用途插头插座第 1 部分：通用要求、\nGB2099.2 家用和类似用途插头插座第 2 部分：器具插座的特殊要求、\nGB2099.3 家用和类似用途插头插座第 2-5 部分：转换器的特殊要求、\nGB2099.4 家用和类似用途插头插座第 2 部分：固定式无联锁带开关插座的特殊要求、\nGB2099.5 家用和类似用途插头插座第 2 部分：固定式有联锁带开关插座的特殊要求、\nGB2099.6 家用和类似用途插头插座第 2 部分：带熔断器插头的特殊要求、\nGB2099.7 家用和类似用途插头插座第 2-7 部分：延长线插座的特殊要求、\nGB2099.8 家用和类似用途插头插座第 2-4 部分：安全特低电压(SELV)插头插座的特殊要求、\nGB2099.9 家用和类似用途插头插座第 2-9 部分：信息插座（即光纤、电话线、宽带线等）的特殊要求\n\n#1997 年\n\n颁布 GB2099.1 ~ GB2099.6 六项新国标，分别规定了插头插座的通用要求和特殊要求。\n\n#2008 年\n\n修订 GB2099.3\n\n#2015 - 2017 年\n\n颁布 GB2099.7 标准，规定了延长线插座（俗称插排、插线板）的特殊要求\n颁布 GB/T 2099.7 推荐标准，规定了延长线插座（俗称插排、插线板）的推荐标准（俗称排插新国标）\n颁布 GB2099.8 标准，规定了安全特低电压（SELV）插头插座的特殊要求\n颁布 GB2099.9 标准，规定了信息插座（即光纤、电话线、宽带线等）的特殊要求\n修订 GB2099.3\n颁布 GB/T 2099.3；\n\n\n\n提高电源线导线线径。新国标要求，额定电流 10A 的延长线插排，导线的最小横截面积由原来的 0.75mm^2 提高到 1mm^2;额定 16A 的则从 1mm^2 提高到 1.5mm^2。\n\n\n淘汰万能插排。新国标要求插排允许将符合国标 GB 1002 要求的二孔插孔与三孔插孔排列组合，但不能相互重合或共用。这意味着万能插排正式退市禁用。\n\n\n插线板必须设置保护安全门，避免儿童因为手指或金属物体插入导致触电。\n\n\n增加插排材料的阻燃等级。新国标插排新增了明火针焰测试，要求针焰明火与插排接触 30 秒钟后不起燃，或者起燃后 30 秒后自动熄灭。\n\n\n必须获得 CCC 认证。新国标要求排插必须要通过国家强制性产品认证，即“CCC&quot;认证，消费者可以在排插的本体上观察是否有这个标志。\n\n\n新增了针焰测试项目，要求针焰明火与插座接触 30 秒钟后不起燃，或者起燃后 30 秒后自动熄灭\n\n\n\n#2021\n\n修订 GB2099.1\n\n#2022\n\n修订 GB2099.3\n\n#2024\n\n修订 GB 2099.7 标准\n\n","categories":["misc","electronic"],"tags":["电子电路"]},{"title":"Verilog","url":"/2024/09/21/verilog/","content":"","categories":["misc","electronic"],"tags":["电子电路"]},{"title":"分布式系统基础","url":"/2024/04/04/distributed-system/","content":"#为什么需要分布式系统\n单点故障，分布式增加冗余提高系统的可用性\n#分布式系统存在的问题\n#CAP 定理\n对于一个分布式计算系统来说，不可能同时满足以下三点：\n\n一致性 (Consistency): 所有节点都能够访问到同一份最新的数据副本或者错误。\n可用性 (Availability): 每次请求都能获取到非错误响应，但不保证获取的数据为最新数据。\n分区容错性 (Partition tolerance): 在节点之间的网络通信出现故障的情况下，系统仍能继续运行。\n\n分析:\n\n网络故障必定存在，如果一发生网络分区整个系统就不可用，是没有意义的。因此分布式系统必须首先要保证 P 性质，接下来有两个选择: C 或者 A。\n如果选择 C 一致性，则当网络分区无法保证数据是最新的时候，系统就会返回错误或者超时，即不可用。\n如果选择 A 可用性，则系统始终尝试返回信息的最新版本，但是无法保证是最新的。\n\n#BASE (碱) 语义\n分布式系统的一种用于实现高可用性的一致性模型，弱化一致性的承诺，保证可用性，因此属于 AP 的一种拓展。\n\n基本可用 (Basically Available): 读写操作尽可能是可用的，但是不保证一致性 (在冲突的情况下，写值可能不会成功; 读值可能不是最新的)\n软状态 (Soft-state): 没有强一致性保证。\n最终一致性 (Eventually consistent): 系统运行足够长的时间后，多个节点最终会达到一致。\n\n#C 与 A 如何取舍\n\n选择一致性 (CP): 对一致性要求较高的系统，例如:\n\n传统 ACID 数据库，服务注册中心 (Etcd, Zookeeper, Nacos)，HBase，金融系统，银行系统，与钱相关的\n\n选择可用性 (AP):\n\nEureka，Redis 集群，Nacos 配置中心\n","categories":["misc","interview"],"tags":["学习","面试"]},{"title":"暑期实习 - 面试八股文大纲","url":"/2024/04/07/interview/","content":"#Leetcode\n\n链表\ndp\n排列组合\n大数模运算\n图算法\n数论\n字符串\n\n#杂项\n\n自我介绍\n开放性问题\n\n项目中遇到的困难问题\n最近关注的新技术\n最近读了什么书\n\n高性能 MySQL\nRedis 源码分析\n设计数据密集型应用 DDIA\n大规模并行处理器程序设计\n\n\n还在面其他的什么公司没\n\n其他几家云厂商，不过首选的是 xx\n\n\n有没有别的 offer\n\n目前拿到了一个 xx 的 offer，但不是最满意的，其他厂在评估中；贵公司的 offer 与我的意向最最相符\n\n\n为什么选择 xx 城市: 学校/家里/npy 在 XX\n为什么来 xx 公司\n\nxx 公司是国内领先\n离学校比较近\n我的兴趣是 xx 和 yy 公司很匹配\n能够让我发挥特长，施展才学，获得成就感\n\n\n时间管理\n\n利用层次化的 todolist 软件分解、管理任务\n优先完成紧急的\n\n\n保研细节\n\n为什么选择这个实验室\n为什么选择这个方向\n\n解决的问题（软件工程问题）很贴近现实且重要\n采用的方法 AI 新颖，是未来的趋势\n\n\n\n\n职业规划\n\n1-2 年内 认真完成工作任务 全面建立起对这个行业和市场的了解\n未来的 3 年之内，我希望靠自己优秀的业绩从 xxxx 职位的执行工作上升到管理方面的工作。\n远期规划，我会根据环境的变化，工作内容的变化，以及我自身能力的变化，不断进行调整的。\n对于职业规划，我暂时的考虑是这样子的。谢谢！\n\n\n关于加班\n\n如果遇到紧急任务 牺牲部分个人时间 没问题\n\n\n用过公司什么产品\n\nECS/轻量应用/数据库/缓存/容器服务\n\n\n最佩服的人\n\nLinus Torvalds 技术大牛 构建了今天的互联网\n\n\n字节的产品 抖音、今日头条、多闪、飞书、西瓜视频、火山小视频、faceU 激萌、悟空问答、飞聊、皮皮虾、懂车帝、图虫\n优点\n\n实事求是\n\n\n缺点\n\ni 人 有时喜欢自闭 但是也在逐步变得 open\n\n\n印象最深的事情\n\n参加学校的 CTF 竞赛，奋战一周，最终获得校内第一名\n\n\n兴趣爱好\n\n骑自行车/跑步\n音乐\n读书\n\n\n烦恼\n\n有点 i\n\n\n最大的挫折\n\n论文被拒\n\n\n朋友评价\n\n(导师) 有责任感\n(同学) 能力强\n热心 乐于助人\n有点 i\n\n\n坚持最久的事情\n\nLeetcode 每日一题\n\n\n激发\n\n挑战\n成就\n\n\n动机\n\n发挥特长 收获满足\n\n\n\n\n什么时间可以入职\n反问问题\n\n面试表现如何/还需要加强哪些方面\n新员工培养机制\n(HR) 面试之后下一步的安排是怎样的\n(HR) 面试之后大概多久能出结果\n对于实习生/这个职位的期望\n如果我有幸能成为公司的一员，请问我在入职前需要提前做哪些准备\n(故作思考)~转正率有多少 (一般不回答)\n我想要了解的都已经了解到了，您讲的也非常清楚，我这里没有什么问题了，谢谢\n\n\nSTAR 原则\n\nSituation\nTask\nAction!\nResult!\n\n\n\n#智力题\n\n高楼扔鸡蛋\n老鼠喝毒药\n海盗分金\n博弈论\n\n两个人轮流拿 1 或 2 或 3 个的石头，怎么能保证拿到最后一个\n\n\n\n#计算机基础\n#数据结构 ✅\n\n哈希表\n\n冲突解决\n\n开链法\n\n链表\nJDK&gt;=8 红黑树\n\n\n开放寻址\n\n线性探测法\n二次探测法\n\n\n再哈希\n\n\n扩缩容\n\n负载因子 = 元素个数 / 桶个数\n\nJDK 默认 &gt;0.75 扩容\nRedis &lt;0.1 缩容\n\n\nRedis - 渐进式 Rehash\n\n防止扩容时 key 过多导致卡顿\n懒迁移\n\n\nHashMap 一次性完成\n\n\n\n\nB 树 B+树\n\n+节点密度大, 3 层能存 2kw 数据, 需要的 IO 少\n-节点分裂合并复杂\n使用场景\n\nMySQL 索引\n\n\n\n\nSkipList\n\n查找,插入 O(logn)\n+编写简单\n+插入删除更简单\n-2kw 需要 24 层, 查询需要的跳转/IO 次数多\n使用场景\n\nRedis\nRocksDB\nLucene &amp; ElasticSearch\n\n\n\n\nAVL\nRb-tree\n堆\n\n#算法 ❓\n\n排序算法\n\n选择排序\n插入排序 (打牌) 稳\n冒泡排序 稳\n快速排序\n归并排序 稳\n桶排序\n堆排序\n基数排序 稳\n希尔排序\n稳定 &amp; 不稳定\nSTL -&gt; timsort\nJava -&gt;\n\n\n\n排序方法 平均 好 坏 空间 稳定性插入排序 n² n n² O(1) 稳希尔排序 n² n n² O(1)\n选择排序 n² n² n² O(1)\n冒泡排序 n² n n² O(1) 稳堆排序 nlogn nlogn nlogn O(1)\n快速排序 nlogn nlogn n² nlogn\n归并排序 nlogn nlogn nlogn n 稳基数排序 O(d(r+n)) O(d(n+rd)) O(d(r+n)) O(rd+n) 稳\n\n布隆过滤器\n\nkey -&gt; hash 到多个 key\n快速判断元素是否存在于某个集合里面\n\n\n图算法\n\nDijkstra: BFS 每个节点更新最短距离，单源，需要无负权边+无环\nFlord\n\n\n\n#体系结构 ❓\n\nMMU\n\n位于 CPU 芯片内部\n将 CPU 请求的虚拟地址翻译成物理地址\nTLB\n\n缓存了当前进程使用的页表项(PTE)\n在 MMU 旁边 被 MMU 使用\n\n\n\n\nCPU Cache\n\n速度\n\nL1: 1ns 64KB\nL2: 4ns 256KB\nL3: 10ns 8MB\nMemory: 100ns 8GB\n\n\n拓扑\n\nWrite Through: 写内存同时写 Cache\nWrite Back: 写 Cache, 异步写内存\n\n\n多核缓存一致性\n\nMESI(梅西)协议\n\nM: Modified\nE: Exclusive\nS: Shared\nI: Invalid\n\n\nMOESI(萌西)协议\n\nO: Owner\n+允许多个 CPU 同时共享同一块缓存\n+性能更好\n\n\n\n\n\n\n内存栅栏 ❓\nNUMA\n\nUMA(Uniform Memory Access) 多核对称\nNUMA(Non-Uniform Memory Access) 多核不对称\nNote: 苹果的 UMA(Unified Memory Architecture) 表示 CPU/GPU 共享内存，和这里的 UMA 不是一回事\n\n\n\n#操作系统 ✅\n\nSocket\n文件系统\nShell 命令 ✅\nCPU 管理\n\n进程 线程 协程\n\n进程\n\n资源分配单位\n有自己的地址空间 堆 PCB 页表 文件表 信号处理\n\n\n线程\n\n执行和调度单位\n有自己的栈 寄存器\n\n\n协程\n\n用户态模拟的线程\n程序自己控制切换\n\n\n\n\n进程状态\n\nR: Running 可被调度\nS: Sleeping 等待中 可以被信号等中断\nD: Sleeping 系统调用中 不可被中断\nT: Stopped 收到了 SIGSTOP/SIGCONT 继续\nZ: Zombies 进程已结束 但在等待父进程读取状态\nX: Dead 已经退出\n\n\n孤儿进程: 父进程退出\n进程文件: lsof, /proc/fd/fd\n调度算法\n\nFCFS\nSPF\nRR\n多级队列\n\n\n进线程切换\n\n上下文切换\n特权级切换\n页表切换\n\n刷新 TLB\n\n\n\n\n进程间通信\n\n信号\n利用锁\n\n消息队列 msg{get,snd,rcv}()\n信号量 sem{init,post,wait}()\n\n\n利用文件\n\n管道/FIFO\nUDS\nTCP\n\n\n利用内存\n\n共享内存 shm{get,at}()\n\n\n\n\n\n\n锁\n\nmutex\nsemaphore\n原子变量\n条件变量\nvolatile\nCAS\n\nABA 问题\n\n\n死锁\n\n\n内存管理\n\n为什么需要段页\n\n-如果连续分配内存会导致碎片\n\n\n段式内存管理\n\n段表 段权限\n-外碎片\n\n\n页式内存管理 x86-64\n\n4k 固定大小 页权限\n+缓解外碎片 提高内存使用效率\n-内碎片\n缺页中断\n换页算法\n\nLRU\nLFU\nFIFO\n\n\n\n\n段页结合\n\n段表+页号+偏移\n-&gt; 页表+页号+偏移\n-&gt; 地址\n\n\nLinux 实际上不使用段机制\n\nSegFault 检测实际上是通过设置页权限实现的\n\n\n\n\n\n#OOP &amp; 设计模式 ❓\n\n封装 继承 多态\n继承的缺点\n\n不够灵活 随着项目发展 不利于重构\n会有菱形继承问题\ngolang Rust 等很多语言去掉继承\n\n\n重载\n工厂模式\n单例模式\nBuilder 模式\n代理模式\n适配器模式\n\n#计算机网络 ❓\n\nOSI 七层\n\nL7 应用层\nL6 表示层\nL5 会话层\nL4 传输层\nL3 网络层\nL2 数据链路层\nL1 物理层\n\n\nOSI 五层\n交换机 路由器\n常见的网络协议\n\nMAC\nTCP/UDP\nSSH\nFTP\nARP\n\n\nIP 协议 网络层\n\nIP 头 20 字节 源 ip 目标 ip\nARP\n\n广播 ARP 请求 询问\nARP 欺骗\n\n\nICMP ping\nDHCP\nNAT\n\n\nTCP 协议 传输层\n\n可靠 点对点 字节流 面向连接 20 字节\n源 port 目标 port seq ack 长度 标志\n可靠性保证 ❓\n\n序列号 确认机制 超时重传\n流量控制 拥塞控制\n握手流程\n\n第二次握手重试\n\n\n挥手流程\n\n第一轮：\n\nEst -(+FIN)-&gt; FIN_WAIT_1 -(-ACK)-&gt; FIN_WAIT_2\nEst -(+ACK/-FIN)-&gt; CLOSE_WAIT\n\n\n第二轮\n\nFIN_WAIT_2 -(+ACK/-FIN)-&gt; TIME_WAIT … CLOSE\nCLOSE_WAIT -(+FIN)-&gt; LAST_ACK -&gt; CLOSE\n\n\n主动方 TIME_WAIT 等待 2MSL ❓\n\n保证对端正确断开\n保证本次连接所有报文都过期 不影响下次连接\n\n\n\n\n保活机制 (Linux SO_KEEPALIVE) 非标准 ❓\n\n\n流量控制 ❓\n\n侧重按照接收者的接收能力来发送数据\n发送 接收 滑动窗口\n\nSelective ACK 额外回复收到了哪些段数据\nDuplicate ACK 额外回复哪些段数据重复接收\nNagle 算法 接收窗口太小时 累计一定数据后(MSS)再发送\n\n-实时性变差\nTCP_NODELAY 关闭\n\n\n延迟 ACK 组合多个 ACK 为一个\n\n-实时性变差\nTCP_QUICKACK 关闭\n\n\n\n\n\n\n拥塞控制 ❓\n\n侧重避免网络出现负载过大的情况\n慢启动 指数增加\n拥塞避免 线性增加\n拥塞发生 超时重传 RTO 指数倍增 保险 回到起点\n快速恢复 快速重传 收到了 3ACK 没超时但是重传\nBBR 拥塞控制算法 ❓\n\n\n调优选项\n\nTCP Fast Open(TFO)\n\n第二次连接 通过 TFO cookie 省略一个 RTT\n\n\ntcp_tw_recycle\n\nTIME_WAIT 状态快速回收\n\n\ntcp_tw_reuse\n\n复用 TIME_WAIT 状态的连接\n依赖 tcp_timestamps 启用时间戳\n\n\n\n\n缺陷\n\n-队头阻塞: 中间丢包导致后面阻塞\n-不支持网络迁移\n\n\n网络攻击\n\nsyn flood\n\n服务器只收到 SYN -&gt; SYN_RCVD 半连接\n增大半连接队列\nsyn cookie\n\n\n\n\n应用问题\n\n对端断电 保持 ESTABLISHED 状态\n\n对端退出 FIN\n对端重启 RST\n\n\n大量 SYN_RCVD (等待对方第三次握手)\n\n缩短超时 (SYN Timeout) 时间\n增大半连接队列\nSYN cookie 技术\n\n自己不再分配内存记录 而是将信息保存到第二次握手回复中\n等第三次握手再获取这个信息\n\n\n\n\n大量 CLOSE_WAIT (等待我方 close())\n\n对端 shutdown 自己没有 shutdown\n\n\n大量 TIME_WAIT (先关闭方 等待第四次的 ack 彻底过期)\n\n确保应用层使用长连接 Connection: Keep-Alive\n\n设置长连接超时时间\n\n\n使用 tcp_tw_reuse 和 timestamp\n增大 tcp_max_tw_buckets 默认 18000\nSO_LINGER: 用 RST 代替 FIN, 不安全\n\n\n\n\n\n\nQUIC 协议 传输层\n\n基于 UDP 实现可靠传输 旨在取代 TCP\n+单连接传输多个流 避免队头阻塞\n+握手过程集成 TLS 节约连接设置的开销\n+包含连接 ID 支持底层网络切换\n\n\nUDP 协议 传输层\n\n无状态 面向报文 8 字节\n\n\nHTTP 协议\n\n0.9\n1.0\n1.1\n\nKeep-Alive 默认开启\n\nConnection: close 服务器回完 resp 就 close\n\n容易导致大量 TIME_WAIT\n\n\n\n\n流水线\n\n-HTTP 队头阻塞\n\n\n\n\n2.0 ❓\n\nhttp 请求头压缩编码\n二进制帧 请求头可以二进制\n引入流 可以单连接传输多个流\n服务器推送\n-TCP 队头阻塞\n\n\n3.0 ❓\n\n底层 TCP-&gt;QUIC\n\n\n报文格式\n\nGET / HTTP/1.1\\r\\nHost: xxx.com\\r\\n\\r\\nPAYLOAD\nHTTP/1.1 200 OK\\r\\nContent-Type: application/json\\r\\n\\r\\nBODY\n\n\n状态码\n\n101 Switch\n200 OK\n301 Moved Permanently\n302 Found\n400 Bad Request\n403 Forbidden\n404 Not Found\n500 Server Error\n502 Bad Gateway\n\n\n请求方法\n请求头\n缓存控制\n\n\nDNS 协议 应用层\n\nover UDP\n\nDOH DOT DOQ\n\n\n本地域名服务器\n\n递归解析\n\n\n根域名服务器\n\nDNS 劫持\nDNS 放大\n\n假装受害者发起大量 DNS 查询请求\n\n\nDNS flood\n\n\n\n\nSSL/TLS 协议 应用层和传输层之间/会话层\n\n建立流程 四次握手 ❓\n\nClientHello - ServerHello\nKeyExchange - Finished\n\n\n密钥协商: RSA(大质数分解) -&gt; DH(离散对数) -&gt; ECHDA\n对称加密\n证书机制\n中间人攻击\n客户端验证 颁发客户端证书 类似网银 ukey\n\n\nRPC\n\n一种调用方式\n没有 http 的条条框框 效率更高\n可以定制 例如使用 protobuf thrift\n一般基于 tcp 实现 也有基于 http2.0\n\n\nWebSocket\n\nHTTP 长连接\nHTTP 长轮询\n\n\n扫码登录\n\n#软件测试\n\n单元测试框架\n\nCppUnit\nGTest\n\nTEST\nTEST_F\nTEST_P\n\n\n\n\n压测\n\nTCP 压测 iperf3\nHTTP 压测 wrk\n\n\n\n#编译原理\n\n编译器过程\nC/C++编译过程\n\n#语言\n#Java ❓\n\n基本数据类型\n\nint vs Integer\n自动拆箱\n小整数优化\n\n\n抽象类 接口\n重载 重写\n字符串处理\nCollections ✅\n\nArrayList\n\nx1.5 扩容\n线程安全版 -&gt; Vector\n\n\nLinkedList\nHash{Map,Set}\n\nx2 扩容\nget() ❓\nput() ❓\n线程安全版 -&gt; HashTable\n线程安全版 2 -&gt; ConcurrentHashMap\n记录插入顺序 -&gt; LinkedHash{Map,Set}\n\n\nTree{Map,Set}\nStack 线程安全\nBlockingQueue 线程安全\n\n\nComparable\n面向对象特性\n\nObject 类\n\n析构 finalize()\n复制 clone()\n容器 hashCode() equals()\n\nequals() 默认实现比较引用是否相同\n\n\n反射 getClass()\n字符串 toString()\n同步相关 wait() notify() notifyAll()\n\nwait() 让持有此对象的监视器的线程等待\nnotify() 唤醒\n\n\n\n\n\n\n并发 ❓\n\nThreadLocal\nLock\nsynchronized\n\n修饰方法\n修饰代码块\n持有对象的锁\n\n\nvolatile\nJUC\n线程\n线程池\n\n创建线程开销大\n状态\n\nRUNNING\nSHUTDOWN\nSTOP\nTERMINATED\n\n\n参数调优\n\nCPU 密集: n+1\nIO 密集: 2n\n\n\n\n\n\n\n反射\nJVM ❓\n\n内存结构 ❓\n\n程序计数器 PC\nJava 栈: 和 C++的栈类似\n本地方法栈(C 栈): JNI 方法的栈\n堆 Heap\n方法区\n\n类信息\n静态变量\n常量\nJIT 编译后的代码\n\n\n直接内存(堆外内存): JNI 直接分配的内存\n\n\n堆 Heap 结构\n\n新生代\n\nEden\nSurvivor\n\n\n老年代\n永久代\n元空间\n\n\n类加载器\n\nBootstrap\nExtension\nApplication\n\n\nGC 算法\n\n引用计数\n\n-循环引用\n\n\n标记清除\n标记整理\nCMS\n\nMinor GC\nFull GC\n\n\nZGC\n\n\n\n\nJDBC\n\n#C/C++ ❓\n\nmalloc\n\nsbrk() mmap()\nptmalloc glibc\njemalloc Facebook\ntcmalloc Google\n\nthread cache\n\n\n\n\n编译\n\n工具\n\ncmake\nmeson\nmakefile/ninja\ngradle\n\n\n\n\n编译和链接\n\n变量位置\n\n局部变量 栈上\n全局 静态变量 零初始化 bss 否则 data\n\n\n\n\nSTL 容器\n\nvector\n\n扩容 VSx1.5 GCCx2\n\nx2 会导致重新分配\n\n\n\n\nvector/stack\nqueue/deque\nlist\nmap/set\nmultimap/multiset\nunordered_map/unordered_set\n\n\n引用\nOOP\n\n类\n\n构造 析构\n复制 移动\n类 vs 结构体\n\n类默认 private 结构体默认 public\n\n\n空类 1 字节\n\n\n封装\n\n保护级别\nfriend\n\n\n继承 &amp; 多态\n\n保护级别 区别\n关键字\n\nfinal 防止继承\noverride 关键字 明确重写\n\n\n虚函数\n\nClass() 不能是虚函数 否则无法调用\n~Class() 必须是虚函数\n纯虚函数: 没有实现(=0)的虚函数\n\n\n抽象类\n\n包含 纯虚函数 的类\n不可以被 new\n\n\n虚函数表\n\n每个(继承抽象类的)类一个 编译器分配\n每个对象有一个隐式的虚表指针成员\nMRO 中有几个独立的抽象类 就会有几个虚表指针\n\n\n虚继承 virtual 继承\n\n防止菱形继承\n使得虚函数表更加复杂 且内存布局变化\n\n\n对象切片问题: 派生类对象赋值给基类对象\n\n\n\n\nC++11\n\nmove() forward()\n\n\nC++20 协程\n四种 cast\n\nstatic_cast\nconst_cast\nreinterpret_cast\ndynamic_cast\nstd::reinterpret_pointer_cast\nstd::bit_cast\n\n\n异常\n\n构造函数抛异常\n析构函数抛异常\nnew 抛异常\n\n\n智能指针\n\nunique_ptr\n\n内部: 对象指针\n\n\nshared_ptr &amp; weak_ptr\n\n内部: 对象指针, &amp;use_count, &amp;weak_count, (可选)Mutex\n多个共享指针持有同一个控制块\nmake_shared 创建智能指针\n: public shared_from_this\n\n内部: 一个 weak_ptr(this)\n\n\n\n\nauto_ptr\n实现\n\n原子变量\n引用计数\n\n\n\n\n并发\n\n原子操作\n\ni++ 不是原子\n\n\nvolatile\n\n标记变量可能被未知的因素更改\n避免激进的优化 每次都去读内存\n\n\n\n\nmalloc\nSAFINE:\nCRTP: Curiously Recurring Template Pattern\n\nA 继承 SomeTemplate&lt;A&gt;\n\n\n\n#Golang\n\nGMP 模型\n\nG 协程\nM OS 线程\nP CPU 核\n调度: M:N 有栈 抢占式\n\n\n避免共享内存 使用通信\ndefer 语法\n\n#JavaScript ❓\n#Rust ❓\n\nOwnerShip\nLifetime\n泛型\ntrait\nTokio 异步\n\n异步任务\n\n\n比 c/c++的优势\n\n内存安全\n\n\nUnsafe\n\n有时候安全更重要\n\n\n\n#Linux\n\n文件\n\n一切皆文件\n类型\n\nRegular\nDirectory\nLink\nChar Dev\nBlock Dev\nSocket\nPipe\n\n\ninode\n\n内容\n\n大小\n权限\n所有者\n三个时间\n链接数\n\n\n\n\n打开文件: path-&gt;inode-&gt;block\n\n\n锁\n\n自旋锁 spinlock\n\n-饥饿\n\n\nRwlock\nseqlock\nRCU\nfutex ❓\n\n\nELF\n\n结构\n\nELF 头\nProgram HT 程序头\nSection HT 节数组\n\n\n静态链接\n动态链接\n\n链接\n\n加载 libc.so\n\n\n装载\n\ndl{open,sym,close}()\n\n\n\n\n\n\nPIE vs PIC\nIO 多路复用\n\nselect\npoll\nepoll\n\n\n零拷贝\n\n内核态 only 减少切换次数: sendfile, splice\n利用 DMA\nCOW\n用户态 IO: SPDK, DPDK\n\n\nCoredump\n链接与库\n\n静态链接\n动态链接\n装载\n\n\n运维命令\n\n负载\n\nuptime\nvmstat\ntop\natop\n\n\n网络\n\nnetstat\nss\nifstat\n\n\n内存\n\nsar\nfree\n\n\n\n\n缓存 ❓\n\nkswapd\n硬盘 Page Cache\nFile Cache\n\n\nI/O 模型\n\nReactor 对 IO 事件反应\n\n单线程\n多线程\n\n\nProactor\n\n\nUDS vs TCP\nconnect() 系统调用实现\n上下文切换\n\nX86 Gate\n\n\n硬件中断\n\n顶半部 底半部\n软件中断\n\nsoftirq\ntasklet\nworkqueue\n\n\n\n\nCPU 调度\n\n时机\n\n时钟中断 时间片用完\n系统调用\n\n\n\n\ntcpdump\n\neBPF\n\n嵌入内核的虚拟机\nHook 内核 API\n\n\n\n\n\n#MySQL ❓\n\nSQL 基本\n\n语句\nSQL 注入\n\nPreparedStatement\nWAF 过滤\n\n\n\n\n数据类型\n范式 ❓\n\n第一范式 列不可分\n第二范式 非主属性完全函数依赖于主键\n第三范式 非主属性既不部分依赖于主键也不传递依赖于主键\nBC 范式 不允许主键的一部分被另一部分或其它部分决定\n\n\n引擎\n\nMyISAM\n\n+插入, 查询性能\n-只有表锁\n-不支持外键\n-不支持事务\n适合非事务型负载 OLAP?\n\n\nInnoDB\n\n+行锁\n+聚簇索引\n+支持外键\n+支持事务\n\n\n底层存储\n\nMyISAM\n\n数据.myd 和索引.myi 分开存储\n数据按插入顺序存储\n索引 B+树叶子节点存数据行号 -&gt; 没有聚簇索引\n\n\nInnoDB\n\n数据和索引保存在一起\n主索引 B+树叶子节点存数据本身 -&gt; 聚簇索引\n需要一个主 key 没有会自己创建\n\n\n\n\n区别\n\n\n索引 ❓❓\n\n优缺点\n\n+加快速度\n-空间开销\n-维护索引开销\n\n\n索引类型\n\nhash 索引\n\n-范围查询\n-模糊查询\n-联合索引 最左匹配\n\n\nBtree 索引\nFulltext 索引\n\n+查询文本中的关键字\n模糊匹配\n\nlike ‘%abc%’ 场景\n\n\n\n\nRtree 索引\n\n+地理数据范围查询\n\n\n\n\n聚簇索引\n\n聚簇索引\n\n数据和索引放在一起存储 索引的叶子节点保存数据\n相对的 其他索引叫做二级索引 次级索引\n举例 InnoDB\n\n\n非聚簇索引\n\n数据和索引分开存储 索引的叶子节点保存数据行号\n举例 MyISAM\n\n\n\n\n覆盖索引\n\n查询结果都包含在索引中\n不需要回表\n\n\n最左匹配原则\n索引失效的场景\n\nlike ‘%abc’ ‘%abc%’\n使用了表达式/函数/UDF\n包含隐式类型转换\n非前缀匹配\nOR 单边索引\n\n\n索引使用原则\n\n经常查询的 where 条件\n长串索引需要限定前缀长度 看区分度\n\n\n\n\n优化\n\n查询慢\n\n缺少索引\n\n\n插入慢\n\n索引太多\n\n\nshow_query_log 定位慢查询\nexplain\n\n“Using index” = 使用了覆盖索引\n“type: ALL” = 全表扫描\n“key: xxxx” = 使用了 xxxx 索引\n\n\noptimize table 碎片整理\n\n\n事务 ✅\n\n概念: BEGIN 和 COMMIT 包裹起来的一组 SQL 语句\nACID ✅\n\n原子性: 事务要么全部执行 要么全部不执行\n一致性: 事务执行前后 数据库的完整性约束没有被破坏\n\n例如 A 转账给 B 两个账户总和不变\n\n\n隔离性: 事务之间互不干扰\n持久性: 事务提交后 数据库状态不会丢失 即使数据库崩溃\n\n\n事务隔离级别\n\nRU 读未提交: 一个事务可以读取另一个事务未提交的数据\n\n-脏读\n\n\nRC 读已提交: 一个事务只能读取另一个事务已提交的数据\n\n对应方案: 行锁\n-不可重复读: 在同一个事务中，多次读取同一数据返回的结果有所不同\n\n\nRR 可重复读: 一个事务在执行期间看到的数据是一致的\n\n对应方案: MVCC 机制\n\n事务开始创建一个 Read View\n递增的事务 id 作为版本号\n\n\n-幻读: 一个事务的写操作会导致另一个事务的查询结果不一致\n\ne.g. 并发预订座位\n解决 1: 物化冲突: 改 SQL 用 FOR UPDATE 显式锁定\n解决 2: 使用序列化隔离级别\n解决 3: 使用范围锁，例如 next-key lock\n\n\n-Write Skew: 更新不同数据破坏了一致性\n\ne.g. 至少有一个医生值班\n解决: 应用侧控制\n\n\n-Lost Update: 两个事务同时更新同一行\n\ne.g. 并发计数器+1\n\n\n\n\nSI 快照隔离\nS 序列化 (完美)\n\n-性能很差\n\n\n\n\nMySQL 如何保证 ACID\n\n保证原子性 A\n\nUndo log\n\n\n保证隔离性 I\n\n锁 + MVCC 机制\n锁\n\n记录锁(行锁) 锁定行 (3)\n间隙锁 锁定一个区间 (3,5)\n临界锁 (3,5]\n插入意向锁\n\n\n\n\n保证持久性 D\n\nWAL 先写日志\nRedo log (innodb)\n\n用于防止 Buffer Pool 因为异步落盘 数据丢失\n两个文件循环写 为了性能\n格式\n\n数据页位置+修改内容\n\n\n\n\nBinlog\n\nServer 层日志\n三种格式\n\nSTATEMENT SQL 语句\nROW 最终数据\nMIXED\n\n\n保存开机后全量日志\n用于备份恢复 主从同步\n\n\n\n\n保证一致性 C\n\n前三个加起来\n\n\n\n\n\n\n锁 ❓\n\n全局锁\n表锁\n\nMyISAM 默认\n\n\n行锁\n\nMyISAM 没有\nInnoDB 默认\n可能导致死锁\n\n\n页面锁\n乐观锁 &amp; 悲观锁\n\n乐观锁: 先更新再检查是否有其他事务修改\n\n举例 共享文档 Git\n\n\n悲观锁: 先加锁再更新\n\n\n\n\nMVCC 多版本并发控制\n\n提高并发性能\n当前读 读最新版本\n快照读\n\n\n主从同步\n\n目的 实现读写分离\n同步模式\n\n异步复制 (默认)\n全同步复制\n半同步复制\n\n\n配置\n\n主库 cnf 设置 server-id log-bin(&gt;=8 默认开启)\n创建同步账户\nSHOW MASTER STATUS;\n从库执行 SQL\n\nSTART SLAVE;\nSHOW SLAVE STATUS \\G;\nSTOP SLAVE SQL_THREAD;\n\n\n从库升级成主库\n\nRESET MASTER;\nCHANGE MASTER TO\nSTART SLAVE;\n\n\n\n\n主库在事务提交时 同步 binlog 到从库\n从库保存到 relay log 重放\n\n从库重放是 SQL Thread 单线程\n\n\n主从延迟\n\n避免大事务\n从库自身读压力太大 -&gt; 增加从库数量分散压力\n设置 slave_parallel_workers 并行复制线程数 slave-parallel-type 并行复制策略\nsemi-sync 半同步复制\n\n\n\n\n分库分表\n\n单表行数超 500 万行或者单表容量超过 2GB\n垂直分库 按业务类型\n垂直分表 按字段\n水平拆分\n\nID 取模\n\n\nMyCat\n\nMySQL Stub\n\n\nShardingSphere\n\nJDBC Stub\n\n\n分布式数据库\n\nTiDB (兼容 mysql, HTAP)\nGoogle Spanner\nCockroachDB (兼容 pg, OLTP)\nYugabyteDB (兼容 pg)\n\n\n\n\n\n#Redis ✅\n\n特点\n\n性能\n功能\n\n\n单线程\n数据类型\n\nstring\nlist\nhash\nset\nzset\n\n\n底层存储\n\nint\nsds\nquicklist\n压缩列表/listpack\nskiplist\n\nSkipList vs B+tree\n\n实现简单\n占用内存少\n插入删除性能更好，没有旋转平衡的开销\n\n\n\n\nhashtable\n\nMurmurHash2\n\n\n\n\n持久化\n\nRDB\n\n压缩\n\n\nAOF 增量日志\n\n会定期 rewrite\n\n\n混合\n\n\nPub/Sub 机制\nKey 过期机制\n\n惰性删除\n定期删除\n\n\n内存淘汰\n\nnoeviction\nvolatile-random\nvolatile-ttl\nvolatile-lru 默认\nvolatile-lfu\nallkeys-random\nallkeys-lru\nallkeys-lfu\nLFU 更好, LRU 容易受缓存污染影响\n\n\n实现事务\n\nRedis 事务\nLua 脚本\n\n\n发布订阅\nRedis 集群\n\n主从\n\n主库读写, 从库只读\n同步机制\n\n老版本 low 全量复制\n新版本 增量复制\n基于长连接的命令传播\n\n\n\n\n主从+哨兵 Sentinel\n\n哨兵: 一个集群 负责维护 Redis 集群的可用性\n\n监控集群是否正常 INFO 探活\n类似 Raft 协议\n自动故障转移\n提供配置\n通知客户端\n\n\n当&gt;=quorum 个哨兵认为主库下线 则开始换主\n\n选择数据最完整的从库作为主库\n\n\n哨兵脑裂问题\n\n主节点虚假故障 丢失数据\n可以用 min-slaves 缓解 但是不能彻底解决\n换用强一致性的 Zookeeper\n\n\n\n\nCluster 模式\n\n去中心化 每个节点存储不同的分片\n\n没有使用一致性 hash 而是使用 crc16(key)/16384 因为开销较大\n存在迁出导入开销\n\n\n可以组合主从模式\nGOSSIP 协议同步路由表和节点状态等信息\nMOVED 重定向用户请求\nASK\n\n\n\n\n使用问题\n\n大 key\n缓存击穿 热点 key 过期\n\n设置热点 key 永不过期\n热点 key 加后缀散列\n\n\n缓存雪崩 大量 key 同时失效\n\n随机过期时间或永不过期\n热点数据分散\n\n\n缓存穿透 热点 key 不存在\n\n设置 key-null\n\n\n\n\n缓存一致性 (分布式一致性问题)\n\n缓存策略\n\nCache Aside: 读者读 db 时顺便设置缓存\n\n-需要缓存设置过期时间\n-过期时间内可能是旧的值\n\n\nRead Through:\nWrite Through: 只写 db, 异步推送到缓存\nWrite Back: 只更新缓存, 异步写入 db\n\n\n同步机制\n\n更新派\n\n🔃 缓存, 🔃 db\n\n不好 缓存可能被另一个进程刷新为旧值\n\n\n🔃 db, 🔃 缓存\n\n不好 更新缓存需要计算 浪费系统资源\n\n\n\n\n删除派\n\n🆑️ 缓存, 🔃 db\n\n不好 缓存可能被另一个进程刷新为旧值\n延迟双删可以: 🆑️ 缓存, 🔃 db, 💤 延迟, 🆑️ 缓存\n\n\n🔃 db, 🆑️ 缓存\n\n好 第二步需要重试保证\n\n\n\n\n\n\n通用技巧\n\n过期时间\n终极方案 分布式锁\n失败重试\n\n单独线程\nMQ\n\n\nbinlog 订阅\n\n\n\n\n\n#Nginx &amp; OpenResty\n\n功能\n\n负载均衡\n\n轮询\n加权轮询\n随机\n源地址(用户 IP)哈希\n最小连接数\n插件\n\nfair (最快的)\nURL(访问目标)哈希\n\n\n\n\n反向代理\n\n\n实现\n\n主从 Reactor 模式\n\n\n\n#前端技术\n\nSession &amp; Cookie &amp; Token\n\nSession 服务端用于保存每个连接用户状态的对象\nCookie\n\n结构 name, domain, path, secure\n生命周期\n\n没设置过期时间 =&gt; 会话 Cookie 浏览器关闭即消失\n设置过期时间 保存在硬盘上\n\n\n\n\n\n\nLocalStorage\n\n生命周期 不会过期\n大小 一般 5M\n\n\nSessionStorage\n\n生命周期 浏览器关闭即消失\n\n\n浏览器加载资源的过程\n浏览器输入 URL 之后的过程\n\n#后端技术\n\n缓存 -&gt; Redis\nSpring\n\nAOP\nIOC\n循环依赖\n\nBean 互相依赖\n三级缓存\n\n一级缓存为单例池 singletonObjects\n二级缓存为早期曝光对象 earlySingletonObjects\n三级缓存为早期曝光对象工厂 singletonFactories\n\n\n\n\nFilter ❓\n\n\nMyBatis\n高并发下库存超卖问题 ❓\n\n设置字段为无符号 依赖异常\n使用悲观锁\nRedis 分布式锁\n\n事务 lua 脚本\n集群同步问题 RedLock\n\n\n\n\n限流\n\n固定窗口/滑动窗口\nQPS 限流\n\n+简单\n+可以改周期增大突发流量\n-周期边界不平滑\n-放过请求不均匀\n\n\n令牌桶\n\n+流量均匀\n-第一个周期不平滑\n\n\n漏桶\n\n-出水速度恒定 瞬时大流量会被丢弃\n\n\n\n\n\n#中间件\n\n缓存\n消息队列\n定时任务\n日志采集\n\n#消息队列\n\n作用 异步削峰解耦 日志 消息系统\n监控\n怎么保证消息不丢失\n\n消费者反馈消息收到\n数据持久化\n\n\n消息积压 ❓\n\n保存到硬盘上\n\n\n消息队列模型\n\n队列模型\n\n每个消费者一个队列\n-不适合广播场景 (需要复制)\n\n\n发布-订阅模型\n\n消息有主题 消费者订阅主题 (Topic)\n+支持广播场景\n示例 Kafka RocketMQ\n\n\n\n\nRabbitMQ ❓\nKafka\n\n流式数据处理平台 by Linkedin\n适合离线和在线消息消费\n组件\n\nBroker 中介人\n\nLeader 提供服务 与客户端交互\nFollower 不提供服务 只与 Leader 同步\n\n\nZooKeeper 元数据管理\n\n\n功能\n\n消息队列\n消息持久化\n消息流式处理\n\n\n消息分区 Partition &amp; 消费者组 Consumer Group\n\nTopic-Partition 一对多关系 目的是提高并行度\n相对地 Consumer Group 对等的消费者集群 消费一整个 topic\nPartition 会平均分布在所有 Broker 上\n底层实现: append-only log file\n\n\n性能高\n\nPage Cache 缓存\n磁盘顺序写\n零拷贝\nPull 拉模式\n\n\n消费模式\n\nPush 模式\n\n-容易挤爆消费者\n\n\nPull 模式\n\n-容易导致空轮询\n\n可以设置成阻塞收取\n\n\n\n\n\n\n确认机制\n\n0 无确认 完全异步\n1 leader 确认即可 (默认)\n-1 leader 和 follower 都确认\n\n\n主从同步 replica\n\n\n\n#分布式系统\n\nCAP 理论 ✅\n\nC 强一致性\nA 可用性\nP 分区可用性\nC+A:\nC+P: Raft\n\n\nBASE 理论 ✅\n\nBA 基本可用\nS 软状态\nE 最终一致性\n\n\n强一致性\n\n线性一致性 集群看起来和单个节点一样\n顺序一致性 弱一点\n\n\n分布式共识算法 CP\n\nPaxos ❓\n\n三种身份 Proposers Acceptors Learners\n-解决不了拜占庭问题\nMulti-Paxos\n\n乱序确认+乱序提交\n\n\n\n\nRaft\n\n关键点: 心跳同步 log, 新 log 覆盖旧 log, 选主过半数机制\n-解决不了拜占庭问题\nLog 结构\n\nTerm\nIndex\nCommand\n\n\nAppendEntries\n\nLeader -&gt; Follower\n包含 log\n用于同步 log\n\n\nRequestVote\n\nCandidate -&gt; Follower\n用于选主\n\n\nMulti-Raft\n\n一种工程架构模式，通过数据分片和并行 Raft Group 解决单组瓶颈，实现高性能与高扩展性\n实例: TiDB, CockroachDB, ByteKV\n\n\nParallel Raft\n\n阿里云 PolarFS 使用的 Raft 变种，借鉴 Multi-Paxos 的思想，允许 log 中存在空洞，提高性能\n\n\n\n\nZAB\n\n过半数 ACK 提交\n\n\n\n\n分布式锁 ❓\n\nRedis setnx\n\n需设置过期时间 防止死锁\n需设置独特 value 防止被别人释放\n锁提前释放 后台线程去续期锁 可以用 Redission\nRedis 集群同步可能丢锁 使用 Redlock\n\n\nZooKeeper 临时文件\n\n写强一致性 读不是(顺序一致性)\n\n读之前 sync()一下就是强一致\n\n\n+连接结束会自动释放\n-如果进程卡住 心跳不及时 会导致提前释放\n\n\nGoogle Chubby (Paxos)\n\n\n一致性哈希\n\n防止节点扩容导致未知变化\nHash 环\n虚拟节点\n\n\n负载均衡算法\n\nRR\nWeight RR\nRandom\nWeight Random\nHash\n\n\n分布式 ID\n\n雪花算法\n\n\n分布式事务\n\n两阶段提交 2PC ❓❓\n\n+原理简单，实现方便\n-同步阻塞 即所偶参与的事务逻辑均处于阻塞状态。\n-单点故障 协调者存在单点故障问题，如果协调者出现故障，参与者将一直处于锁定状态。\n-脑裂问题 在阶段二中，如果只有部分参与者接受并执行了 Commit 请求，会导致节点数据不一致。\n\n\n三阶段提交 3PC ❓❓\n\n+降低了阻塞范围 在等待超时后，协调者或参与者会中断事务。\n+避免单点故障 阶段 3 中协调者出现问题时，参与者会继续提交事务。\n-脑裂问题\n\n\nTCC\n\n\n系统\n\nZooKeeper\n\n分布式注册中心 按照目录层级划分 可以监听节点\nZAB 算法\n最好是奇数个节点\n\n\nEtcd\n\n强一致性\n\n\nConsul\n\n\n\n#大数据\n\n\n分布式文件系统\n\nGFS: The Google File System\n\n牺牲一致性 追求高吞吐量\n\n\nHDFS: The Hadoop Distributed File System\nTaobaoFS: 淘宝的分布式文件系统\n\n\n\nNoSQL 数据库\n\nGoogle BigTable\n\n分布式存储系统\n三个维度: 表名, 行键, 列键\n适合海量数据存储\n适合读多写少\n\n\nHBase\n\nOLTP 数据库 列式存储模型\n存储 PB 级别的海量数据 支持最多几万个稀疏列 支持快速随机查询\n追求数据一致性，CP 系统\nHBase Shell\n\n类似 redis-cli\n\n\n底层是 kv 存储, 相对于基于 RocksDB 的分布式存储系统，成本更高\n\n结构: RegionServer, HMaster, ZooKeeper\nRegionServer: 存储数据, 包含 Hlog 和若干个 HRegion\nHRegion\n\n存储一段连续的 Tuples, 包含多个 Store\n基于 range 策略分散数据 支持分裂\n\n\nStore(类似 Column Family): 包含一个 MemStore(类似 memtable) 和多个 StoreFile(类似 sstfile)\nHFile\n\n\n热 Key 问题\n\n加盐\n哈希 类似 redis crc16\n反转 类似手机号\n\n\n\n\nCassandra ❓\n\n\n\n数据仓库 &amp; 大数据分析\n\nGoogle Spanner ❓\nGoogle F1 ❓\nHive\n\nOLAP 数据仓库 大数据查询、处理和计算引擎\n使用 HiveQL 进行查询\n底层支持多种执行引擎 MapReduce Tez Spark\n\n\nSpark\n\n通用大数据处理引擎, 支持批处理、交互式查询、流处理、机器学习\nRDD\n\n\n\n\n\nOLAP\n\n\nKafka -&gt; 消息队列\n\n\nLucene\n\n\n海量数据处理问题\n\n离线 Top n\n在线 Top n\n通用技巧\n\n数据结构: 位图 平衡树 索引 Trie BloomFilter\n分治 合并\n\n\n\n\n\nMapReduce\n\n\n#微服务\n\nRPC\n\n原理\n框架\n\n\nNacos 注册中心\nDubbo 微服务框架\n\n#存储\n\n块存储\n\nCeph\n\n\n文件存储\n\nCeph\nHDFS\n\n\n对象存储\n\nCeph\nMinIO\n\n\nKV 存储\n\nRedis\nLevelDB\nRocksDB\n\n\nSSD\n\n优点\n\n读写速度快\n无噪音\n无碎片\n\n\n缺点\n\n寿命短\n价格高\n容量小\n\n\n\n\nLog-Structured Merge Tree\n\nvs B+Tree\n读放大: 读取的数据量和用户写入的数据量之间的比值\n写放大: 用户写入的数据量和实际写入磁盘的数据量之间的比值\nLevelDB\nRocksDB\n\n\n快照机制\n\nCOW\n\n-1 次写变 1 读 2 写 写放大 性能低\n\n\nROW (Redirect On Write)\n\n+不会有额外的写入\n-有数据碎片\n\n\n\n\n\n#数据库内核\n\nLSM-Tree ❓\nLevelDB ❓\nRocksDB\n\nWAL\nMemTable\n\nImmutable MemTable\n\n\nFlush\nCompaction\n\nLeveled Compaction\n\n10 倍\n+减少读放大\n-写放大\n\n\nUniversal Compaction\n\n相当于前者只有一个 level\n\n\nFIFO Compaction\n\n适合时间序列数据\n\n\nTiered Compaction\n\n+减少写放大\n-空间放大\n\n\n\n\n读路径\n\nMemTable\nImmutable MemTable\nL0 全部 + BloomFilter\nL1~Ln 二分查找 + BloomFilter\n\n\n写路径\n\nWAL\nMemTable\n\n\nBlockCache\n\n保存 Data, Index, Filter\n\n\n\n\nMySQL\nBuffer Pool\n\n文件系统缓存层\n\n\n执行模型\n\n火山模型/迭代器模型\nBatch 模型\n\n\n\n#搜索\n\n粗排\n精排\n召回\n算法\n\n#离散数学 &amp; 密码学\n\n格 理想\n哈希算法\n\nMD5 SHA1 SHA256\nMurmurHash3 Fnv1a\n\n\n对称加密\n\nDES 3DES AES\n\n\n非对称加密\n\nRSA\n\n-不支持前向保密\n大质因数分解\n\n\nDSA\n\n大数离散对数\n\n\n\n\n密钥交换\n\nDiffie-Hellman\nEC Diffie-Hellman\n\n\n\n#隐私计算\n\n方法\n\n差分隐私 DP\n\n将原始数据淹没在噪音中, 使得无法得到原始数据\n\n\n混淆电路 GC\n密钥分享 SS\n同态加密 HE\n\n原理 先加密再运算\n类型\n\n半同态加密 (Partially HE, PHE) 加密后只支持加法或乘法运算\n全同态加密 (Fully HE, FHE)\n\n\n方法\n\nPailier 算法\n复合剩余类问题 DCRA\n\n\n应用场景\n工具\n\nMicrosoft SEAL\n\n\n\n\n\n\n应用场景\n\n联合建模\n联合统计\n安全预测\n隐匿查询\n机构之间的联合计算\n\n\n安全两方计算 2PC\n\n工具\n\nCheetah 猎豹 阿里安全\n\n\n\n\n多方安全计算 MPC\n\n把多个参与方的数据放在一起计算出特定的结果 同时保证每一方信息的细节不被泄露 典型 百万富翁问题\n\n\n硬件设备\n\n多方安全计算 MPC\n同态加密 HE\n可信执行环境 TEE\n可信密态计算 TECC\n\n\n可信执行环境 TEE\n\n将处理器的一个区域与 CPU 的其余部分分开来使用基于硬件的安全计算模型\n\n隔离\n数据加密\n完整性校验\n远程认证\n\n\n+安全性\n-侧信道攻击\n实现\n\nIntel SGX\n\nLibOS\nGrammy\n\n\nARM TrustZone\n\n\nTEE OS\n\nOcclum 蚂蚁\n\n\n\n\n联邦学习 FL\n隐私计算框架\n\nTensorFlow Federated (TFF)\nKubeTEE 蚂蚁\n隐语 蚂蚁\nPySyft OpenMined\n\n\n\n#安全\n\nWeb 漏洞\n\nCSRF\nXSS\nSQL\nWAF\n\n\n安全机制\n\nSELinux\nAppArmor\nNX(Dep)\nSeccomp 过滤系统调用\nnamespace\n\nUser\nNet\nPID\nFS\n\n\n\n\n\n#云计算 &amp; 虚拟化\n\n云计算\n\nIaaS\nPaaS\nFaaS\nDBaaS\n\n\nQEMU\nKVM\nHypervisor\n体系结构\nvirtio\n\n模拟 PCI 设备 麻烦\nVRing\nKick\n\n\n\n#容器\n\nDocker\n\n底层\n\nnamespace\ncgroups\n\n\nContainerd\n\n\nK8S\n\n架构\n\nkubelet\napiserver\netcd (raft) 配置信息和服务发现\ncontroller\nscheduler\nruntime\n\ncontainerd\n\n\nkube-proxy\n\n\n概念\n\nPod\nNode\nContainer\n\n\nCRI\nCNI\nOCI\n\n\nDocker\nContainerd\nKata\n\n+资源管理\n+隔离彻底\n-比较重\n\n\ngVisor\n\n+轻量级 开销更小\n-不支持资源限制\n-仍然是同一个内核\n两种工作模式\n\nptrace + Seccomp\n\n退出场景\n\n执行 syscall\n收到 signal\n\n\n\n\n\n\n\n\n\n#音视频\n\n数字图像滤波\n图像特征\nRGB YUV\n视频编码\n视频格式\n视频帧\n\n\n#渲染引擎\n\nC++\nUnity Unreal 等引擎经验\n图形学 渲染 计算几何\n动画\n物理仿真\n\n#机器学习 &amp; 深度学习\n\nC++\nCNN\n\n卷积层\n\n\nRNN\nLSTM\nTransformer\n移动端部署\n\nTensorFlow Lite\nNCNN\nMNN\nPaddle-Lite\n技术点\n\nbig.LITTLE 调度\nOpenCL\n\n\n\n\n\n","categories":["misc","interview"],"tags":["面试"]},{"title":"近况","url":"/2024/04/12/job-directions/","content":"#未来方向\n未来需要确定一个方向去深入\n#DPU 云计算网络\n\n虚拟化\n\n#弹性计算 (ECS) 管控/调度/计算巢\n\nJava / Golang\n服务编排\n大数据技术\n\n#中间件研发 消息队列\n\nJava / Golang\n\n#安全容器\n\n#云存储\n\n块存储\n文件存储\n对象存储\n\n#数据库内核\n\nCMU15-445 数据库实现\nCMU15-721 内存数据库\n\n#深度学习框架\n\nCUDA 编程\n高性能计算\n算子\n编译原理\n\n#Java 后端开发\n\n6.824\nJava\nSpring 框架\nJVM 深入\n\n#后端基础设施\n\n6.824\nJava\nGolang\n\n#大数据\n\nHadoop\nSpark\nFlink\nKafka\nClickHouse\n\n#TODO\n\n迷你 OS 魔改 xv6 three easy pieces ✅\n类似 QEMU 的 ISA 模拟器 ✅\n迷你存储引擎 类似 LevelDB, RocksDB ✅\n迷你 c 编译器 ✅\n迷你 docker ✅\n迷你 S3 对象存储\n迷你消息队列\n迷你 JVM\n迷你 RPC 框架\n迷你 pytorch\n\n","categories":["misc","interview"],"tags":["面试"]},{"title":"软考高级系统架构设计师笔记","url":"/2025/03/14/note/","content":"#考试安排\n一般是周末\n上午 8:30-12:30 选择题 75 题 满分 75；\n上午 8:30-12:30 案例分析(简答题) 5 题 第一题必做，后面 4 选 2 满分 75；\n下午 14:30-16:30 论文题\n\n考试形式：机考，机器上有输入法。\n有草稿纸，需要自带水笔。\n#数据库\nE-R 图集成冲突\n:\n\n属性冲突: 包括属性域和属性取值的冲突。\n命名冲突: 包括同名异义和异名同义。\n结构冲突: 包括同一对象在不同应用中具有不同的抽象，以及同一实体在不同的局部 E-R 图中所包含的属性个数和属性排列次序不完全相同。\n\n关系数据库操作的对象和结果是集合\n三级模式\n\n外模式: 视图级别，用户与数据库系统的接口，描述局部数据的逻辑结构和特征\n模式: 表级别，描述全部数据的逻辑结构和特征\n内模式: 文件级别，存储记录的类型、存储域的表示、存储记录的物理顺序，指引元、索引和存储路径等数据的存储组织\n概念模式: 全体数据的逻辑结构和特征的描述，\n\n完整性约束\n\n实体完整性: 确保每个关系的主键唯一且不能为空，保证表中每条记录的唯一性。\n参照完整性: 外键必须指向另一个表中的主键或候选键\n用户定义完整性: 是指用户根据实际业务需求自定义的约束规则。\n\n范式:\n\n1NF: 没有重复的组+属性不可分: 只要是关系型数据库，就必须满足第一范式\n2NF: 非主属性完全依赖于码 / 没有部分依赖例如 { A, B, C, A -&gt; C, B -&gt; C }, 则不属于 2NF\n拆分成 { A, C, A -&gt; C } 和 { B, C, B -&gt; C } 即可\n3NF: 非主属性对码没有传递依赖。例如 { A, B, C, A -&gt; B, B -&gt; C }, 则不属于 3NF\nBCNF(Boyce-Codd Normal Form)是对第三范式的进一步强化。\n4NF: 关系模式中不存在多值依赖\n\nArmstrong 公理\n\n自反律: 若 Y⊆XY \\subseteq XY⊆X，则 X→YX \\to YX→Y。\n传递律: 若 X→YX \\to YX→Y，Y→ZY \\to ZY→Z，则 X→ZX \\to ZX→Z。\n增广律: 若 X→YX \\to YX→Y，则 XZ→YZXZ \\to YZXZ→YZ。\n分解规则: 若 X→YZX \\to YZX→YZ，则 X→YX \\to YX→Y 和 X→ZX \\to ZX→Z。\n合成规则: 若 X→YX \\to YX→Y 和 A→BA \\to BA→B，则 XA→YBXA \\to YBXA→YB。\n合并规则: 若 X→YX \\to YX→Y 和 X→ZX \\to ZX→Z，则 X→YZX \\to YZX→YZ。\n伪传递规则: 若 X→YX \\to YX→Y 和 WY→ZWY \\to ZWY→Z，则 WX→ZWX \\to ZWX→Z。\n拓展规则: 若 X→YX \\to YX→Y，则 X→XYX \\to XYX→XY。\n\n\n无损分解\n分解不会拆散函数依赖关系。判断方法: 如果 R1 R2 交集是 R1 或者 R2 的超码，则是无损分解。\n\n数据库设计:\n\n用户需求分析:\n概念设计:\n逻辑设计: 用某个具体的 DBMS 实现用户需要，将概念结构转换相应的数据模型，并根据用户处理要求、安全性考虑，在基本表的基础上建立必要的视图，并对数据模型进行优化。数据模型设计、E-R 图转换为关系模式、关系模式规范化、确定完整性约束、确定用户视图、反规范化设计\n物理设计:\n\n分布式数据库\n\n全局概念模式: 描述了全部数据的特性和逻辑结构，是全局数据的逻辑视图\n\n#嵌入式系统\n\nSoC\nSystem on Chip，即片上系统，是将整个计算机系统集成到一个芯片上，包括处理器、存储器、输入输出接口、控制器等。\nNDB\n基于网络的数据库(Netware Database，NDB)系统是基于手机 4G/5SG 的移动通信基础之上的数据库系统，在逻辑上可以把嵌入式设备看作远程服务器的一个客户端。实际上，嵌入式网络数据库是把功能强大的远程数据库映射到本地数据库，使嵌入式设备访问远程数据库就像访问本地数据库一样方便。\nAI 芯片的关键特点\n新型计算范式、训练和推断、大数据处理能力、数据精度、可重构能力、软件工具。\n\n#计算机体系结构\n中断分类\n\n访管中断: 系统调用\n信号中断通常是由外部设备或定时器发出的，与用户程序主动发起的系统调用无关。\n外部中断由硬件设备(如键盘、鼠标、定时器等)触发，不涉及用户程序的特权指令。\n\n#操作系统\n死锁:\n\n互斥条件\n请求和保持条件\n不可抢占条件\n循环等待条件\n\n预防死锁的方法:\n\n破坏请求和保持条件\n破坏不可抢占条件\n破坏循环等待条件\n\n\n最短移臂调度算法\n移臂就是找柱面(柱面一样找扇区)，旋转则找扇区，它们均按找最近原则调度。\n\n#计算机网络\nOSI 七层模型\n\n会话层没有安全服务\n\n以太网交换机\n\n初始 MAC 地址表为空\n如果没有相应的表项，采用 ARP 洪泛操作，即广播方式进行转发\n通过读取输入帧中的源地址添加相应的 MAC 地址表项\nMAC 地址表项是动态增长的\n\n以太网传输使用曼彻斯特编码，没有差分信号\n路由器工作在网络层，交换机工作在数据链路层。\nInternet 采用分组交换方式\nSDN 软件定义网络:\n\n应用层。对应用户不同的业务和应用。\n控制层。主要负责处理数据平面资源的编排，维护网络拓扑、状态信息等。\n转发层。负责用户数据的转发。\n\n应用层协议\n\nSMTP，邮件发送协议，缺省端口 25\nPOP3，邮件接收协议，缺省端口 110\nIMAP，交互式邮件存取协议，缺省端口 143\n\n#软件工程\n\nISO9000\n质量管理体系\nPMBOK\n项目管理知识体系\n\nCMMI 软件能力成熟度模型集成\n\n五个级别: 初始级，可管理级，已定义级，稳定级，持续优化级\n四个层次: 顶层方针、过程文件、规程文件、模板文件(Word/Excel 模版)。\n\n软件开发模型\n\n瀑布模型: 沟通/策划/建模/构建/部署\n增量模型: 降低了实现需求变更的成本，更容易反馈，更早使用\n原型模型\n螺旋模型: 目标设定/风险分析/开发和有效性验证/评审\n\n基于快速(原型)模型: 不能更早使用\n\n\n敏捷模型\n\n以快速(原型)模型为基础\n\n\n构件模型\n喷泉模型: 适合面向对象\nV 模型: 需求分析\\概要设计\\详细设计\\编码/单元测试/集成测试/系统测试/验收测试\nW 模型: 测试与开发是同步进行的\n\n软件生命周期模型\n\n需求分析: 确定可靠性目标、分析影响因素、制定验收标准、管理框架、文档规范、初步计划和数据收集规范\n\n工具: 基于自然语言或图形描述的工具和基于形式化需求定义语言的工具。\n\n\n概要设计: 确定度量、详细验收方案、设计、计划调整、数据收集、后续阶段计划和文档编制\n详细设计: 设计、预测、计划调整、数据收集、后续阶段计划和文档编制\n编码阶段: 测试、排错、计划调整、数据收集、后续阶段计划和文档编制\n测试阶段: 测试、排错、建模、评价、计划调整、数据收集、后续阶段计划和文档编制\n实施阶段: 包括测试、排错、数据收集、模型调整、评价和文档编制\n\n构件\n\n不可再分的软件单元 / 部署、版本控制和替换的基本单位 / 一组通常需要同时部署的原子构件\n具有唯一的标志 / 没有(外部的)可见状态，但可以利用容器管理自身对外的可见状态\n独立而成熟的构件: 数据库管理系统和操作系统\n有限制的构件: 基础类库\n适应性构件: ActiveX\n装配的构件: 软件商提供的大多数软件产品\n可修改的构件\n构件组装: 定制、集成和扩展\n\nCOP 面向构件的编程\n\n多态性、模块封装性、后期的绑定和装载、安全性\n\nCORBA 构件模型\n\n伺服对象 Servant\n对象适配器 POA\n对象请求代理\n\n中间件\n\n操作系统之上，管理计算资源和网络通信，实现应用之间的互操作\n连接和通信，屏蔽硬件、操作系统、网络和数据库的差异。\n负载均衡和高可用性、安全机制与管理功能，以及交易管理机制，保证交易的一致性。\n一组通用的服务去执行不同的功能，避免重复的工作和使应用之间可以协作。\n\n\n产品配置\n指一个产品在其生命周期各个阶段所产生的各种形式(机器可读或人工可读)和各种版本的文档、计算机程序、部件及数据的集合。\n\n软件过程/软件活动\n\n制作软件产品的一组活动以及结果\n活动包括: 软件描述、软件开发、软件有效性验证、软件进/演化 (没有分析、没有测试)\n\n软件工具\n\n软件开发工具: 需求分析工具、设计工具、编码与排错工具\n软件维护工具: 版本控制工具、文档分析工具、开发信息库工具、逆向工程工具、再工程工具\n软件管理和支持工具: 项目管理工具、配置管理工具、软件评价工具\n\n逆向工程: 重构, 设计恢复, 再工程\n逆向工程 4 个抽象层次: 实现级、结构级(相互依赖关系)、功能级(功能及程序段之间关系)和领域级(程序分量与概念之间关系)\n\n需求管理\n包括变更控制、版本控制、需求跟踪。不包括需求获取的部分\n需求抽取\n迭代过程 1. 需求发现 2.需求分类和组织 3.需求优先级划分和协商 4. 需求规格说明\n需求跟踪\n编制每个需求与系统元素之间的联系文档，这些元素包括其它需求、体系结构、设计部件、源代码模块、测试、帮助文件和文档等。\n需求变更\n问题分析和变更描述、变更分析和成本计算、变更实现\n\n软件需求开发的最终文档经过评审批准后，则定义了开发工作的需求基线(baseline)。这个基线在客户和开发者之间构筑了计划产品功能需求和非功能需求的一个约定(agreement)。需求约定是需求开发和需求管理之间的桥梁。\n结构化分析\n\n功能建模: DFD 数据流图\n\n描述数据的流向和处理\n\n\n行为建模: 状态转换图\n数据建模: ER 图\n\n软件设计\n\n数据设计: 改善程序结构和模块划分，降低过程复杂性\n结构设计: 主要部件之间的关系\n人机界面设计\n过程设计: 系统结构部件转换成软件的过程描述\n\n结构化设计\n\n四个任务: 体系结构设计、接口设计、数据设计和过程设计\n结构化设计工具\n\n需求分析: 数据流图\n概要设计: 模块结构图、层次图和 HIPO 图，软件需求转化为数据结构和系统结构\n详细设计: 程序流程图、伪代码和盒图，细化得出软件的详细数据结构和算法\n\n\n\n敏捷开发\n\n以人为核心、迭代、循序渐进的开发方法。\n主打适应型，而非可预测型\n对比: 结构化开发方法是面向过程的。\n以原型开发思想为基础\n\n用户界面不应该经常修改\nRUP 统一软件开发过程\n\n用例驱动、以体系结构为中心、迭代式开发\n迭代式开发\n\n初始阶段\n细化阶段\n构建阶段\n交付阶段: 测试、发布\n\n\n9 个核心工作流: 业务建模、需求、分析与设计、实现、测试、部署、配置与变更管理、项目管理、环境 (软件开发时不考虑成本)\n4+1 视图模型:\n\n核心: 场景\n逻辑/实现视图: 最终用户功能特性\n进程/过程视图: 设计的并发和同步特证，集成人员\n开发视图: 在开发环境中软件的静态组织结构，开发人员\n物理视图: 系统工程师\n\n\n\nUML 模型\n\n用例图、类图、时序/顺序图、部署图、组件图、构件图\n对象图: 对象和关系\n活动图: 控制流和数据流\n状态图: 接口、类、协作行为\n\n\nSysML\n多了一个需求图\n耦合\n内容耦合 &gt; … &gt; 数据耦合\n内聚\n功能内聚 &gt; …\n\n用例图\n\n用例\n\n扩展关系(Extend)\n包含关系(Include): A 包含 B 行为\n泛化关系(Generalization): 一般和特殊，“会员注册”和“电话注册”、“邮件注册”\n\n\n参与者\n\n继承关系\n\n\n\n软件复杂度\n\n代码行方法\nHelstead 方法\nMcCabe 方法: 环形复杂度 = 闭环个数 + 1\n\n软件构件:\n\n独立部署单元，不可拆分\n作为第三方的组装单元\n没有（外部的）可见状态，但可以利用容器管理自身对外的可见状态\n构件可以在适当的环境中被复合使用，因此构件需要提供清楚的接口规范，可以与环境交互。\n在任何环境中，最多仅有特定构件的一份副本。\n可部署: 二进制形式，无需编译，可配置\n\n构件分类: 关键字分类法、刻面分类法和超文本组织方法\n构件组装: 基于功能的、基于数据的和面向对象的\n\n业务流分析\n通过对系统进行长期监听，利用统计分析方法对诸如通信频度、通信的信息流向、通信总量的变化等参数进行研究，从而发现有价值的信息和规律。\n\nABSDM 基于架构的软件设计模型\n\n过程: 需求、设计、文档化、复审、实现、演化\n由商业、质量和功能需求的组合驱动\n采用视角与视图的概念描述软件架构\n自顶向下，递归细化: 自顶向下更快得到系统演示原型\n直到能产生软件构件和类\n基础: 功能分解、选择架构风格、软件模版的使用\n体系结构需求来自: 系统的质量目标、系统的商业目标，系统开发人员的商业目标\n文档化: 从使用者角度，体系结构说明+质量说明书\n架构演化 6 个步骤: 需求变化归类；制订体系结构演化计划；构件变动；更新构件的相互作用；构件组装与测试；技术评审；演化后的体系结构。\n\n体系结构视角 perspective\n\n静态视角: 判断质量特性\n动态视角: 判断系统行为特性\n逻辑视图: 设计元素的功能和概念接口\n进程视图\n实现视图\n配置视图\n\n架构风格: 包含定义、词汇表和约束\n\n软件系统的结构、行为和属性的高级抽象\n批处理架构: 强调顺序执行\n管道-过滤器架构: 过滤器之间可以并行处理数据\n仓库架构: 中央数据结构 + 独立构件\n\n黑板架构: 一种利用模块化和分散式框架解决问题的方法，每个专家贡献一部分解决方案。属于数据共享型架构风格。\n\n\n进程通信架构: 通过消息传递进行通信，具体的实现方式可以包括消息队列、管道、共享内存等。\n事件驱动/隐式调用架构: 系统的组件通过事件进行交互，独立性，非耦合性，例如推荐系统\n分层/层次型架构: 物联网，嵌入式；问题: 级联修改、性能、层层依赖\n递归架构: 嵌入式\n主程序/子过程架构\n面向对象架构\n虚拟机架构: 自定义流程\n\n基于规则的架构: 规则集、规则解释器、规则/数据选择器和工作内存\n解释器架构: 更加灵活\n\n\nC2架构: 构件和连接件，底部连接到另一个的顶部\n\nDSSA 特定领域软件架构:\n\n四种角色: 领域专家、领域分析师、领域设计人员和领域实现人员\n基本活动: 领域分析、领域设计和领域实现\n\n分析阶段: 分析特定应用领域，获取领域模型，从而为后续的设计和实现提供基础\n设计阶段: 获得DSSA 特定领域软件架构\n实现阶段: 开发和组织可重用信息，并对基础软件架构进行实现\n\n\n建立过程: 强调并发、递归和反复迭代\n定义领域特定元素阶段: 重点是确定领域的通用功能和特定需求，而不是直接定义用户需求\n定义领域模型和体系结构阶段: 目标是抽象出领域内通用的模型和体系结构\n领域开发环境(领域架构师)、领域特定应用开发环境(应用工程师)和应用执行环境(操作员)\n\n\n垂直域: 定义了一个特定的系统族，包含整个系统族内的多个系统，结果是在该领域中可作为系统的可行解决方案的一个通用软件体系结构。\n水平域: 定义了在多个系统和多个系统族中功能区城的共有部分。在子系统级上涵盖多个系统族的特定部分功能。\n\nATAM 架构权衡分析方法\n\n过程: 场景和需求收集、架构视图和场景实现、属性模型构造和分析、架构决策与折中\n场景是从风险承担者的角度对与系统的交互的简短描述\n\n头脑风暴的三种场景: 用例场景、增长场景、探索性场景\n\n\n质量属性: 可用性、易用性、性能、可修改性、可测试性、安全性、可靠性、可维护性\n\n刻画手段: 刺激源、刺激、环境、制品、响应、响应度量\n一般采用刺激、环境和响应三方面来对场景进行描述\n\n\n开发之前先进行评价和折中\n核心概念: 属性\n关注点: 需求说明\n效用树: 树根 – 质量属性 – 属性分类 – 质量属性场景(叶子节点)\n敏感点: 一个或多个构件(和／或构件之间的关系)的特性\n权衡点: 影响多个质量属性的特性，是多个质量属性的敏感点\n改变加密级别的设计决策属于权衡点\n风险点: 泛指可能引起风险的因素\n\nSAAM 基于场景的架构分析方法\n\n过程: 场景开发、架构描述、单场景评估、场景交互评估和总体评估。\n针对最终架构进行评估\n主要输入: 问题描述、需求声明和体系结构描述\n\nSAEM 从外部和内部质量属性两个角度进行评估，创建了一个基础框架，用于规约建模、创建度量准则和评估质量属性。\nSAABNet 贝叶斯信念网络\nSACMM 软件架构修改度量方法\nSASAM 软件架构静态分析方法\nALRRA 软件架构可靠性风险评估方法\nAHP 层次分析法\nADL\n软件维护\n\n排错性维护、适应性维护、**完善性维护(新功能)**和预防性维护\n\nN 版本设计\n\n相异成分规范评审\n相异性确认\n背对背测试\n\n软件重用\n\n需求分析文档、设计过程、设计文档、程序代码、测试用例和领域知识等\n横向重用: 不同应用领域中的软件元素，例如数据结构、分类算法和人机界面等\n纵向重用: 在相同或相似的应用领域中对特定功能或模块的重用\nCOM 对象重用: 包含(Contain)和聚集(Cluster)\n\nEJB、COM+ 较为适用于应用服务器。\nWSDL\n\n服务做什么 What\n如何使用/访问服务 How\n服务位于何处 Where\n\nSOA 面向服务系统\n\nUDDI: Web 服务注册和查找的标准\nWSDL: Web 服务描述语言\nSOAP: 基于 XML, 信封和 XML 编码定义在相同命名空间\nBPEL: Web 服务定义和执行业务流程的语言, 用来将分散的、功能单一的 Web 服务组织成一个复杂的有机应用。\n服务注册表模式\n企业服务总线 ESB 模式\n\n\nUDDI\n描述、发现和集成Web 服务\n\n六种业务流程建模方法\n\n流程图、角色活动图和角色交互图、IDEF0 和 IDEF3、Petri-Net(从流程的角度出发)、UML 活动图和 BPMN\n\n\n软件工程过程\nPDCA, 计划 执行 检查 处理\n\nDO-178: 目标、过程、数据\n专家系统的学习机制主要依赖其核心组成部分: 知识库和推理机。\n企业集成\n\n应用集成: 高层\n服务集成: 中\n会聚集成: 中层\n数据集成: 底层\n\n软件复用\n\n主要阶段: 分析，构造/获取，管理，使用\n机会复用: 开发过程中，只要发现有可复用的资产，就对其进行复用\n系统复用: 开发之前，就要进行规划，以决定哪些需要复用。\n\n构件组装的常见方式: 层次、叠加和顺序等\n构件管理: 构件描述，构件分类，构件库组织，人员及权限管理，用户意见反馈等\n操作不完备: 一个构件的提供接口是另一个构件请求接口的一个子集\n面向对象设计\n\n实体类\n控制类: 控制\n边界类: 界面，接口，隔离\n分析模型: 顶层架构图、用例与用例图、领域概念模型\n设计模型: 包图(描述架构)、交互图、类图、状态图、活动图\n\n面向对象设计原则\n\n单一职责原则、开放-封闭原则、李氏替换原则、依赖倒置原则、接口隔离原则和组合重用原则\n李氏替换原则: 子类可以替换父类\n依赖倒置原则: 细节依赖于抽象，抽象不依赖于细节\n\n面向对象设计模式\n\n创建型: 单例模式、工厂方法模式、抽象工厂模式、建造者模式、原型模式\n\n抽象工厂模式: 为创建一系列相关或相互依赖的对象提供了一个接口\n建造者模式: 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示\n原型模式: 不了解要创建对象的确切类以及如何创建等细节的情况下创建自定义对象\n\n\n结构型: 适配器模式、桥接模式、组合模式、装饰模式、外观模式、享元模式、代理模式\n\n桥接模式: 将抽象部分与它的实现部分分离，使它们可以独立地变化；画图软件: 图形=抽象部分，画图方法=实现部分\n装饰模式: 比直接继承更加灵活\n\n\n行为型: 责任链模式、命令模式、解释器模式、迭代器模式、中介者模式、备忘录模式、观察者模式、状态模式、策略模式、模板方法模式、访问者模式、备忘录模式(Memento)\n另一种划分方法: 类设计模式 vs 对象设计模式\n\n\nDCMM 数据管理能力成熟度评估模型\n数据战略、数据治理、数据架构、数据应用、数据安全、数据质量、数据标准和数据生存周期 (没有数据维护)\n\n失配问题\n\n构件失配\n连接子失配\n\n遗留系统的演化策略\n\n技术含量较高、业务价值较高 -&gt; 改造\n技术含量较高、业务价值较低 -&gt; 集成，利于后续维护\n技术含量较低、业务价值较高 -&gt; 继承，保留\n技术含量较低、业务价值较低 -&gt; 淘汰\n\n信息隐蔽: 提高可修改性、可测试性和可移植性\n三层 C/S 架构\n\n表示层: GUI\n功能层: 应用本体，业务逻辑\n\n增加了一个应用服务器\n\n\n数据层: 数据库管理系统\n\nMVC 模式\nEAI 企业应用集成:\n\n服务层次低到高: 通讯服务、信息传递与转化服务、应用连接服务和流程控制服务\n原则: 应用程序独立性、面向商业流程、独立于技术和平台无关\n数据复制，基于接口的数据集成，数据联邦\n\n#推荐系统\n基于内容的推荐系统:\n\n能推荐新的或不流行的项目\n能为具有特殊兴趣的用户进行推荐(个性化)\n不需要其他用户的数据\n无法解决冷启动问题\n\n协同过滤\n\n需要其他用户的数据\n\n#法律\n\n信息化需求\n包含三个层次，即战略需求、运作需求和技术需求。\n\n\n战略需求。组织信息化的目标是提升组织的竞争能力，为组织的可持续发展提供一个支持环境。\n运作需求。组织信息化的运作需求是组织信息化需求非常重要且关键的一环，它包含三方面的内容: 一是实现信息化战略目标的需要；二是运作策略的需要，三是人才培养的需要。\n技术需求。由于系统开发时间过长等问题在信息技术层面上对系统的完善、升级、集成和整合提出了需求。\n\n\n《软件产品管理办法》\n任何单位和个人不得开发、生产、销售、进出口含有以下内容的软件产品:\n\n\n侵犯他人知识产权的；\n含有计算机病毒的；\n可能危害计算机系统安全的；\n含有国家规定禁止传播的内容的；\n不符合我国软件标准规范的。\n\n外观设计专利的相似外观设计不得超过 10 项。\n著作权:\n\n一般作者: 具有永久署名、修改、保护完整 / 50 年内的发表、使用、报酬权利。\n软件作者: 具有永久署名、修改 / 50 年内的发表、复制、发行…权利。\n商标: 自注册过的, 10 年, 冲突: 1 先来后到 2 先使用 3 协商或抽签\n发明专利: 20 年, 实施许可: 独占、排他实施、普通\n实用新型专利: 10 年\n艺术作品: 著作权属于作者, 所有权和展览权属于拥有者\n软件著作权: 产生自软件开发完成之日起, 在公司完成属于公司, 开发软件所用的思想、处理过程、操作方法或者数学概念不受保护, 不知情情况下使用者不构成侵权, 提供者承担责任\n法律、法规等不适用于著作权法保护\n改编、翻译、注释、整理已有作品而产生的作品，其著作权由改编、翻译、注释、整理人享有，但行使著作权时不得侵犯原作品的著作权。\n受委托创作的作品，著作权的归属由委托人和受托人通过合同约定。合同未作明确约定或者没有订立合同的，著作权属于受托人(即实际开发的人)。\n\n职务发明创造: 离职后1 年内完成的，与其在原单位的工作或任务相关的发明创造。\n作者的署名权、修改权、保护作品完整权的保护期不受限制\n#信息安全\n\n数字证书\n用户登录过程中可通过验证CA 发出的签名确认该数字证书的有效性\n安全审计四要素\n控制目标、安全漏洞、控制措施和控制测试\n\n数据分级分类\n\n基础安全层: 数据分级分类、数据备份、数据加密等基本安全措施\n权限控制层: 负责数据的访问权限管理\n战略安全层应用接口层: 高层安全策略或接口相关管理\n\n《计算机信息系统安全保护等级划分准则》:\n\n第 1 级: 用户自主保护级\n第 2 级: 系统审计保护级\n第 3 级: 安全标记保护级\n第 4 级: 结构化保护级\n第 5 级: 访问验证保护级\n\nTBAC 基于任务的访问控制\n\n组成要素包括工作流、授权结构体、受托人集和许可集\n\n数据资产的特性: 可控制，可量化，可变现，虚拟性、共享性、时效性、安全性、交换性和规模性\n信息安全系统的措施: 技术方面的安全措施、管理方面的安全措施、政策法律方面的安全措施\n#项目管理\n\n三点估算法\n期望时间 = (最短时间 + 4 * 最可能时间 + 最长时间) / 6\n\n盈亏平衡点\n\n可变成本: 需要换算成单位产品的成本\n税率: 单位产品的税率\n\n#软件测试\n静态测试: 通过对软件的需求规格说明书、设计说明书以及源程序做结构分析和流程图分析，从而找出错误。\n灰盒测试: 除了重视输出相对于输入的正确性，也看重其内部的程序逻辑。\n系统测试:\n\n依据需求规格说明书进行测试\n为了发现需求分析环节的错误\n\n软件备份\n\n静态备份\n动态备份\n\n可靠性: 是指产品在规定的条件下和规定的时间内完成规定功能的能力。\nMTBF、MTTD、MTTR、MTBR\n\nMTBF(Mean Time Between Failures): 平均故障间隔时间，是系统在两个故障之间的平均正常运行时间。\nMTTF(Mean Time To Failure): 平均故障时间，是系统正常运行到发生故障的平均时间。\nMTTD(Mean Time To Detect): 平均故障检测时间，是检测到故障所需的平均时间。\nMTTR(Mean Time To Repair): 平均修复时间，是修复故障所需的平均时间。\nMTBR(Mean Time Between Repairs): 平均修复间隔时间，是修复后到再次发生故障的平均时间。\n\n脚本结构\n\n线性脚本: 线性脚本是录制手工测试的测试用例时得到的脚本，这些脚本是未做修改的。\n结构化脚本: 结构化脚本具有各种逻辑结构，包括选择型结构、分支结构、循环迭代结构，而且具有函数调用功能。结构化脚本具有很好的可用性和灵活性，易于维护。\n共享脚本: 共享脚本是指一个脚本可以被多个测试用例使用，即脚本语言允许一个脚本调用另一个脚本。\n数据驱动脚本: 数据驱动脚本是指将测试输入存储在独立的数据文件中，而不是脚本中。这样，脚本可以针对不同的数据输入实现多个测试用例。\n关键字驱动脚本: 关键字驱动脚本是数据驱动脚本的逻辑扩展，它用测试文件描述测试用例，它说明测试用例做什么，而不是如何做。关键字驱动脚本允许使用描述性的方法，只需要提供测试用例的描述，即可生成测试用例。\n\n#案例分析\n第一题必做，后面 4 选 2\n#题型\n选词填空型、简答填空型、概念对比型、概念简述型、解决方案简述型\nDFD 数据流图\n\n父图 ( 上层数据流图 ) 与 子图 ( 下层数据流图 ) 平衡（也叫作分层细化过程平衡）。个数一致：两层数据流图中的数据流个数一致。方向一致：两层数据流图中的数据流方向一致。\n黑洞 加工只有输入没有输出（只进不出）\n奇迹 加工只有输出没有输入（只出不进）\n灰洞 加工不出输出流\n数据存储 一般来说这个点可以不提。正常情况下必须既有读的数据流，又有写的数据流；在某张子图中，可能只有读没有写，或者只有写没有读。\n\nSTD 状态(转换)图\n\n（1）状态、初态、终态。\n（2）事件、转移、动作。\n（3）并发状态机。状态机图的推荐使用场合：包括类设计场合\n\n用例图\n\n用例之间的关系: 包含、扩展和泛化\n\n类图\n\n类之间的关系: 关联–、依赖-|+&gt;、泛化-|&gt;、聚合-&lt;&gt;、组合-&lt;+&gt;和实现-.-|&gt;\n\n关联–: 一般的关联关系\n聚合-&lt;&gt;和组合-&lt;+&gt;的区别: 组合关系的部分不能脱离整体而存在，聚合关系没有生命周期约束\n依赖-|+&gt;，泛化-|&gt;，实现-.-|&gt;的区别: 依赖关系是 A 依赖 B，泛化关系是 A 是 B 的子类，实现关系是 A 实现 B 的接口\n\n\n\n顺序图/时序图\n\n对象 生命线\n交互片段\n执行发生\n同步、异步、返回消息\n\n通信图/协作图\n\n对象 生命线\n连接\n同步、异步、返回消息\n\n时序图 vs 通信图: 顺序图以时间轴为主线，通过垂直生命线和消息箭头清晰展示消息传递的时序关系，适合体现复杂的操作流程、并发或者异步操作；而通信图则以对象间的结构关系为中心，通过节点和链接呈现交互路径，强调对象协作的拓扑结构，适合分析静态关系紧密、消息路径简单的场景。两者语义等价，可以相互转换。实际应用中需要根据具体需求和复杂度选择合适的图示方式，关注时序细节用顺序图，强调结构关系用通信图，也可以结合使用以全面描述交互逻辑。\n活动图\n\n活动 对象 分区\n起点 终点 决策点 分叉/合并\n控制流 对象流\n\n类识别\n\n实体类 边界类 控制类\n\n面向对象设计原则\n\nSOLID\n\nS: 单一职责原则\nO: 开放-封闭原则\nL: 里氏替换原则\nI: 接口隔离原则\nD: 依赖倒置原则\n\n\n\n开闭原则: 面向对象设计的开闭原则（Open Closed Principle, OCP）指的是一个软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。也就是说，在设计一个模块时，应该允许在不修改原有代码的情况下，通过增加新代码来扩展其功能或者行为。\n单一职责原则: 单一职责原则（Single Responsibility Principle, SRP）是指一个类应该只有一个设计原因，即一个类应该只有一个职责。\n里氏替换原则: 里氏替换原则（Liskov Substitution Principle, LSP）是指子类对象应该能够替换父类对象，并且程序的行为不会发生变化。\n表现层层次结构\n\nMVC 模式\n\n视图 -&gt; 控制器 -&gt; 模型 -&gt; 视图更新\n\n\nMVP 模式\n\n视图 &lt;-&gt; 呈现器 &lt;-&gt; 模型\n\n\nMVVC 模式\n\n视图 &lt;-&gt; 视图模型 &lt;-&gt; 模型\n\n\n\n中间层架构\n数据层架构\n\n在线访问模式: 应用程序直接操作数据库\nDAO: 通过接口封装数据库操作\nDTO: 用于跨进程/跨网络传输数据的对象\nORM: 对象关系映射\n\n数据库程序采用在线访问方式的优点包括性能较高、能够处理复杂查询；缺点是要求开发人员具备 SQL 编程能力，且代码修改维护相对困难。\nORM 的优点是简化了数据库操作，降低了学习和开发成本，无需编写 SQL 语句，减少代码量，降低 SQL 质量带来的影响；缺点是性能不如在线访问方式，且处理复杂查询困难。选择 ORM 的主要考虑因素是团队的技术水平和项目的复杂性。\n增加数据访问层的原因是为了实现数据和业务逻辑的分离，提高系统的可维护性和可扩展性。数据访问层的主要功能是提供对数据源的访问和操作，封装数据访问的细节，使得业务逻辑层不需要关心数据存储的具体实现。简化开发流程，方便后续的维护和扩展，提升系统的可重用性和可测试性。\n物联网\n\n感知层 网络层 应用层\n\n云原生架构原则\n\n弹性原则 系统自动伸缩，适配业务需求\n韧性原则 抵御软硬件异常，保障系统稳定\n\n云原生架构模式\n\n服务化架构模式: 以应用模块划分软件，通过接口契约、标准协议定义业务关系，结合 DDD、TDD、容器化部署提升代码质量与迭代速度，典型模式为微服务、小服务\nMesh 化架构模式: 将中间件框架（RPC、缓存等）从业务进程分离，业务进程仅保留薄 Client，中间件逻辑由 Mesh 进程完成\nServerless 模式: 开发者无需关心应用运行环境，云自动处理应用启动、调度、关闭，业务流量/ 事件触发处理\n存储计算分离模式: 分布式环境中，将暂态数据、持久数据采用云服务保存，有状态场景通过时间日志 + 快照恢复服务\n分布式事务模式: 针对多数据源场景，提供多种事务模式：XA 模式（强一致，性能差）；基于消息最终一致性（高性能，通用性有限）；TCC 模式（应用层控制，侵入性强）；SAGA 模式（补偿事务，开发维护成本高）；SEATA AT 模式（高性能、无代码工作量，有场景限制）\n可观测架构: 通过 Logging（多级别日志）、Tracing（请求链路跟踪）、Metrics（系统量化度量）实现可观测性，定义组件 SLO（并发度、耗时等）\n事件驱动架构: 基于事件集成应用 / 组件，事件含 schema 可校验，具备 QoS 保障与失败响应机制，用于服务解耦、数据变化通知、事件流处理等\n\n云原生架构反模式\n\n庞大的单体应用\n单体应用“硬拆”为微服务\n缺乏自动化能力的微服务\n\n容器技术\n容器技术是一种轻量级的隔离技术，允许多个应用在同一操作系统内核上运行，每个容器运行一个或者一组应用，具有独立的文件系统、运行环境等，实现应用的快速部署和隔离。虚拟机技术通过虚拟化技术在物理服务器上模拟出多个独立的虚拟计算机，每个虚拟机有自己的硬件资源、操作系统和应用程序，实现完全的隔离和资源分配。容器技术相较于虚拟机技术更加轻量级，资源占用更少，启动速度更快，能更加高效地利用硬件资源，满足系统升级中对于资源利用优化的需求。且在当前技术发展下，安全问题可通过安全策略、容器隔离等方式进行防护，在满足业务需求的同时，能以更低的成本和更高的效率实现资源的利用。契合企业系统升级的初衷。\n容器编排技术\n\nSOA 面向服务架构\n面向服务架构（SOA）是一种软件架构风格，强调将应用程序的不同功能单元划分为松耦合的服务，并将这些服务通过标准化的接口和契约联系起来。接口是采用中立的方式进行定义的，而且独立于实现服务的硬件平台、操作系统和编程语言。者使得构建和集成复杂的分布式系统变得更加灵活和高效。\n\n服务注册模式\n\n服务提供者 服务注册中心 服务请求者\nUDDI 注册中心\nWSDL 服务定义语言\nSOAP 简单对象访问协议 基于 XML 用于通信\n\nREST 风格\n\n资源 URI\n统一资源接口\n资源表述\n资源链接\n状态转移\n\n\nESB 企业服务总线\n企业服务总线（ESB）提供位置透明的消息路由和寻址服务、服务注册和命名管理功能，支持多种消息传递模式（点对点、发布/订阅等），多种传输协议，多种消息格式，提供服务编排和转换功能，支持事务处理和安全性等。\n\n微服务\n\n解耦复杂应用、独立部署、技术选型灵活、容错性好、易扩展。\n聚合器微服务\n代理微服务\n链式微服务\n分支微服务\n数据共享微服务\n异步消息微服务\n\n软件架构风格\n\n数据流架构\n\n批处理\n管道-过滤器\n\n\n调用/返回架构\n\n主程序/子程序\n面向对象\n客户端/服务器\n\n\n以数据为中心的架构\n\n仓库\n黑板\n\n\n虚拟机\n\n解释器\n\n\n独立构件\n\n事件驱动\n\n\n特殊\n\nC2\n过程控制\n\n\n\n面向对象和解释器风格的对比分析: 应该选择解释器风格。折扣规则的变化频繁，且变化的规则数量较多，使用面向对象风格会导致类的数量过多，增加了系统的复杂性和维护成本。而解释器风格则可以通过动态加载和解析规则来实现灵活的规则处理，适应性更强，且可以更好地支持规则的变化和扩展。个性化折扣定义灵活性方面，解析器风格可以通过定义规则的语法和语义来实现个性化折扣的定义，做到千人千面。系统性能方面，面向对象风格可能略优于解释器风格，但在实际应用中，解释器风格的性能也可以通过优化解析器和规则引擎来提升。综合考虑，解释器风格更适合该系统的需求。\n\n面向架构评估的质量属性\n易理解性、可扩展性、可重用性、可测试性、可维护性、可移植性、性能、安全性\n\n质量属性场景描述\n\n刺激源 刺激 环境 制品 响应 响应度量\n\n系统架构评估方法\n\n基于调查问卷或调查表\n基于场景的评估 包括 SAAM ATAM\n基于度量的评估\n\nSAAM\n\n输入: 问题描述、需求声明、架构描述\n过程: 场景开发、架构描述、单场景评估、场景交互评估、总体评估\n\nATAM 基于 SAAM\n\n演示、调查和分析、测试、报告\n\n调查和分析: 场景和需求收集、架构视图和场景实现、属性模型构造和分析、权衡\n\n\n效用树: 树根-质量属性-属性分类-质量属性场景（叶子节点）\n主要关注的质量属性: 性能、安全性、可修改性、可用性\n\n安全架构\n\n可用性、完整性、机密性\n\n数据加密技术\n\n对称加密\n非对称加密\n数字签名\n数字证书\n\n基于口令的简单认证机制 vs 基于公钥体系的机制:\n（1）基于口令的认证方式实现简单，但由于口令复杂度及管理方面的原因，易受到认证攻击；而在基于公钥体系的认证机制中，由于使用了公钥和私钥的非对称加密技术，在认证过程中，私钥仅存储在用户本地，不在网络中传输，因此可以有效地防止认证攻击，与基于口令的认证方式相比，安全性更高。（2）按照需求描述，系统在完成用户身份鉴别后，需根据用户的身份信息进行权限控制。在基于口令的认证机制中，用户口令为用户和认证服务器共享，没有用户独有的直接秘密信息。而在基于公钥体系的认证机制中，可基于用户私钥对私有数据进行加密，只有用户本人才能解密，实现更加简便。（3）基于公钥体系的认证机制的协议和计算相对复杂，因此在性能上会略逊于基于口令的认证机制。但由于企业业务环境的总用户在 100 人以内，规模不大，运行环境又为局域网环境，因此基于公钥体系的认证机制可以满足平台效率要求。\n系统架构原则\n批处理和流处理\nLambda 架构\n\n批处理层 Hive Spark MapReduce\n速度处理层 Storm Spark Streaming Flink\n服务层 Kylin Presto Impala Druid\n\n以批处理层和加速层结果数据为基础，对外提供低延时数据查询和即席查询服务\n\n\n\nKappa 架构\n\n只保留流处理层，批处理层被流处理层替代\n\n对比\nIOTA 架构\n数据库系统\n索引\n数据分片\n数据分区\n数据库高可用模式\n\n读写分离\n主从复制\n一致性问题\n\n数据库选型\n分布式系统\n\nCAP 理论: 数据一致性、可用性和分区容错性\nBASE 理论: 基本可用、软状态和最终一致性\n\n分布式事务\n\n2PC 两阶段提交协议\nTCC 柔性事务\n柔性事务本地消息表\n\nRedis\nMongoDB\n\n分片集群\n主从副本\n\nElasticSearch\n\n全文搜索\n倒排索引\n\n#论文\n#\n","categories":["misc","ruankao"],"tags":["软考"]},{"title":"日本语 0x01","url":"/2021/02/15/1/","content":"#What is Japanese\n#Japanese Sound System\n23 consonant sounds + 5 vowel sounds, with pitch accent\nWhen spoken, the Japanese language is formed using either vowels or a consonant-vowel pair, except the “n” sound (single consonant sound)\n#Japanese Writing System\nCome around 1500 years age from China\nCurrentlt has 3 types of writing: hiragana, katakana, kanji\nKanji were imported from China, traditionally used for both meaning and sound.\nHiragana: 46 characters with two diacritical marks that are used to denote the sounds\nEach hiragana symbolizes with a vowel or a consonant combined with a vowel.\nHiraganas are in particular used for function words and inflectional endings, as well as in some content words.\n\n\n\nHiragana\nん\nw\nr\ny\nm\nh\nn\nt\ns\nk\n\n\n\n\n\na\nん\nわ\nら\nや\nま\nは\nな\nた\nさ\nか\nあ\n\n\ni\n\n\nり\n\nみ\nひ\nに\nち\nし\nき\nい\n\n\nu\n\n\nる\nゆ\nむ\nふ\nぬ\nつ\nす\nく\nう\n\n\ne\n\n\nれ\n\nめ\nへ\nね\nて\nせ\nけ\nえ\n\n\no\n\nを\nろ\nよ\nも\nほ\nの\nと\nそ\nこ\nお\n\n\n\nKatagana is also a syllabary, is used for loan words, onomatopoeic words and scientific words.\n","categories":["misc","japanese"]},{"title":"20 行代码速通 Safe Rust Double Free (整活向)","url":"/2025/02/11/unsound-rust/","content":"受到 cve-rs 项目启发，利用 Type system 的固有 Unsoundness 实现一个经典的 Double Free。\n#代码\n#[inline(never)]pub const fn lifetime_translator&lt;&#x27;a, &#x27;b, T: ?Sized&gt;(_: &amp;&#x27;a &amp;&#x27;b (), x: &amp;&#x27;b mut T) -&gt; &amp;&#x27;a mut T &#123;    x&#125;pub fn expand&lt;&#x27;a, &#x27;b, T: ?Sized&gt;(x: &amp;&#x27;a mut T) -&gt; &amp;&#x27;b mut T &#123;    let f: for&lt;&#x27;x&gt; fn(_, &amp;&#x27;x mut T) -&gt; &amp;&#x27;b mut T = lifetime_translator;    f(STATIC_UNIT, x)&#125;pub const STATIC_UNIT: &amp;&amp;() = &amp;&amp;();pub fn main() &#123;    let dead = expand(&amp;mut vec![1, 2, 3, 4, 5, 6, 7, 8]);    dead.push(1);&#125;\n#运行\nfrezcirno@homelab:~/unsound-rust$ rustc unsound.rsfrezcirno@homelab:~/unsound-rust$ ./unsoundfree(): double free detected in tcache 2已中止\n","categories":["misc","rust"]},{"title":"ELF 文件格式分析","url":"/2024/08/10/ELF-file-format/","content":"#宏观结构\n\n运行视图：分为若干个程序段（Segment），一个段表示一种加载类型，包括需要加载到内存的地址和属性（是否可读, 是否可写, 是否可执行）；\n链接视图：分为若干个程序节（Section），表示不同作用的程序组成部分，用于链接，例如 .data, .rodata, .text；类似标签的作用；文件中的节区不能重叠；不允许一个字节存在于两个节区中的情况发生。\n一般来说一个 Segment 会包含多个 Section，例如：\nSegment 1: Text Segment* .text* .rodata* .hash* .dynsym* .dynstr* .plt* .rel.gotSegment 2: Data Segment* .data* .dynamic* .got* .bss\n#数据类型定义\n/* Type for a 16-bit quantity.  */typedef uint16_t Elf32_Half;typedef uint16_t Elf64_Half;/* Types for signed and unsigned 32-bit quantities.  */typedef uint32_t Elf32_Word;typedef int32_t  Elf32_Sword;typedef uint32_t Elf64_Word;typedef int32_t  Elf64_Sword;/* Types for signed and unsigned 64-bit quantities.  */typedef uint64_t Elf32_Xword;typedef int64_t  Elf32_Sxword;typedef uint64_t Elf64_Xword;typedef int64_t  Elf64_Sxword;/* Type of addresses.  */typedef uint32_t Elf32_Addr;typedef uint64_t Elf64_Addr;/* Type of file offsets.  */typedef uint32_t Elf32_Off;typedef uint64_t Elf64_Off;/* Type for section indices, which are 16-bit quantities.  */typedef uint16_t Elf32_Section;typedef uint16_t Elf64_Section;/* Type for version symbol information.  */typedef Elf32_Half Elf32_Versym;typedef Elf64_Half Elf64_Versym;\n#ELF Header (Ehdr)\n\n魔数：[0x7f, 'E', 'L', 'F']\nPhdr 大小，Phdr 个数\nShdr 大小，Shdr 个数\n入口地址 e_entry\n\ntypedef struct&#123;  unsigned char e_ident[EI_NIDENT]; /* Magic number and other info */  Elf32_Half e_type;   /* Object file type */  Elf32_Half e_machine;  /* Architecture */  Elf32_Word e_version;  /* Object file version */  Elf32_Addr e_entry;  /* Entry point virtual address */  Elf32_Off e_phoff;  /* Program header table file offset */  Elf32_Off e_shoff;  /* Section header table file offset */  Elf32_Word e_flags;  /* Processor-specific flags */  Elf32_Half e_ehsize;  /* ELF header size in bytes */  Elf32_Half e_phentsize;  /* Program header table entry size */  Elf32_Half e_phnum;  /* Program header table entry count */  Elf32_Half e_shentsize;  /* Section header table entry size */  Elf32_Half e_shnum;  /* Section header table entry count */  Elf32_Half e_shstrndx;  /* Section header string table index */&#125; Elf32_Ehdr;typedef struct&#123;  unsigned char e_ident[EI_NIDENT]; /* Magic number and other info */  Elf64_Half e_type;   /* Object file type */  Elf64_Half e_machine;  /* Architecture */  Elf64_Word e_version;  /* Object file version */  Elf64_Addr e_entry;  /* Entry point virtual address */  Elf64_Off e_phoff;  /* Program header table file offset */  Elf64_Off e_shoff;  /* Section header table file offset */  Elf64_Word e_flags;  /* Processor-specific flags */  Elf64_Half e_ehsize;  /* ELF header size in bytes */  Elf64_Half e_phentsize;  /* Program header table entry size */  Elf64_Half e_phnum;  /* Program header table entry count */  Elf64_Half e_shentsize;  /* Section header table entry size */  Elf64_Half e_shnum;  /* Section header table entry count */  Elf64_Half e_shstrndx;  /* Section header string table index */&#125; Elf64_Ehdr;\n#Program Header (Phdr)\n一般在文件开头紧邻 EHdr，有多个 Phdr，起始偏移量由 e_phoff 指定，数量由 e_phnum 指定；\n\np_vaddr 段加载的虚拟地址\np_flags 段属性\n\ntypedef struct&#123;  Elf32_Word p_type;   /* Segment type */  Elf32_Off p_offset;  /* Segment file offset */  Elf32_Addr p_vaddr;  /* Segment virtual address */  Elf32_Addr p_paddr;  /* Segment physical address */  Elf32_Word p_filesz;  /* Segment size in file */  Elf32_Word p_memsz;  /* Segment size in memory */  Elf32_Word p_flags;  /* Segment flags */  Elf32_Word p_align;  /* Segment alignment */&#125; Elf32_Phdr;typedef struct&#123;  Elf64_Word p_type;   /* Segment type */  Elf64_Word p_flags;  /* Segment flags */  Elf64_Off p_offset;  /* Segment file offset */  Elf64_Addr p_vaddr;  /* Segment virtual address */  Elf64_Addr p_paddr;  /* Segment physical address */  Elf64_Xword p_filesz;  /* Segment size in file */  Elf64_Xword p_memsz;  /* Segment size in memory */  Elf64_Xword p_align;  /* Segment alignment */&#125; Elf64_Phdr;\n#Section Header (Shdr)\n一般在文件末尾，起始偏移量由 e_shoff 指定，数量由 e_shnum 指定；\ntypedef struct&#123;  Elf32_Word sh_name;  /* Section name (string tbl index) */  Elf32_Word sh_type;  /* Section type */  Elf32_Word sh_flags;  /* Section flags */  Elf32_Addr sh_addr;  /* Section virtual addr at execution */  Elf32_Off sh_offset;  /* Section file offset */  Elf32_Word sh_size;  /* Section size in bytes */  Elf32_Word sh_link;  /* Link to another section */  Elf32_Word sh_info;  /* Additional section information */  Elf32_Word sh_addralign;  /* Section alignment */  Elf32_Word sh_entsize;  /* Entry size if section holds table */&#125; Elf32_Shdr;typedef struct&#123;  Elf64_Word sh_name;  /* Section name (string tbl index) */  Elf64_Word sh_type;  /* Section type */  Elf64_Xword sh_flags;  /* Section flags */  Elf64_Addr sh_addr;  /* Section virtual addr at execution */  Elf64_Off sh_offset;  /* Section file offset */  Elf64_Xword sh_size;  /* Section size in bytes */  Elf64_Word sh_link;  /* Link to another section */  Elf64_Word sh_info;  /* Additional section information */  Elf64_Xword sh_addralign;  /* Section alignment */  Elf64_Xword sh_entsize;  /* Entry size if section holds table */&#125; Elf64_Shdr;\n#实例\n\nEHdr\nPHdr 1\nPHdr 2\nPHdr …\nSegment: PT_PHDR (self)\nSegment: PT_INTERP\n\n---- .interp ----\n&quot;/lib64/ld-linux-x86-64.so.2&quot;\n\n\nSegment: PT_NOTE r–\n\n---- .note.gnu.property ----\n---- .note.gnu.build-id ----\n---- .note.ABI-tag ----\n\n\nSegment: PT_GNU_HASH\nSegment: Symbol Table\nSegment: PT_LOAD r-x\n\n---- .init ----\n\n_init_proc\n\n\n---- .plt ----\n\nsub_4020\nsub_4030\n...\n\n\n---- .plt.got ----\n\nfree\nstrcmp\n...\n\n\n---- .plt.sec ----\n\n___ctype_toupper_loc\n...\n\n\n---- .text ----\n\nloc_4D70\n...\n\n\n---- .fini ----\n\n_term_proc\n\n\n\n\nSegment: PT_LOAD r–\n\n---- .rodata ----\n\nunk_19000\n...\n\n\n---- .eh_frame_hdr ----\n---- .eh_frame ----\n\n\nSegment: PT_LOAD rw-\nSegment: PT_GNU_RELRO rw-\n\n---- .init_array ----\n\n0x6E10\n\n\n---- .fini_array ----\n\n0x6DD0\n\n\n\n\nSHdr 1:\n\n---- .data.rel.ro ----\n\n\nSHdr …n\n\n– Segment: PT_DYNAMIC rw-\nstru_22A38\n---- .got ----\n---- .data ----\n---- .bss ----\n\n\nSegment: Externs\nSegment: String Table\n\n\n#典型 Segment\n#PT_LOAD\n普通的需要加载的段\n#PT_INTERP\n包含一个 C 字符串，指向动态链接器的路径，\n当创建一个可执行文件时，如果依赖其它的动态链接库，那么链接编辑器会在可执行文件的程序头中加入一个 PT_INTERP 项，告诉系统这里需要使用动态链接器，一般链接器为 ld。\n#PT_NOTE\n附加信息，例如 ABI 版本，构建 ID 等\n#PT_PHDR\n包含一个程序头表的副本，用于动态链接器的加载\n#典型 Section\nSection 类型表：\n#define SHT_NULL   0  /* Section header table entry unused */#define SHT_PROGBITS   1  /* Program data */#define SHT_SYMTAB   2  /* Symbol table */#define SHT_STRTAB   3  /* String table */#define SHT_RELA   4  /* Relocation entries with addends */#define SHT_HASH   5  /* Symbol hash table */#define SHT_DYNAMIC   6  /* Dynamic linking information */#define SHT_NOTE   7  /* Notes */#define SHT_NOBITS   8  /* Program space with no data (bss) */#define SHT_REL    9  /* Relocation entries, no addends */#define SHT_SHLIB   10  /* Reserved */#define SHT_DYNSYM   11  /* Dynamic linker symbol table */#define SHT_INIT_ARRAY   14  /* Array of constructors */#define SHT_FINI_ARRAY   15  /* Array of destructors */#define SHT_PREINIT_ARRAY 16  /* Array of pre-constructors */#define SHT_GROUP   17  /* Section group */#define SHT_SYMTAB_SHNDX  18  /* Extended section indices */#define SHT_RELR   19            /* RELR relative relocations */#define SHT_NUM    20  /* Number of defined types.  */#define SHT_LOOS   0x60000000 /* Start OS-specific.  */#define SHT_GNU_ATTRIBUTES 0x6ffffff5 /* Object attributes.  */#define SHT_GNU_HASH   0x6ffffff6 /* GNU-style hash table.  */#define SHT_GNU_LIBLIST   0x6ffffff7 /* Prelink library list */#define SHT_CHECKSUM   0x6ffffff8 /* Checksum for DSO content.  */#define SHT_LOSUNW   0x6ffffffa /* Sun-specific low bound.  */#define SHT_SUNW_move   0x6ffffffa#define SHT_SUNW_COMDAT   0x6ffffffb#define SHT_SUNW_syminfo  0x6ffffffc#define SHT_GNU_verdef   0x6ffffffd /* Version definition section.  */#define SHT_GNU_verneed   0x6ffffffe /* Version needs section.  */#define SHT_GNU_versym   0x6fffffff /* Version symbol table.  */#define SHT_HISUNW   0x6fffffff /* Sun-specific high bound.  */#define SHT_HIOS   0x6fffffff /* End OS-specific type */#define SHT_LOPROC   0x70000000 /* Start of processor-specific */#define SHT_HIPROC   0x7fffffff /* End of processor-specific */#define SHT_LOUSER   0x80000000 /* Start of application-specific */#define SHT_HIUSER   0x8fffffff /* End of application-specific */\n#.shstrtab 节名字符串表\n字符串表，存储 ELF 结构中所有的 Section 的名字。类型是 SHT_STRTAB；由 EHdr 中的 e_shstrndx 确定本节的位置；\n#.dyntab 符号名字符串表\n类型 SHT_STRTAB; 存储程序中的符号名，包含变量名、函数名等；\n\n这部分信息在进行  strip  后就会消失\n\n#SHT_SYMTAB 符号表\n包含\ntypedef struct&#123;  Elf32_Word st_name;  /* Symbol name (string tbl index) */  Elf32_Addr st_value;  /* Symbol value */  Elf32_Word st_size;  /* Symbol size */  unsigned char st_info;  /* Symbol type and binding */  unsigned char st_other;  /* Symbol visibility */  Elf32_Section st_shndx;  /* Section index */&#125; Elf32_Sym;typedef struct&#123;  Elf64_Word st_name;  /* Symbol name (string tbl index) */  unsigned char st_info;  /* Symbol type and binding */  unsigned char st_other;  /* Symbol visibility */  Elf64_Section st_shndx;  /* Section index */  Elf64_Addr st_value;  /* Symbol value */  Elf64_Xword st_size;  /* Symbol size */&#125; Elf64_Sym;\n#.dynamic 动态链接节\n","categories":["program-analysis","binary-analysis"],"tags":["二进制","ELF"]},{"title":"模糊测试 Fuzzing test","url":"/2023/12/19/afl-fuzz/","content":"#编译项目，插桩\n#!/bin/shset -eexport CC=afl-clang-fastexport CXX=afl-clang-fast++./configure --enable-shared=no --enable-static=yesmake -j 32\n#运行 Fuzz\n#!/bin/shtmux new-window afl-fuzz -i fuzz-input -o fuzz-output -t 100000 -m 200 -S 1 -- ./utilities/magick convert @@ /tmp/out.png &amp;tmux new-window afl-fuzz -i fuzz-input -o fuzz-output -t 100000 -m 200 -S 2 -- ./utilities/magick convert @@ /tmp/out.png &amp;tmux new-window afl-fuzz -i fuzz-input -o fuzz-output -t 100000 -m 200 -S 3 -- ./utilities/magick convert @@ /tmp/out.png &amp;tmux new-window afl-fuzz -i fuzz-input -o fuzz-output -t 100000 -m 200 -M 0 -- ./utilities/magick convert @@ /tmp/out.png\n","categories":["program-analysis","software-test"],"tags":["软件测试","模糊测试","fuzz"]},{"title":"jpf-symbc 踩坑记录","url":"/2023/12/20/jpf-symbc/","content":"#JPF\nAnalysis system for Java\nDeveloped by NASA\njpf 可以用来:\n\nsoftware model checking (deadlock &amp; race detection)\ndeep inspection (numeric analysis, invalid access)\ntest case generation (symbolic execution)\n\n#组件\nJava 字节码解释器(虚拟机)\nListeners: To print results of symbolic analysis (path conditions, test vectors or test sequences); to influence the search\n#相关资源\n\njpf-symbc 作者的 notion 笔记\nJavaPathfinder Tutorial 02/2010\nJPF Tutorial – Part 2 Symbolic PathFinder – Symbolic Execution of Java Byte-code\nJava Pathfinder Lecture 2: Under the Hood\nJava Path Finder 2015\n\n\n#安装和配置\n安装配置文档竟在作者的 notion 笔记里面…\n下面讲一下笔者的踩坑经验\n\n克隆项目\n\njpf-core 和 jpf-symbc 放在同一个目录下面\njpf-core 要用这个版本 -&gt; https://github.com/yannicnoller/jpf-core\n.├── jpf-core└── jpf-symbc\n\n编译项目\n\njpf-symbc 只支持 Java 8\n切换到 Java 8\n$ jabba install temulin@8$ jabba use temulin@8\njpf-core 用 ant 编译\n## in jpf-coreant build## (Optional) testJUNIT_HOME=/usr/share/maven-repo/junit/junit/4.13.2/ ant test\njpf-symbc 用 ant 编译\n## in jpf-symbcant build## (Optional) testJUNIT_HOME=/usr/share/maven-repo/junit/junit/4.13.2/ ant test\n\n配置 ~/.jpf/site.properties\n\n里面填写如下内容\njpf-core = $&#123;user.home&#125;/src/javapathfinder/jpf-corejpf-symbc = $&#123;user.home&#125;/src/javapathfinder/jpf-symbcextensions = $&#123;jpf-core&#125;,$&#123;jpf-symbc&#125;\n其中对应修改两个项目的路径\n\n测试下能否正常使用\n\n## in jpf-symbc$ java -Xmx1024m -ea -jar ../jpf-core/build/RunJPF.jar src/examples/demo/NumericExample.jpfsymbolic.min_int=-2147483648symbolic.min_long=-9223372036854775808symbolic.min_short=-32768symbolic.min_byte=-128symbolic.min_char=0symbolic.max_int=2147483647symbolic.max_long=9223372036854775807symbolic.max_short=32767symbolic.max_byte=127symbolic.max_char=65535symbolic.min_double=4.9E-324symbolic.max_double=1.7976931348623157E308JavaPathfinder core system v8.0 - (C) 2005-2014 United States Government. All rights reserved.====================================================== system under testdemo.NumericExample.main()====================================================== search started: 23-12-29 下午2:55&gt;0&lt;=0Property Violated: PC is constraint ## = 1((a_1_SYMINT[15] + b_2_SYMINT[-13]) - CONST_2) == CONST_0Property Violated: result is  &quot;java.lang.ArithmeticException: div by 0...&quot;****************************====================================================== error 1gov.nasa.jpf.vm.NoUncaughtExceptionsPropertyjava.lang.ArithmeticException: div by 0        at demo.NumericExample.test(NumericExample.java:26)        at demo.NumericExample.main(NumericExample.java:34)====================================================== snapshot #1thread java.lang.Thread:&#123;id:0,name:main,status:RUNNING,priority:5,isDaemon:false,lockCount:0,suspendCount:0&#125;  call stack:        at demo.NumericExample.test(NumericExample.java:26)        at demo.NumericExample.main(NumericExample.java:34)====================================================== Method SummariesInputs: a_1_SYMINT,b_2_SYMINTdemo.NumericExample.test(-50,17)  --&gt; Return Value: --demo.NumericExample.test(0,0)  --&gt; Return Value: --demo.NumericExample.test(15,-13)  --&gt; &quot;java.lang.ArithmeticException: div by 0...&quot;====================================================== Method Summaries (HTML)&lt;h1&gt;Test Cases Generated by Symbolic JavaPath Finder for demo.NumericExample.test (Path Coverage) &lt;/h1&gt;&lt;table border=1&gt;&lt;tr&gt;&lt;td&gt;a_1_SYMINT&lt;/td&gt;&lt;td&gt;b_2_SYMINT&lt;/td&gt;&lt;td&gt;RETURN&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;-50&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;Return Value: --&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;Return Value: --&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;-13&lt;/td&gt;&lt;td&gt;&quot;java.lang.ArithmeticException: div by 0...&quot;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;====================================================== resultserror #1: gov.nasa.jpf.vm.NoUncaughtExceptionsProperty &quot;java.lang.ArithmeticException: div by 0  at demo.N...&quot;====================================================== statisticselapsed time:       00:00:00states:             new=5,visited=0,backtracked=5,end=2search:             maxDepth=3,constraints=0choice generators:  thread=1 (signal=0,lock=1,sharedRef=0,threadApi=0,reschedule=0), data=2heap:               new=371,released=20,maxLive=349,gcCycles=3instructions:       3246max memory:         709MBloaded code:        classes=64,methods=1325====================================================== search finished: 23-12-29 下午2:55\n#jpf-symbc 使用\n对于每个要进行符号执行的程序, 需要编写一个 .jpf 文件, 然后调用 RunJPF.jar:\n## in jpf-symbc$ java -Xmx1024m -ea -jar ../jpf-core/build/RunJPF.jar src/examples/demo/NumericExample.jpf\n#.jpf 文件内容含义\n; 测试的application类target=x.y.TreeMapSimpletarget.args=arg1,arg2classpath=$&#123;jpf-symbc&#125;/build/examples  ; path to your class examplesourcepath=$&#123;jpf-symbc&#125;/build/examples  ; path to the source of your example; 注册到jpf-core的listener; 控制接下来做什么; gov.nasa.jpf.listener.PreciseRaceDetector  竞争情况检查; gov.nasa.jpf.listener.CoverageAnalyzer   分析覆盖率; gov.nasa.jpf.listener.DeadlockAnalyzer   分析代码死锁; gov.nasa.jpf.listener.MethodAnalyzer; gov.nasa.jpf.symbc.SymbolicListener   输出 PC 和 JUnit test; gov.nasa.jpf.symbc.sequences.SymbolicSequenceListener  输出 JUnit test; gov.nasa.jpf.symbc.GreenListenerlistener=gov.nasa.jpf.symbc.SymbolicListener,gov.nasa.jpf.symbc.sequences.SymbolicSequenceListener,gov.nasa.jpf.listener.PreciseRaceDetector,gov.nasa.jpf.listener.CoverageAnalyzer; specify some search limitsearch.depth_limit=15;coverage.include=dataflow.Billingcoverage.show_requirements=true; 遇到exception的时候打印stack trace; 和 PreciseRaceDetector 一起使用; error trace snapshotreport.console.property_violation=error,trace; 对于 random.nextInt 枚举随机数cg.enumerate_random=true; 和 MethodAnalyzer 一起使用method.include=*Robot*; 测试的目标方法, 多个函数用逗号分隔; 函数参数用 (sym) 表示, 多个参数用 ## 分隔; 例如: triangle.Triangle.getType(sym#sym#sym)symbolic.method=TreeMapSimple.containsKey(sym),TreeMapSimple.put(sym),TreeMapSimple.remove(sym); specify the decision procedure to use; z3, z3inc, z3bitvector, z3bitvectorinc; cvc3, cvc3bitvec, choco, iasolversymbolic.dp=z3bitvectorsymbolic.bvlength=64; 各种类型的范围symbolic.min_int=-100symbolic.max_int=100symbolic.min_double=-100.0symbolic.max_double=100.0; print some debug informationsymbolic.debug=true; handling symbolic arrayssymbolic.lazy=onsymbolic.arrays=true; specify string analysissymbolic.strings=truesymbolic.string_dp=ABC  ; or z3strsymbolic.string_dp_timeout_ms=3000\n\n","categories":["program-analysis","software-test"],"tags":["jpf-symbc"]},{"title":"KLEE 符号执行","url":"/2023/12/19/klee/","content":"#基本使用\n\n修改程序源码，在合适的地方插入符号值\n\nint main() &#123;  int a;+  klee_make_symbolic(&amp;a, sizeof(a), &quot;a&quot;);  return get_sign(a);&#125;\n\n编译成 LLVM bitcode\n\n$ clang -I ../../include -emit-llvm -c -g -O0 -Xclang -disable-O0-optnone get_sign.c\n\n符号模拟执行 bitcode\n\n$ klee get_sign.bc\n\n查看模拟得到的 test cases\n\n$ ktest-tool klee-last/test000001.ktest\n#内部实现\n#Directory Structure\n\nlib - KLEE 核心源代码\n\nBasic - 不依赖 LLVM 的工具代码, 一堆工具类\nSupport - 依赖 LLVM 的工具代码, 一堆工具类\n\nKTest.cpp\nStatistics.cpp\n\n\nModule - klee facilities for working with LLVM modules, including the shadow module/instruction structures we use during execution.\nExpr - 表达式对象\nSolver - 求解器 binder\nCore - 核心符号虚拟机\n\n\ntools - KLEE 的二进制\n\nkleaver - 查询什么 log\nklee - 符号模拟执行一个 bitcode\nklee-replay - 复现 ktest\nklee-stats\nklee-zesti\nktest-gen\nktest-randgen\nktest-tool\n\n\nruntime - 符号模拟执行时依赖的库\n\n#Utilities KLEE 辅助类\n#引用计数 ReferenceCounter ref&lt;T&gt;\n这两个类用于实现引用计数\nReferenceCounter 放在被计数的对象中\nclass Human &#123;  ReferenceCounter _refCount;&#125;;\nref&lt;T&gt; 是对象的引用, 重载了*和-&gt;运算符, 可以像指针一样使用\nref&lt;Human&gt; r(new Human());r-&gt;hello();\n#tools/klee KLEE 工具源码\n#main 主函数\n\n\nklee::loadFile 加载输入文件，可以是单个 bitcode(.bc)或者 archive(.a), 得到一组llvm::Module\n\n\nklee::linkModules 把所有 Module 链接成一整个\n\n\n检查 triple\n\n\n配置 Libcxx, Libc\n\n\n配置程序运行环境: argc, argv, envp\n\n\n创建 Handler, Executor impl Interpreter\n\n\nexternalsAndGlobalsCheck 做检查\n\n\nReplay or 模拟执行\n\n\ninterpreter-&gt;runFunctionAsMain 开始模拟执行\n\n\n#Executor::run 开始\n\n\nusingSeeds 判断\n\n\n主循环\n\n\n// main interpreter loopwhile (!states.empty() &amp;&amp; !haltExecution) &#123;  ExecutionState &amp;state = searcher-&gt;selectState();  // 选择一个 State  KInstruction *ki = state.pc;  // 取指令  stepInstruction(state);  // 更新 PC  executeInstruction(state, ki);  // 执行指令  timers.invoke();  if (::dumpStates) dumpStates();  if (::dumpPTree) dumpPTree();  updateStates(&amp;state);  // 更新 State  if (!checkMemoryUsage()) &#123;    // update searchers when states were terminated early due to memory pressure    updateStates(nullptr);  &#125;&#125;\n很清晰的代码, 每次循环选择一个程序 ExecutionState, 执行下条 KInstruction, 更新状态, 然后重复.\n其中 Searcher 实现了探索的策略, klee 中有多种实现, 如 BFS/DFS/Random 等\n#程序状态 ExecutionState, StackFrame\n执行状态, 包括栈帧\nclass ExecutionState &#123;  /// @brief Pointer to instruction to be executed after the current  /// instruction  KInstIterator pc;  /// @brief Stack representing the current instruction stream  std::vector&lt;StackFrame&gt; stack;  // ...&#125;;\n栈帧里面包含 locals, 是一堆预分配的 Cell, 数量等于函数参数+指令总数\nstruct StackFrame &#123;  KInstIterator caller;  KFunction *kf;  CallPathNode *callPathNode;  std::vector&lt;const MemoryObject *&gt; allocas;  Cell *locals;&#125;;\nCell 即对 Expr 的引用\nnamespace klee &#123;  struct Cell &#123;    ref&lt;Expr&gt; value;  &#125;;&#125;\n#Executor::executeInstruction 开始\n根据指令 opcode 对应处理\ncase Instruction::Ret:      // 求解return返回的值      result = eval(ki, 0, state).value;      // 考虑是否需要进行 coercion      // bindLocal caller指令 的 dest cell 的 value      bindLocal(kcaller, state, result);case Instruction::Br:case Instruction::IndirectBr:case Instruction::Switch:case Instruction::Unreachable:case Instruction::Invoke:case Instruction::Call:case Instruction::PHI:case Instruction::Select:case Instruction::VAArg:case Instruction::Add:case Instruction::Sub:case Instruction::Mul:case Instruction::UDiv:case Instruction::SDiv:case Instruction::URem:case Instruction::SRem:case Instruction::And:case Instruction::Or:case Instruction::Xor:case Instruction::Shl:case Instruction::LShr:case Instruction::AShr:case Instruction::ICmp:case Instruction::Alloca:case Instruction::Load:case Instruction::Store:case Instruction::GetElementPtr:case Instruction::Trunc:case Instruction::ZExt:case Instruction::SExt:case Instruction::IntToPtr:case Instruction::PtrToInt:case Instruction::BitCast:case Instruction::FNeg:case Instruction::FAdd:case Instruction::FSub:case Instruction::FMul:case Instruction::FDiv:case Instruction::FRem:case Instruction::FPTrunc:case Instruction::FPExt:case Instruction::FPToUI:case Instruction::FPToSI:case Instruction::UIToFP:case Instruction::SIToFP:case Instruction::FCmp:case Instruction::InsertValue:case Instruction::ExtractValue:case Instruction::Fence:case Instruction::InsertElement:case Instruction::ExtractElement:case Instruction::ShuffleVector:case Instruction::Resume:case Instruction::LandingPad:case Instruction::AtomicRMW:case Instruction::AtomicCmpXchg:\n","categories":["program-analysis","software-test"],"tags":["Program Analysis"]},{"title":"动态规划算法","url":"/2021/01/09/dynamic-programming/","content":"#特点\n\n重叠子问题\n最优子结构\n状态转移方程\n状态压缩\n\n#思路\n\n找出所有的状态\n列举 dp 数组\n找出状态转移方程\n\n","categories":["programming","algorithm-practice"],"tags":["算法"]},{"title":"C++ 并发三件套 - future, promise, async","url":"/2024/04/20/cpp-concurrency/","content":"C++标准库提供了很多用于并发编程的工具，不太熟悉，总结一下。\n#异步计算结果 std::future&lt;T&gt;\nstd::future&lt;T&gt; 表示一个异步计算过程的结果。\nwait() 方法阻塞当前线程直至计算完成。如果计算已经完成，则直接返回。\nget() 方法等待计算完成，并获取 future&lt;T&gt; 表示的值或异常。get() 方法会消耗掉当前 future 保存的值，因此只能调用一次。\nvalid() 方法表示当前 future 中是否还有值可以被获取，如果值已经被 get 过，则会返回 false。\nshare() 方法返回一个 std::shared_future&lt;T&gt;。\nstd::future&lt;T&gt; 是 move-only 的结构，且独占 T 的所有权。当多个对等的线程需要访问同一个期望值时，可以使用 copyable 的 std::shared_future&lt;T&gt;。注意，std::shared_future&lt;T&gt; 本身并不是线程安全的，需要每个线程拷贝并持有一个自己的 std::shared_future&lt;T&gt;。\n注: 如果调用析构函数的那个 future 是某一 shared state 的最后持有者，而相关的任务已启动但尚未结束，析构函数会造成阻塞，直到任务结束\n#异步生产者 std::promise&lt;T&gt;\nstd::promise&lt;T&gt; 用于生产者提供一个 std::future&lt;T&gt; 及其中的值给消费者。std::promise&lt;T&gt;::get_future() 方法返回一个 std::future&lt;T&gt; 给消费者。\n对于生产者，使用 std::promise&lt;T&gt;::set_value() 方法填充 future 中的值；或者使用 std::promise&lt;T&gt;::set_exception() 方法填充 future 中的异常；\nextern std::promise&lt;double&gt; some_promise;try &#123;  some_promise.set_value(calculate_value());&#125; catch(...) &#123;  some_promise.set_exception(std::current_exception());&#125;\n#异步执行 std::async()\nstd::async 是一个函数，它启动一个异步任务去执行指定的函数，返回一个表示计算结果的 std::future\nstd::async 允许额外指定一个std::launch类型的参数来指定任务启动的方式，有以下几种启动策略：\n\nstd::launch::async: 任务必须在独立线程中执行，因此需要库去负责线程创建和销毁；\nstd::launch::deferred: 任务可以延迟到wait()或者get()的时候，在调用者线程上同步执行 (有点类似 Rust Tokio 的设计)；\nstd::launch::async | std::launch::deferred: 默认的方式，表示以上两种执行时机都可以，由具体实现去选择。\n\n","categories":["programming","cpp"],"tags":["C++","并发编程"]},{"title":"C++ `constexpr` 的限制","url":"/2024/04/21/cpp-constexpr-usage/","content":"\n\nconstexpr函数不能包含条件语句（if、switch）、循环（for、while）、异常处理（try、catch）等。\n\n\nconstexpr函数不能进行动态内存分配。\n\n\nconstexpr变量必须是编译时常量，即其值在程序运行期间不能被修改。\n\n\nconstexpr变量不能是类的成员变量，因为类的成员变量可能不是编译时常量。\n\n\nconstexpr变量不能是左值，因为左值在编译时无法确定其值。\n\n\nconstexpr函数不能有副作用，即不能修改全局变量的值或调用非constexpr函数。\n\n\nconstexpr函数和变量的返回类型只能是内置类型或枚举类型，不能是自定义类型或类类型。\n\n\nconstexpr函数和变量的参数必须是编译时常量表达式或字面量。\n\n\nconstexpr函数和变量的返回值必须是编译时常量表达式或字面量。\n\n\nconstexpr函数和变量的定义必须在声明之前，且必须具有相同的类型和值。\n\n\n","categories":["programming","cpp"],"tags":["C++"]},{"title":"支持元素更新的D元最小堆模版","url":"/2024/04/21/cpp-d-ary-heap-structure/","content":"做 CMU 15-445 的时候需要实现一个支持元素更新的堆。\nSTL 中的堆是不支持元素更新的，但是我们可以利用元素上浮和元素下沉的操作实现元素更新。具体做法也很简单，根据元素是增大还是减小，调用对应操作保证符合堆的性质即可。\n不幸的是 STL 又没有暴露元素上浮和元素下沉的接口，于是只能自己写一个了。\n为了能够更新元素的值，给每个元素都附带了一个key标签，凭key去更新元素值，所以最终整个数据结构演变成了类似哈希表的结构。\n实现时顺便实现了D 元堆，元数作为模版参数。\n/** * KeyedMinHeap is a min-heap that supports the following operations: * - Insert: Insert a key with a score into the heap * - Update: Update the score of a key in the heap * - TopKey: Get the key with the minimum score * - TopScore: Get the minimum score * - Pop: Remove the key with the minimum score * - Erase: Remove a key from the heap */template &lt;typename Key, std::size_t ARITY = 4&gt;class KeyedMinHeap &#123; public:  using heap_index_t = std::size_t;  using score_t = std::size_t; public:  KeyedMinHeap() = default;  auto Insert(const Key &amp;key, score_t score) &#123;    keys_map_[key] = heap_.size();    heap_.emplace_back(key, score);    BubbleUp(heap_.size() - 1);  &#125;  auto Increment(const Key &amp;key) &#123;    auto index_iter = keys_map_.find(key);    assert(index_iter != keys_map_.end() &amp;&amp; &quot;Key not found&quot;);    heap_index_t h_index = index_iter-&gt;second;    heap_[h_index].second++;    BubbleDown(h_index);  &#125;  auto Update(const Key &amp;key, score_t new_score) &#123;    auto index_iter = keys_map_.find(key);    assert(index_iter != keys_map_.end() &amp;&amp; &quot;Key not found&quot;);    heap_index_t h_index = index_iter-&gt;second;    score_t old_score = heap_[h_index].second;    if (old_score == new_score) &#123;      return;    &#125;    heap_[h_index].second = new_score;    if (new_score &lt; old_score) &#123;      BubbleUp(h_index);    &#125; else &#123;      BubbleDown(h_index);    &#125;  &#125;  auto TopKey() const -&gt; Key &#123;    assert(!heap_.empty() &amp;&amp; &quot;Heap is empty&quot;);    return heap_.front().first;  &#125;  auto TopScore() const -&gt; score_t &#123;    assert(!heap_.empty() &amp;&amp; &quot;Heap is empty&quot;);    return heap_.front().second;  &#125;  auto Pop() -&gt; void &#123;    assert(!heap_.empty() &amp;&amp; &quot;Heap is empty&quot;);    auto top_key = heap_.front().first;    auto last_key = heap_.back().first;    keys_map_.erase(top_key);    keys_map_[last_key] = 0;    std::swap(heap_.front(), heap_.back());    heap_.pop_back();    BubbleDown(0);  &#125;  auto Erase(const Key &amp;key) -&gt; void &#123;    auto entry = keys_map_.find(key);    assert(entry != keys_map_.end() &amp;&amp; &quot;Key not found&quot;);    heap_index_t index = entry-&gt;second;    keys_map_.erase(entry);    keys_map_[heap_.back().first] = index;    std::swap(heap_[index], heap_.back());    heap_.pop_back();    BubbleUp(index);    BubbleDown(index);  &#125;  auto Size() const -&gt; std::size_t &#123; return heap_.size(); &#125;  auto Empty() const -&gt; bool &#123; return heap_.empty(); &#125; private:  auto ParentIndex(heap_index_t h_index) const -&gt; heap_index_t &#123; return (h_index - 1) / ARITY; &#125;  // Returns the index of the child_number-th child of the node at index  // The children are numbered from 1 to ARITY  auto ChildIndex(heap_index_t h_index, std::size_t child_number) const -&gt; heap_index_t &#123;    return ARITY * h_index + child_number;  &#125;  void BubbleUp(heap_index_t h_index) &#123;    while (h_index != 0) &#123;      std::size_t parent = ParentIndex(h_index);      auto &amp;parent_entry = heap_[parent];      auto &amp;entry = heap_[h_index];      if (entry.second &lt; parent_entry.second) &#123;        std::swap(keys_map_[entry.first], keys_map_[parent_entry.first]);        std::swap(entry, parent_entry);        h_index = parent;      &#125; else &#123;        break;      &#125;    &#125;  &#125;  void BubbleDown(heap_index_t h_index) &#123;    while (true) &#123;      heap_index_t smallest = h_index;      for (std::size_t i = 1; i &lt;= ARITY; ++i) &#123;        heap_index_t child = ChildIndex(h_index, i);        if (child &lt; heap_.size() &amp;&amp; heap_[child].second &lt; heap_[smallest].second) &#123;          smallest = child;        &#125;      &#125;      if (smallest != h_index) &#123;        const auto &amp;entry = heap_[h_index];        const auto &amp;smallest_entry = heap_[smallest];        std::swap(keys_map_[entry.first], keys_map_[smallest_entry.first]);        std::swap(heap_[h_index], heap_[smallest]);        h_index = smallest;      &#125; else &#123;        break;      &#125;    &#125;  &#125; private:  std::unordered_map&lt;Key, heap_index_t&gt; keys_map_;  std::vector&lt;std::pair&lt;Key, score_t&gt;&gt; heap_;&#125;;\n","categories":["programming","cpp"],"tags":["C++","数据结构"]},{"title":"C++ 古怪类型与容器的交互","url":"/2024/04/21/cpp-opaque-type-vs-container/","content":"C++允许定义禁止复制和移动的类型，例如：\nclass OpaqueData &#123; public:  OpaqueData() = default;  ~OpaqueData() = default;  // Disable copy  OpaqueData(const OpaqueData &amp;) = delete;  auto operator=(const OpaqueData &amp;) -&gt; OpaqueData &amp; = delete;  // Disable move  OpaqueData(OpaqueData &amp;&amp;) = delete;  auto operator=(OpaqueData &amp;&amp;) -&gt; OpaqueData &amp; = delete;&#125;;\n对于这种类型，容器的很多需要元素移动的操作都是不合法的。因此需要使用 emplace 来在容器内就地构造元素。\n这对于std::list是没问题的，但是对于map/unordered_map是不足够的：\n#include &lt;list&gt;#include &lt;map&gt;using namespace std;auto main() -&gt; int &#123;  std::list&lt;OpaqueData&gt; list;  list.emplace_back();  // ok  std::map&lt;int, OpaqueData&gt; map;  map.emplace(0, &#123;&#125;);  // Error  return 0;&#125;\n因为map/unordered_map需要在一次调用构造两个元素key和value，因此没办法区分哪些参数用于构造key，哪些用于构造value。\n为了解决这个问题，需要使用 std::piecewise_construct 和 std::forward_as_tuple 明确哪些参数构造key/value，将上述代码修改成下面的样子，即可通过编译：\nauto main() -&gt; int &#123;    // ...    map.emplace(std::piecewise_construct, std::forward_as_tuple(0), std::forward_as_tuple());  // Ok    // ...&#125;\n那么这两个东西是什么呢？查看标准库中emplace的代码会发现，这些参数会不断地被forward，最终会原封不动地被用来构造底层元素，而底层元素实际上是std::pair。\n也就是说，这些奇奇怪怪的参数其实是std::pair的一种构造方式，查看std::pair的构造函数证明了这一点：\n/* In stl_pair.h *//// Tag type for piecewise construction of std::pair objects.struct piecewise_construct_t &#123; explicit piecewise_construct_t() = default; &#125;;/// Tag for piecewise construction of std::pair objects.constexpr piecewise_construct_t piecewise_construct = piecewise_construct_t();\n/* In tuple *//// Create a tuple of lvalue or rvalue references to the argumentstemplate&lt;typename... _Elements&gt;constexpr tuple&lt;_Elements&amp;&amp;...&gt; forward_as_tuple(_Elements&amp;&amp;... __args) noexcept &#123;    return tuple&lt;_Elements&amp;&amp;...&gt;(std::forward&lt;_Elements&gt;(__args)...);&#125;\n/* In stl_pair.h */template&lt;class _T1, class _T2&gt;template&lt;typename... _Args1, typename... _Args2&gt;_GLIBCXX20_CONSTEXPRinlinepair&lt;_T1, _T2&gt;::pair(piecewise_construct_t, tuple&lt;_Args1...&gt; __first, tuple&lt;_Args2...&gt; __second) : pair(__first, __second, typename _Build_index_tuple&lt;sizeof...(_Args1)&gt;::__type(), typename _Build_index_tuple&lt;sizeof...(_Args2)&gt;::__type())&#123; &#125;\n","categories":["programming","cpp"],"tags":["C++"]},{"title":"C++11（新？）特性之右值引用","url":"/2021/01/12/cpp-right-value/","content":"#右值\n不能放到赋值表达式左侧的值，例如:\n\n字面量 1，&quot;Hello&quot;，'c'\n返回值而非引用的函数调用 foo()\n运算结果 a + b\n\n右值不能被直接赋值给一个非常引用，只能先赋给一个变量再取引用\nint&amp; ref = 9; // error，“invalid initialization of non-const reference of type int&amp; from an rvalue of type int”const int&amp; const_ref = 9; // only allow thisint nine = 9;int&amp; ref = nine; // ok\n上述特性在泛型函数重载时会带来一些问题，这导致我们不得不对有无 const 两种情况分别重载\ntemplate &lt;typename T，typename A1&gt;std::unique_ptr&lt;T&gt; factory(A1&amp; a1)&#123;    return std::unique_ptr&lt;T&gt;(new T(a1));&#125;factory&lt;foo&gt;(5); // error，a1-&gt;int&amp;，can not bind to literal 5\n#右值引用\n顾名思义，就是对右值的引用\n#增加右值引用的好处\n增加了右值引用之后，我们具备了识别和重载右值的能力\n#进一步消除不必要的复制\n在 c++11 之前，临时变量的创建一直是难以处理的问题. 有时这些临时变量可以被编译器优化（例如返回值优化），但是这并不总是可行的，通常这会导致高昂的对象复制成本.\n考虑下面的代码：\n#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;vector&lt;string&gt; make_poem()&#123;    vector&lt;string&gt; lines;    lines.push_back(&quot;Hello&quot;);    lines.push_back(&quot;World&quot;);    return lines; // 1.&#125;int main()&#123;    vector&lt;string&gt; poem;    poem = make_poem(); // 2.&#125;\n理论上这里最多可能发生两次复制操作：\n\n\n返回临时变量\n\n\nvector 赋值\n\n\n其中，第一次的复制操作可能会被编译器优化掉(RVO)，然而第二次复制是不可避免的\n当然我们可以通过其他方法来避免这次复制，比如通过指针或者传递一个已经填充好的 vector\n有了右值引用之后，我们可以就可以对右值进行特殊化处理，从而避免多余的复制(移动构造函数和=运算符重载)，例如\n// copyfoo(foo const&amp; other)&#123;    this-&gt;length = other.length;    this-&gt;ptr = new int[other.length];    copy(other.ptr，other.ptr + other.length，this-&gt;ptr);&#125;// 与之对应的movefoo(foo&amp;&amp; other)&#123;   this-&gt;length = other.length;   this-&gt;ptr = other.ptr;   other.length = 0;   other.ptr = nullptr;&#125;\n#移动语义的引入\n可以显式指定某个资源被移交给另一个函数，自己不再需要\nResource resource;foo(move(resource)); // 不再需要resource，可以全权交给foo函数，将会调用foo(Resource&amp;&amp;)\n对于有些不能被复制的资源(如std::unique_ptr，std::thread等等)，这是很有用的\nstd::string s;std::string another(s);           // calls std::string(const std::string&amp;);std::string more(std::string(s)); // calls std::string(std::string&amp;&amp;);\n#模板的完美转发\n引入右值后，出现了一个新问题，观察下面的代码：\nvoid bar(int&amp; x)  &#123; std::cout &lt;&lt; &quot;左值引用\\n&quot;; &#125;void bar(int&amp;&amp; x) &#123; std::cout &lt;&lt; &quot;右值引用\\n&quot;; &#125;template &lt;typename T&gt;void foo(T t) &#123;    // 函数内部的具名参数变量始终是左值    bar(t);&#125;foo(5); // 5是右值，但是t是左值\n按照 C++ 的定义，当参数被传递到函数内部时，无论外部如何传递，函数内部的参数变量本身一定是具名的，因此被视为左值。那么就导致了：\n\n右值的语义在函数传递过程中被丢失\n右值需要被复制成左值（函数参数），有性能损耗\n\n我们可以通过 std::forward 来实现完美转发，这指的是函数模板可以将自己的参数完好无损地转发给内部调用的其他函数中\ntemplate &lt;typename T&gt;void foo(T&amp;&amp; t) &#123;    // 如果传入参数是左值 U：则 `T = U&amp;`，`typeof(t) = T&amp;&amp; = U&amp; &amp;&amp; = U&amp;`    // 如果传入参数是右值 U&amp;&amp;：则 `T = U`，`typeof(t) = T&amp;&amp; = U&amp;&amp;`    // 函数内部的具名参数变量始终是左值    bar(std::forward&lt;T&gt;(t));&#125;\n#实现\nmove函数: 提醒编译器重载时选择移动构造函数\n利用移动交换两个变量:\nvoid move_swap(Res &amp;a，Res &amp;b)&#123;    Res t = move(a);    a = move(b);    b = move(t);&#125;\nRVO 和 NRVO\n返回值优化（Return value optimization，缩写为 RVO）是 C++的一项编译优化技术。即删除保持函数返回值的临时对象。这可能会省略两次复制构造函数，即使复制构造函数有副作用。\n","categories":["programming","cpp"],"tags":["C++"]},{"title":"C++ 模版函数重载匹配规则","url":"/2025/03/03/cpp-template-matching/","content":"#1. 非模板函数优先\n如果有非模版函数，优先调用非模版函数。\nvoid foo(int x) &#123;&#125;          // 非模板函数template &lt;typename T&gt; void foo(T x) &#123;&#125;  // 模板函数foo(5);  // 调用非模板函数 void foo(int)\n#2. 更特化的模板优先\n如果多个模板可行，编译器选择更特化（更具体）的模板版本。\ntemplate &lt;typename T&gt; void foo(T x) &#123;&#125;      // 通用模板template &lt;typename T&gt; void foo(T* x) &#123;&#125;     // 更特化的指针版本int x = 0;foo(&amp;x);  // 调用指针版本的 f&lt;int*&gt;(int*)\n#3. 精确匹配优于类型转换\n若参数匹配需要隐式类型转换，模板生成的精确匹配版本优先于需要转换的非模板函数。\nvoid foo(double x) &#123;&#125;        // 非模板函数，需要 int→double 转换template &lt;typename T&gt; void foo(T x) &#123;&#125;  // 模板生成 f&lt;int&gt;(int)foo(5);  // 调用模板函数 f&lt;int&gt;(int)（无需转换）\n#4. 引用和值类型的优先级\n引用类型（尤其是 T&amp;&amp; 万能引用）可能影响匹配：\ntemplate &lt;typename T&gt; void foo(T&amp;&amp; x) &#123;&#125;  // 万能引用void foo(int x) &#123;&#125;          // 非模板函数foo(5);    // 调用非模板函数（精确匹配）foo(x);    // x 是 int 变量，调用模板函数 f&lt;int&amp;&gt;(int&amp;)（更匹配左值）\n#5. 若存在多个同样好的匹配，编译器报错：\ntemplate &lt;typename T&gt; void f(T x) &#123;&#125;template &lt;typename T&gt; void f(T* x) &#123;&#125;int x = 0;f(&amp;x);  // 歧义：T=int*（通用模板） vs. T=int（指针模板）\n#6. 可变参数模板优先级最低\ntemplate &lt;typename... Args&gt; void f(Args... args) &#123;&#125;  // 最低优先级template &lt;typename T&gt; void f(T x) &#123;&#125;f(5);  // 调用非可变参数模板 f&lt;int&gt;(int)\n","categories":["programming","cpp"],"tags":["C++"]},{"title":"C++ Type Traits 类型特征","url":"/2025/07/18/cpp-type-traits/","content":"#类型修饰类\n\n\nstd::add_const_t&lt;T&gt;: 为类型 T 添加 const 修饰符。\n\n\nstd::add_volatile_t&lt;T&gt;: 为类型 T 添加 volatile 修饰符。\n\n\nstd::add_cv_t&lt;T&gt;: 为类型 T 添加 const 和 volatile 修饰符。\n\n\nstd::remove_const_t&lt;T&gt;: 移除类型 T 的 const 修饰符。\n\n\nstd::remove_volatile_t&lt;T&gt;: 移除类型 T 的 volatile 修饰符。\n\n\nstd::remove_cv_t&lt;T&gt;: 移除类型 T 的 const 和 volatile 修饰符。\n\n\nstd::add_lvalue_reference_t&lt;T&gt;: 为类型 T 添加左值引用修饰符。\n\n\nstd::add_rvalue_reference_t&lt;T&gt;: 为类型 T 添加右值引用修饰符。\n\n\nstd::remove_reference_t&lt;T&gt;: 移除类型 T 的引用修饰符。\n\n\nstd::remove_extent_t&lt;T&gt;: 移除类型 T 的数组维度。\n\n\nstd::remove_all_extents_t&lt;T&gt;: 移除类型 T 的所有数组维度。\n\n\nstd::remove_pointer_t&lt;T&gt;: 移除类型 T 的指针修饰符。\n\n\nstd::add_pointer_t&lt;T&gt;: 为类型 T 添加指针修饰符。\n\n\nstd::make_signed_t&lt;T&gt;: 将类型 T 转换为有符号类型。\n\n\nstd::make_unsigned_t&lt;T&gt;: 将类型 T 转换为无符号类型。\n\n\n#类型推导类\n\nstd::enable_if_t&lt;condition, T&gt;: 如果 condition 为真，则展开为 T，否则模版替换失败。\nstd::conditional_t&lt;condition, T, F&gt;: 如果 condition 为真，则展开为 T，否则展开为 F。\nstd::common_type_t&lt;T, U&gt;: 计算类型 T 和 U 的公共类型。\nstd::underlying_type_t&lt;T&gt;: 获取类型 T 的底层类型。\nstd::result_of_t&lt;T&gt;: 获取类型 T 的结果类型。\nstd::invoke_result_t&lt;T, Args...&gt;: 获取类型 T 的调用结果类型。\nstd::invoke_result_t&lt;T, Args...&gt;: 获取类型 T 的调用结果类型。\nstd::decay_t\n\n#类型判断类\n\n\nstd::is_const_v&lt;T&gt;: 检查给定的类型是否有const 修饰符。\n\n\nstd::is_volatile_v&lt;T&gt;: 检查给定的类型是否有 volatile 修饰符。\n\n\nstd::is_trivial_v&lt;T&gt;: 检查给定的类型是否是平凡类型。\n\n\nstd::is_pod_v&lt;T&gt;: 检查给定的类型是否是 POD 类型。\n\n\nstd::is_literal_type_v&lt;T&gt;: 检查给定的类型是否是字面类型。\n\n\nstd::is_aggregate_v&lt;T&gt;: 检查给定的类型是否是聚合类型。\n\n\nstd::is_trivially_copyable_v&lt;T&gt;: 检查给定的类型是否是可平凡复制的类型。\n\n\nstd::is_standard_layout_v&lt;T&gt;: 检查给定的类型是否是标准布局类型。\n\n\nstd::is_empty_v&lt;T&gt;: 检查给定的类型是否是空类型。\n\n\nstd::is_signed_v&lt;T&gt;: 检查给定的类型是否是有符号类型。\n\n\nstd::is_unsigned_v&lt;T&gt;: 检查给定的类型是否是无符号类型。\n\n\nstd::is_integral_v&lt;T&gt;: 检查给定的类型是否是整数类型。\n\n\nstd::is_floating_point_v&lt;T&gt;: 检查给定的类型是否是浮点类型。\n\n\nstd::is_arithmetic_v&lt;T&gt;: 检查给定的类型是否是算术类型。\n\n\nstd::is_array_v&lt;T&gt;: 检查给定的类型是否是数组类型。\n\n\nstd::is_pointer_v&lt;T&gt;: 检查给定的类型是否是指针类型。\n\n\nstd::is_reference_v&lt;T&gt;: 检查给定的类型是否是引用类型。\n\n\nstd::is_function_v&lt;T&gt;: 检查给定的类型是否是函数类型。\n\n\nstd::is_enum_v&lt;T&gt;: 检查给定的类型是否是枚举类型。\n\n\nstd::is_union_v&lt;T&gt;: 检查给定的类型是否是联合类型。\n\n\nstd::is_class_v&lt;T&gt;: 检查给定的类型是否是类类型。\n\n\nstd::is_polymorphic&lt;T&gt;: 检查给定的类型是否是多态类型。\n\n\nstd::is_final_v&lt;T&gt;: 检查给定的类型是否是最终类型。\n\n\nstd::is_abstract_v&lt;T&gt;: 检查给定的类型是否是抽象类型。\n\n\nstd::is_same_v&lt;T, U&gt;: 检查类型 T 和 U 是否相同。\n\n\nstd::is_base_of_v&lt;Base, Derived&gt;: 检查类型 Derived 是否是类型 Base 的派生类。\n\n\nstd::is_convertible_v&lt;From, To&gt;: 检查类型 From 是否可以转换为类型 To。\n\n\nstd::is_invocable_v&lt;T, Args...&gt;: 检查类型 T 是否可以被调用。\n\n\nstd::is_nothrow_invocable_v&lt;T, Args...&gt;: 检查类型 T 是否可以被调用，且不会抛出异常。\n\n\nstd::is_invocable_r_v&lt;Ret, T, Args...&gt;: 检查类型 T 是否可以被调用，且返回类型为 Ret。\n\n\nstd::is_nothrow_invocable_r_v&lt;Ret, T, Args...&gt;: 检查类型 T 是否可以被调用，且返回类型为 Ret，且不会抛出异常。\n\n\n#条件运算类\n\nstd::conjunction_v&lt;...&gt;: 检查所有给定的类型特征是否为真。\nstd::disjunction_v&lt;...&gt;: 检查至少一个给定的类型特征是否为真。\nstd::negation_v&lt;...&gt;: 检查给定的类型特征是否为假。\n\n","categories":["programming","cpp"],"tags":["C++"]},{"title":"folly::Future 简介与使用","url":"/2025/10/19/folly-future/","content":"#什么是 folly::Future？\nfolly::Future 是 Meta 开源的 C++ 库 Folly 中提供的一个异步编程工具。它类似于其他语言中的 Future 或 Promise 概念，用于表示一个可能在未来某个时间点完成的异步操作的结果。通过使用 folly::Future，开发者可以更方便地处理异步任务，避免回调地狱，提高代码的可读性和维护性。\nfolly::Future 相对于标准库中的 std::future，提供了更多的功能和更灵活的接口，例如链式调用、错误处理等。\n#基本用法\n#1. 创建 Future\n#通过 Promise 创建 Future\nfolly::Future&lt;int&gt; asyncOperation() &#123;    folly::Promise&lt;int&gt; promise;    auto future = promise.getFuture();    // 模拟异步操作    std::thread([p = std::move(promise)]() mutable &#123;        std::this_thread::sleep_for(std::chrono::seconds(1));        if (rand() % 2 == 0) &#123;            p.setValue(42); // 设置结果        &#125; else &#123;            p.setException(std::domain_error(&quot;Simulated error&quot;)); // 设置异常        &#125;    &#125;).detach();    return future;&#125;\n#创建已完成的 Future\nfolly::Future&lt;int&gt; completedFuture() &#123;    return folly::makeFuture(100); // 立即返回一个已完成的 Future&#125;folly::Future&lt;int&gt; failedFuture() &#123;    return folly::makeFuture&lt;int&gt;(std::runtime_error(&quot;Error occurred&quot;)); // 返回一个失败的 Future&#125;\n#2. 等待 Future\n#阻塞等待 Future 完成并获取结果\nvoid waitForFutureResult() &#123;    auto fut = asyncOperation();    try &#123;        int result = std::move(fut).get(); // 阻塞等待结果        std::cout &lt;&lt; &quot;Future completed with value: &quot; &lt;&lt; result &lt;&lt; std::endl;    &#125; catch (const std::exception&amp; e) &#123;        std::cerr &lt;&lt; &quot;Future failed with error: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;    &#125;&#125;\n#阻塞等待 Future 完成\nvoid waitForFutureFinish() &#123;    auto fut = asyncOperation();    try &#123;        std::move(fut).wait(); // 阻塞等待完成        std::cout &lt;&lt; &quot;Future completed&quot; &lt;&lt; std::endl;    &#125; catch (const std::exception&amp; e) &#123;        std::cerr &lt;&lt; &quot;Future failed with error: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;    &#125;&#125;\n#3. 回调函数\n#在 Future 上追加回调函数\nfolly::Future&lt;int&gt; registerCallback() &#123;    auto fut = asyncOperation();    auto fut2 = std::move(fut)                    .thenValue([](int value) &#123;                        std::cout &lt;&lt; &quot;Future completed with value: &quot; &lt;&lt; value &lt;&lt; std::endl;                        return value * 2;                    &#125;)                    .thenError&lt;std::domain_error&gt;([](std::domain_error &amp;&amp;e) &#123;                        std::cerr &lt;&lt; &quot;Future failed with domain_error: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;                        return -1;                    &#125;)                    .thenError&lt;std::exception&gt;([](std::exception &amp;&amp;e) &#123;                        std::cerr &lt;&lt; &quot;Future failed with exception: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;                        return -1;                    &#125;);    return fut2;&#125;\n追加多个回调函数，实现链式调用\nvoid chainFutures() &#123;    asyncOperation()        .thenValue([](int value) &#123;            return value * 2;  // 返回新的值        &#125;)        .thenValue([](int value) &#123;            return std::to_string(value);  // 转换为字符串        &#125;)        .thenValue([](const std::string &amp;str) &#123;  //            std::cout &lt;&lt; &quot;Final result: &quot; &lt;&lt; str &lt;&lt; std::endl;        &#125;)        .wait();&#125;\n#控制回调函数的执行线程\nvoid futureWithExecutor() &#123;    auto main_executor = folly::getGlobalCPUExecutor().get();    folly::CPUThreadPoolExecutor pool_executor(4);    asyncOperation()        .via(main_executor)  // 指定后续callback在默认执行器上运行        .thenValue([](int value) &#123;            std::cout &lt;&lt; &quot;Callback running on main_executor thread, value: &quot; &lt;&lt; value &lt;&lt; std::endl;            return value + 1;        &#125;)        .via(&amp;pool_executor)  // 指定后续callback在pool_executor上运行        .thenValue([](int value) &#123;            std::cout &lt;&lt; &quot;Callback running on pool_executor thread, value: &quot; &lt;&lt; value &lt;&lt; std::endl;            return value + 1;        &#125;)        .wait();&#125;\n#4. 组合多个 Future\n#等待多个 Future 完成\nvoid combineFutures() &#123;    auto fut1 = asyncOperation();    auto fut2 = completedFuture();    auto fut3 = failedFuture();    auto combinedFuture =        folly::collectAll(std::array&lt;folly::Future&lt;int&gt;, 3&gt;&#123;std::move(fut1), std::move(fut2), std::move(fut3)&#125;)            .via(folly::getGlobalCPUExecutor().get())            .thenValue([](auto &amp;&amp;results) &#123;                int sum = 0;                for (folly::Try&lt;int&gt; &amp;result : results) &#123;                    if (result.hasValue()) &#123;                        sum += result.value();                    &#125;                    if (result.hasException()) &#123;                        std::cerr &lt;&lt; &quot;One of the futures failed: &quot; &lt;&lt; result.exception().what() &lt;&lt; std::endl;                    &#125;                &#125;                return sum;            &#125;)            .thenValue([](int sum) &#123;                std::cout &lt;&lt; &quot;Sum of successful futures: &quot; &lt;&lt; sum &lt;&lt; std::endl;                return sum;            &#125;);    combinedFuture.wait();&#125;\n#等待第一个完成的 Future\nvoid raceFutures() &#123;    auto fut1 = asyncOperation();    auto fut2 = completedFuture();    auto fut3 = failedFuture();    auto raceFuture =        folly::collectAny(std::array&lt;folly::Future&lt;int&gt;, 3&gt;&#123;std::move(fut1), std::move(fut2), std::move(fut3)&#125;)            .via(folly::getGlobalCPUExecutor().get())            .thenValue([](std::pair&lt;size_t, folly::Try&lt;int&gt;&gt; result) &#123;                if (result.second.hasValue()) &#123;                    std::cout &lt;&lt; &quot;First completed future value: &quot; &lt;&lt; result.second.value() &lt;&lt; std::endl;                &#125; else if (result.second.hasException()) &#123;                    std::cerr &lt;&lt; &quot;First completed future failed: &quot; &lt;&lt; result.second.exception().what() &lt;&lt; std::endl;                &#125;                return result;            &#125;);    raceFuture.wait();    // First completed future value: 100    auto fut4 = failedFuture();    auto raceFuture2 = folly::collectAny(std::array&lt;folly::Future&lt;int&gt;, 1&gt;&#123;std::move(fut4)&#125;)                           .via(folly::getGlobalCPUExecutor().get())                           .thenValue([](std::pair&lt;size_t, folly::Try&lt;int&gt;&gt; result) &#123;                               if (result.second.hasValue()) &#123;                                   std::cout &lt;&lt; &quot;First completed future value: &quot; &lt;&lt; result.second.value() &lt;&lt; std::endl;                               &#125; else if (result.second.hasException()) &#123;                                   std::cerr &lt;&lt; &quot;First completed future failed: &quot; &lt;&lt; result.second.exception().what()                                             &lt;&lt; std::endl;                               &#125;                               return result;                           &#125;);    raceFuture2.wait();    // First completed future failed: std::runtime_error: Error occurred&#125;\n#收集成功的 Future 结果\nvoid collectSuccessfulFutures() &#123;    auto fut1 = asyncOperation();    auto fut2 = completedFuture();    auto fut3 = failedFuture();    auto collectedFuture =        folly::collect(std::array&lt;folly::Future&lt;int&gt;, 3&gt;&#123;std::move(fut1), std::move(fut2), std::move(fut3)&#125;)            .via(folly::getGlobalCPUExecutor().get())            .thenValue([](std::vector&lt;int&gt; results) &#123;                int sum = 0;                for (int value : results) &#123;                    sum += value;                &#125;                std::cout &lt;&lt; &quot;Sum of successful futures: &quot; &lt;&lt; sum &lt;&lt; std::endl;                return sum;            &#125;);    collectedFuture.wait();&#125;\n#5. 复杂控制流\n#循环重试直到成功\nvoid retryUntilSuccess(std::function&lt;folly::Future&lt;int&gt;()&gt; operation, int maxRetries) &#123;    std::function&lt;folly::Future&lt;int&gt;(int)&gt; attempt = [&amp;](int retriesLeft) -&gt; folly::Future&lt;int&gt; &#123;        return operation().thenError&lt;std::exception&gt;([&amp;](std::exception &amp;&amp;e) &#123;            if (retriesLeft &gt; 0) &#123;                std::cout &lt;&lt; &quot;Operation failed: &quot; &lt;&lt; e.what() &lt;&lt; &quot;, retries left: &quot; &lt;&lt; retriesLeft &lt;&lt; std::endl;                return attempt(retriesLeft - 1);            &#125; else &#123;                std::cerr &lt;&lt; &quot;Operation failed after max retries: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;                throw;  // 重新抛出异常            &#125;        &#125;);    &#125;;    attempt(maxRetries).wait();&#125;\n#条件循环执行异步操作\nvoid useWhileDo() &#123;    using namespace std::chrono_literals;    const auto start = std::chrono::steady_clock::now();    std::atomic&lt;bool&gt; done&#123;false&#125;;    folly::whileDo([&amp;]() &#123; return !done &amp;&amp; std::chrono::steady_clock::now() - start &lt; 5s; &#125;,                   [&amp;]() &#123;                       return asyncOperation()                           .thenValue([&amp;](int value) &#123;                               done = true;                               std::cout &lt;&lt; &quot;Success value: &quot; &lt;&lt; value &lt;&lt; std::endl                                         &lt;&lt; &quot;Elapsed time: &quot;                                         &lt;&lt; std::chrono::duration_cast&lt;std::chrono::seconds&gt;(                                                std::chrono::steady_clock::now() - start)                                                .count()                                         &lt;&lt; &quot; seconds&quot; &lt;&lt; std::endl;                           &#125;)                           .thenError&lt;std::exception&gt;([&amp;](std::exception &amp;&amp;e) &#123;                               std::cerr &lt;&lt; &quot;Operation failed: &quot; &lt;&lt; e.what() &lt;&lt; std::endl;                               return folly::futures::sleep(500ms);                           &#125;);                   &#125;)        .then([&amp;](auto &amp;&amp;) &#123;            if (!done) &#123;                std::cout &lt;&lt; &quot;Operation did not succeed within timeout 5 seconds.&quot; &lt;&lt; std::endl;            &#125;        &#125;)        .wait();&#125;\n","categories":["programming","cpp"],"tags":["C++","folly","异步编程"]},{"title":"Bash 基本使用总结","url":"/2021/02/11/bash/","content":"#Bash 基本使用\n#常用快捷键\n^c   终止当前进程^z   将当前进程挂起到后台^d   退出, 等价于exit^l   清屏^r   搜索历史命令^a   光标移到开头^e   光标移到结尾^u   删除光标前所有内容^k   删除光标后所有内容\n#常用通配符\n*                       匹配前面的内容任意次?                       匹配前面的内容0-1次[list]                  list中任意一个字符&#123;string1,string2,...&#125;   string1,string2或者更多字符串\n#引号\n\n双引号&quot;&quot; 把引号中的内容作为一个整体, 允许通过$符号来引用其他变量值\n单引号'' 把引号中的内容作为一个整体, 禁止通过$符号来引用其他变量值, 其中的 shell 特殊符号都视为普通字符\n反引号 与$()相同, 先执行引号中的内容, 不能嵌套\n\n#shell 脚本\n#结构\n\nHashBang\n\n#!/usr/bin/env bash\n\n命令\n\n$ echo &#x27;Hello World&#x27;\n#执行脚本\n\n标准方式\n\n$ chmod +x script.sh$ ./script.sh\n\n非标准方式\n\n\n直接指定解释器执行\n\n$ bash script.sh# -x 显示脚本的执行过程, 用于debug# -n 检查语法是否有问题\n\n使用 source 命令执行\n\n$ source script.sh\n#变量\n\n变量名不能由数字开头\n区分大小写\n\n#读取用户输入\n语法：read [选项] 变量名\n\n-p 提示信息\n-n 输入长度\n-s 不回显\n-t 超时时间, 单位 s\n\n#内置变量\n$$      当前进程号$?      上一条命令的返回值$_      上一条命令的最后一个参数$0      当前执行的程序（脚本文件）名$&#123;x&#125;    第x个参数$*      所有参数$@      所有参数$##      执行时所带的参数个数$!      后台运行的最后一个进程的进程号\n#简单四则运算\n\n\n\n表达式\n举例\n\n\n\n\n$(( ))\necho $((1+1))\n\n\n$[ ]\necho $[10-5]\n\n\nexpr\nexpr 10 / 5\n\n\n\n","categories":["programming","linux"],"tags":["linux","shell"]},{"title":"gdb 常用命令","url":"/2025/11/02/gdb/","content":"#gdb 基本使用\n#1. 启动 gdb\n\n\n调试可执行文件：\n  gdb &lt;可执行文件&gt;\n\n\n调试正在运行的进程：\n  gdb -p &lt;进程ID&gt;\n\n\n调试 core 文件：\n  gdb &lt;可执行文件&gt; &lt;core 文件&gt;\n\n\n#2. 设置调试环境\n\n\n查看动态库信息：\n  info sharedlibraryinfo share\n\n\n设置动态库搜索路径：\n  set sysroot &lt;库文件根目录&gt;  # 常用于交叉编译环境set solib-search-path &lt;库文件目录&gt;\n实际找动态库时会在 sysroot 下的 solib-search-path 中查找。\n\n\n设置断点时忽略动态库：\n  set breakpoint pending on\n\n\n设置分页显示：\n  set pagination on\n\n\n#3. 基本控制流\n\nrun 或 r：启动程序的执行。\nstart：从程序的入口点开始执行, 在 main 函数处暂停。\ncontinue 或 c：继续执行程序直到下一个断点。\nfinish 或 fin：执行当前函数直到返回。\nuntil &lt;位置&gt;：继续执行直到指定位置。\nquit 或 q：退出 gdb。\n\n单步执行：\n\nnext 或 n：单步执行，不进入函数调用。\nnexti 或 ni：单步执行汇编指令，不进入函数调用。\nstep 或 s：单步执行，进入函数调用。\nstepi 或 si：单步执行汇编指令，进入函数调用。\n\n查看控制流状态：\n\nbacktrace 或 bt：显示当前调用栈。\nframe &lt;帧编号&gt; 或 f &lt;帧编号&gt;：切换到指定的栈帧。\n\n多线程调试：\n\ninfo threads 或 i t：列出所有线程。\nthread &lt;线程编号&gt; 或 t &lt;线程编号&gt;：切换到指定线程。\nthread apply all &lt;命令&gt;：对所有线程执行指定命令。\n\n#4. 断点和观察点\n\nbreak &lt;位置&gt; 或 b &lt;位置&gt;：设置断点。\nbreak &lt;位置&gt; if &lt;条件&gt; 或 b &lt;位置&gt; if &lt;条件&gt;：设置条件断点。\ninfo breakpoints 或 i b：列出所有断点。\ndelete &lt;断点编号&gt; 或 d &lt;断点编号&gt;：删除指定断点。\nenable &lt;断点编号&gt; 或 en &lt;断点编号&gt;：启用指定断点。\ndisable &lt;断点编号&gt; 或 dis &lt;断点编号&gt;：禁用指定断点。\nclear &lt;位置&gt;：清除指定位置的断点。\n\n观察点：\n\nwatch &lt;变量名&gt; 或 w &lt;变量名&gt;：设置观察点，当变量的值发生变化时暂停程序。\nrwatch &lt;变量名&gt; 或 rw &lt;变量名&gt;：设置读观察点，当变量被读取时暂停程序。\nawatch &lt;变量名&gt; 或 aw &lt;变量名&gt;：设置读写观察点，当变量被读取或修改时暂停程序。\ninfo watchpoints 或 i w：列出所有观察点。\ndelete &lt;观察点编号&gt; 或 d &lt;观察点编号&gt;：删除指定观察点。\ndisable &lt;观察点编号&gt; 或 dis &lt;观察点编号&gt;：禁用指定观察点。\nenable &lt;观察点编号&gt; 或 en &lt;观察点编号&gt;：启用指定观察点。\n\n#5. 变量和内存\n\nprint &lt;变量名&gt; 或 p &lt;变量名&gt;：打印变量的值。\nprint &lt;类型&gt; &lt;变量名&gt; 或 p &lt;类型&gt; &lt;变量名&gt;：以指定类型打印变量的值。\nset &lt;变量名&gt; = &lt;值&gt;：修改变量的值。\nexamine 或 x/&lt;格式&gt; &lt;地址&gt;：以指定格式查看内存内容。\n\n查看变量信息：\n\ninfo locals 或 i l：显示当前栈帧的局部变量。\ninfo args 或 i a：显示当前栈帧的函数参数。\n\n命中断点时显示变量：\n\ndisplay &lt;变量名&gt; 或 disp &lt;变量名&gt;：每次暂停时自动显示变量的值。\nundisplay &lt;显示编号&gt; 或 undisp &lt;显示编号&gt;：取消自动显示变量。\n\n调整print命令的输出格式：\n\nset print pretty &lt;on|off&gt;：启用漂亮打印。\nset print elements &lt;数量&gt;：设置打印数组时的元素数量限制。\n\nExamine命令的格式选项：\n\nx/&lt;数量&gt;&lt;格式&gt;&lt;大小&gt; &lt;地址&gt;：查看内存内容。\n\n&lt;数量&gt;：要查看的单元数。\n&lt;格式&gt;：显示格式，如 x（十六进制）、d（十进制）、o（八进制）、t（二进制）、c（字符）、f（浮点数）。\n&lt;大小&gt;：单元大小，如 b（字节）、h（双字节）、w（四字节）、g（八字节）。\n\n\n\n#6. 源码导航\n\n\n设置源代码路径：\n  set directories &lt;源代码目录&gt;show directories# 对源码路径进行一个 replace(a, b) 操作set substitute-path &lt;a&gt; &lt;b&gt;\n\n\nlist 或 l：显示当前执行位置的代码。\n\n\nlist &lt;行号&gt; 或 l &lt;行号&gt;：显示指定行号附近的代码。\n\n\nlist &lt;函数名&gt; 或 l &lt;函数名&gt;：显示指定函数的代码。\n\n\nlist &lt;起始行号&gt;,&lt;结束行号&gt; 或 l &lt;起始行号&gt;,&lt;结束行号&gt;：显示指定范围的代码。\n\n\n#7. 信号处理\n\ninfo signals 或 i s：显示当前信号处理设置。\nhandle &lt;信号&gt; &lt;stop|nostop&gt; &lt;print|noprint&gt; &lt;pass|nopass&gt;：设置对指定信号的处理方式。\nsignal &lt;信号&gt;：向被调试程序发送指定信号。\n\n#8. core dump\n\nset core-file &lt;文件名&gt;：指定 core dump 文件用于调试。\ngenerate-core-file &lt;文件名&gt;：生成 core dump 文件。\n\n通过 kill -SIGSEGV &lt;进程ID&gt; 也可以让程序产生 core dump 文件\n通过 gcore &lt;进程ID&gt; 也可以生成 core dump 文件\n#9. 脚本和自动化\n\n\n启动 gdb 时执行初始命令：\n  gdb -x &lt;脚本文件&gt; ...gdb -ex &quot;&lt;命令1&gt;&quot; -ex &quot;&lt;命令2&gt;&quot; ...\n\n\n执行初始命令后自动退出 gdb：\n  gdb --batch ...\n\n\n","categories":["programming","linux"],"tags":["gdb"]},{"title":"How to compile the Linux kernel","url":"/2022/01/08/how-to-compile-linux-kernel/","content":"\n#访问 https://www.kernel.org/ 查看最新的内核版本，获取下载链接\n#下载并解压到当前目录\nwget -qO- https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.15.13.tar.xz | tar Jxvf -cd linux-5.15.13\n#编译配置\ncp /boot/config-`uname -r` .configmake menuconfig\n#开始编译\nmake -j $(nproc)\n#安装到系统中\nsudo make INSTALL_MOD_STRIP=1 modules_installsudo make headers_installsudo make install\n","categories":["programming","linux"],"tags":["Linux"]},{"title":"gettext 国际化使用总结","url":"/2025/11/09/i18n-gettext/","content":"#gettext 是什么\ngettext 是 GNU 的国际化与本地化 (i18n) 函数库。它常被用于编写多语言程序。\nGNU gettext 支持 C、C++、Objective-C、Pascal/Object Pascal、sh 脚本、bash 脚本、Python、GNU CLISP，Emacs Lisp、ibrep、GNU Smalltalk、Java、GNU awk、wxWidgets（通过 wxLocale 类）、YCP（YaST2 语言）、Tcl、Perl、PHP、Pike、Ruby 以及 R。\n具体来说，GNU gettext 是一组工具，其他软件包可以在其中生成国际化消息。这些工具包括：\n\n一套关于如何编写程序以支持消息目录的约定: mo、po 和 pot 文件格式。\n消息目录本身的目录和文件命名组织: locale 目录结构。\n一个支持检索翻译消息的运行时库: gettext 库。\n一些独立的程序，用于以各种方式处理可翻译字符串集或已翻译字符串: xgettext、msgmerge、msgfmt 等。\n一个支持解析和创建包含翻译消息的文件的库。\nEmacs 的一个特殊模式，它有助于准备这些集合并使其保持最新。\n\n#gettext i18n 使用流程\n\n\n在源代码中绑定语言文件，并使用 gettext 函数获取翻译后的字符串。例如：\n#include &lt;libintl.h&gt;#include &lt;locale.h&gt;#include &lt;stdio.h&gt;#define PACKAGE &quot;myprogram&quot;#define LOCALEDIR &quot;.&quot; // default to &quot;/usr/local/share/locale&quot;#define _(String) gettext(String)int main() &#123;  // 设置当前环境的本地化信息  setlocale(LC_ALL, &quot;&quot;);  // 将 domain 绑定到本地目录 LOCALEDIR  // gettext 将使用路径: LOCALEDIR/language/LC_MESSAGES/PACKAGE.mo 查找 .mo 文件  // 不以前缀 &#x27;/&#x27; 开头时，LOCALEDIR 被认为是相对路径  bindtextdomain(PACKAGE, LOCALEDIR);  // 设置当前使用的 domain  textdomain(PACKAGE);  printf(_(&quot;Hello, World!\\n&quot;));&#125;\nlibintl.h 头文件定义了 gettext 函数。宏 _() 用于标记需要翻译的字符串。\nlibintl 已经包含在 glibc 中，因此链接 glibc 时无需额外链接。\n\n\n使用 xgettext 工具从源代码中提取标记的字符串，生成一个 POT (Portable Object Template) 文件：\nxgettext -k_ -o myprogram.pot myprogram.c\n文件 myprogram.pot 将包含所有需要翻译的字符串。\n# SOME DESCRIPTIVE TITLE.# Copyright (C) YEAR THE PACKAGE&#x27;S COPYRIGHT HOLDER# This file is distributed under the same license as the PACKAGE package.# FIRST AUTHOR &lt;EMAIL@ADDRESS&gt;, YEAR.##, fuzzymsgid &quot;&quot;msgstr &quot;&quot;&quot;Project-Id-Version: PACKAGE VERSION\\n&quot;&quot;Report-Msgid-Bugs-To: \\n&quot;&quot;POT-Creation-Date: 2025-11-09 13:06+0800\\n&quot;&quot;PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n&quot;&quot;Last-Translator: FULL NAME &lt;EMAIL@ADDRESS&gt;\\n&quot;&quot;Language-Team: LANGUAGE &lt;LL@li.org&gt;\\n&quot;&quot;Language: \\n&quot;&quot;MIME-Version: 1.0\\n&quot;&quot;Content-Type: text/plain; charset=CHARSET\\n&quot;&quot;Content-Transfer-Encoding: 8bit\\n&quot;#: myprogram.c:21#, c-formatmsgid &quot;Hello, World!\\n&quot;msgstr &quot;&quot;\n\n\n为需要翻译的目标语言创建/更新对应的 PO (Portable Object) 文件：\n如果是第一次创建某个语言的 PO 文件，可以使用 msginit 工具：\nmsginit -l zh_CN -i myprogram.pot -o zh_CN.po\n如果 PO 文件已经存在（已经有一部分翻译，程序中增加了新的条目），可以使用 msgmerge 工具更新：\nmsgmerge -U zh_CN.po myprogram.pot\n\n\n翻译人员进行翻译，更新 LANG.po 文件中的 msgstr 字段。\n#: myprogram.c:21#, c-formatmsgid &quot;Hello, World!\\n&quot;msgstr &quot;你好，世界！\\n&quot;\n\n\n使用 msgfmt 工具将 PO 文件编译为供程序读取的 MO (Machine Object) 文件：\nmsgfmt zh_CN.po -o zh_CN.mo\nMO 文件是二进制格式，供程序在运行时使用。\n\n\n将生成的 MO 文件安装到系统中，例如：\ninstall -Dm644 zh_CN.mo ./zh_CN/LC_MESSAGES/myprogram.mo\n\n\n运行程序时，设置正确的环境变量，即可切换到对应的语言：\n$ LANG=zh_CN.UTF-8 ./myprogram你好，世界！$ LANG=en ./myprogramHello, World!\n\n\n","categories":["programming","linux"],"tags":["linux","gettext","i18n"]},{"title":"Linux命令行实用工具","url":"/2021/02/09/linux-cmd-utils/","content":"#杂项\n#清屏\n按下 Ctrl+l\n#快速清空文件内容\n利用文件写入\n&gt; filename\n#实用程序\n#cat\n显示文件内容, 如果输入是多个文件也可以用于文件拼接\n\n-n 显示行号\n\n#grep\n行过滤\n\n-n 显示行号\n-i 忽略大小写\n-c 统计结果行数\n-A 显示匹配处后多少行\n-B 显示匹配处前多少行\n-C 显示匹配处前后多少行\n--color=auto 彩色显示\n^ 行开头\n$ 行末尾\n\n#cut\n列截取\n\n-d 分隔符\n-f 分割后取出哪些列\n\n#sort\n对标准输入进行排序, 默认是升序排列\n\n-r 逆序排列, 即按降序排列\n\n\n#uniq\n连续行去重, 即对标准输入中连续重复的行只保留一个\n#tee\n把标准输入写入到标准输出和一个文件中, 即: 双向覆盖重定向(屏幕+文件输出)\n\n-a 双向追加重定向\n\n#diff\n描述怎样改变第一个文件使之与第二个文件匹配\n#patch\n基于 diff 的结果修改一个文件\n#paste\n将两个文件逐行拼接, 与 cut 相反\n\n-d 指定分割符, 默认是 tab\n-s 将结果转置\n\n#tr\n字符转换, 替换和删除. 主要用于删除文件中的控制字符, 或者进行字符转换\n格式:\n$ tr set1 set2 &lt; stdin\n把字符集 1 中的字符替换成字符集 2 中的字符\n例:\n\n小写转大写 tr a-z A-Z &lt; package.json\n\n","categories":["programming","linux"],"tags":["linux","shell"]},{"title":"Linux Kernel 文件子系统","url":"/2024/04/30/linux-kernel-file-subsystem/","content":"","categories":["programming","linux"],"tags":["Linux"]},{"title":"Linux Kernel 中断子系统","url":"/2024/04/16/linux-kernel-interrupt-subsystem/","content":"Linux 将中断处理流程划分成两个部分，一个是 top half，另一个是 bottom half。\n在执行 top half 时是关闭硬件中断的，即系统没办法响应后续的中断事件，因此应该保证 top half 尽可能短。\n把不那么紧急的事情 defer 到 bottom half 去做。\n#bottom half\nbottom half 有多个实现机制\n\n\nBH 最早的实现，已经弃用了\n\n\nTask queue 已经弃用了\n\n\nSoftirq\n运行在 interrupt context\ntop half 中raise_softirq(nr)，每个 cpu 串行执行\n\n\ntasklet\n运行在 interrupt context\n\n\nworkqueue\n运行在 process context\n\n\n#Softirq\nsoftirq 和 hardirq（硬件中断）对应，\nenum&#123;    HI_SOFTIRQ=0,    TIMER_SOFTIRQ,    NET_TX_SOFTIRQ,    NET_RX_SOFTIRQ,    BLOCK_SOFTIRQ,    BLOCK_IOPOLL_SOFTIRQ,    TASKLET_SOFTIRQ,    SCHED_SOFTIRQ,    HRTIMER_SOFTIRQ,    RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */    NR_SOFTIRQS&#125;;\nsoftirq 有三个处理时机\n\n中断上下文 irq_exit() 的时候\n任意 local_bh_enable()\nsoftirqd 内核线程定期执行\n\n#tasklet\nper-cpu 会有一个 tasklet 的链表\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);\n","categories":["programming","linux"],"tags":["Linux","嵌入式"]},{"title":"Linux Kernel 网络子系统","url":"/2024/04/16/linux-kernel-network-subsystem/","content":"#NAPI\nNAPI 机制采用中断和轮询结合的方式收包\nsoftnet_data per-cpu 全局变量，保存网络收包的 backlog\nstruct softnet_data &#123;    struct Qdisc *output_queue;    struct Qdisc **output_queue_tailp;    struct list_head poll_list; //设备轮询列表    struct sk_buff *completion_queue;    struct sk_buff_head process_queue;    /* 统计数据 */    unsigned int processed;    unsigned int time_squeeze;    unsigned int cpu_collision;    unsigned int received_rps;    unsigned dropped; //被丢弃的包的数量    struct sk_buff_head input_pkt_queue; //收包队列    struct napi_struct backlog; //处理积压队列的napi结构&#125;;\nsoftnet_data\n","categories":["programming","linux"],"tags":["计算机网络","Linux"]},{"title":"Minimal Linux 桌面操作系统构建指南","url":"/2025/11/09/minimal-linux/","content":"#引言\n一个完整的 Linux 操作系统包括多少个组件？涉及多少个 deb 包？如何从零开始构建一个最小化的 Linux 系统？本文将带你了解最小化 Linux 系统的构建过程, 涵盖内核, 引导加载程序, 基本工具链和必要的用户空间组件。\n#最小化 Linux 系统组件\n\nLinux Kernel: Linux 内核是操作系统的核心, 负责管理硬件资源和提供系统调用接口。\nlibc &amp; loaders: GNU C 库是 Linux 系统的标准 C 库, 提供基本的系统调用封装和标准库函数。\n\n/lib/x86_64-linux-gnu/libc.so.6\n/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n/lib64/ld-linux-x86-64.so.2 -&gt; ../lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n\n\n/sbin/init: 1 号进程, 第一个用户态进程, 负责启动其他进程和服务, 常见的类型有:\n\nSysVinit\nsystemd\nOpenRC\n\n\nShell: 提供命令行解释器, 常用的有 bash, ash, dash 等。\nlibc-bin: 提供与 C 库相关的二进制文件, 如 ldd, ldconfig 等。\ncoreutils: 提供基本的用户命令行工具, 如 ls, cp, mv, rm 等。\nutil-linux: 提供额外的系统管理工具, 如 su, login, mount, dmesg, fdisk 等。\nprocps-ng: 提供进程管理工具, 如 ps, top, free 等。\ndebianutils: 提供一些 Debian 特有的实用工具, 如 which, run-parts 等。\nnet-tools: 提供传统的网络管理工具, 如 ifconfig, netstat, arp, route 等。\niproute2: 提供网络管理工具, 如 ip, ss, tc 等。\nwget: 用于从网络上下载文件的命令行工具。\n\n#构建一个基本的 rootfs\n#!/usr/bin/env bash# build-rootfs.sh# 构建最小 rootfs（从宿主系统拷贝二进制 + 自动收集依赖库）## 用法:#   sudo ./build-rootfs.sh /path/to/rootfs [mode]#   mode: &quot;glibc&quot; (默认) | &quot;busybox-static&quot;## 说明:#  - 默认 mode=glibc：拷贝指定的 coreutils/util-linux/procps 二进制并收集 ldd 依赖#  - mode=busybox-static：使用单个已编译静态 busybox（用户需在 HOST_BUSYBOX_PATH 提供）#set -euo pipefailROOTFS=&quot;$&#123;1:-./rootfs&#125;&quot;# 可按需调整要拷贝的命令列表（优先用宿主系统的路径）LIBC_BIN_CMDS=( \\  /sbin/ldconfig \\  /usr/bin/catchsegv \\  /usr/bin/getconf \\  /usr/bin/getent \\  /usr/bin/iconv \\  /usr/bin/ldd \\  /usr/bin/locale \\  /usr/bin/localedef \\  /usr/bin/pldd \\  /usr/bin/tzselect \\  /usr/bin/zdump \\  /usr/sbin/iconvconfig \\  /usr/sbin/zic \\)CORE_CMDS=( \\  /bin/cat /bin/tac /usr/bin/nl /usr/bin/od \\  /usr/bin/base32 /usr/bin/base64 /usr/bin/basenc \\  /usr/bin/fmt /usr/bin/pr /usr/bin/fold \\  /usr/bin/head /usr/bin/tail /usr/bin/split /usr/bin/csplit \\  /usr/bin/wc /usr/bin/sum /usr/bin/cksum /usr/bin/md5sum \\  /usr/bin/b2sum /usr/bin/sha1sum /usr/bin/sha224sum \\  /usr/bin/sha256sum /usr/bin/sha384sum /usr/bin/sha512sum \\  /usr/bin/sort /usr/bin/shuf /usr/bin/uniq /usr/bin/comm \\  /usr/bin/ptx /usr/bin/tsort /usr/bin/cut /usr/bin/paste \\  /usr/bin/join /usr/bin/tr /usr/bin/expand /usr/bin/unexpand \\  /bin/ls /usr/bin/dir /usr/bin/vdir /usr/bin/dircolors \\  /bin/cp /usr/bin/dd /usr/bin/install /bin/mv /bin/rm \\  /usr/bin/shred /usr/bin/link /bin/ln /usr/bin/mkdir \\  /usr/bin/mkfifo /usr/bin/mknod /usr/bin/readlink /usr/bin/rmdir \\  /usr/bin/unlink /usr/bin/chown /usr/bin/chgrp /usr/bin/chmod \\  /usr/bin/touch /usr/bin/df /usr/bin/du /usr/bin/stat \\  /usr/bin/sync /usr/bin/truncate /bin/echo /usr/bin/printf \\  /usr/bin/yes /usr/bin/false /usr/bin/true /usr/bin/test \\  /usr/bin/expr /usr/bin/tee /usr/bin/basename /usr/bin/dirname \\  /usr/bin/pathchk /usr/bin/mktemp /usr/bin/realpath /usr/bin/pwd \\  /usr/bin/stty /usr/bin/printenv /usr/bin/tty /usr/bin/id \\  /usr/bin/logname /usr/bin/whoami /usr/bin/groups /usr/bin/users \\  /usr/bin/who /usr/bin/pinky /usr/bin/date /usr/bin/arch \\  /usr/bin/nproc /usr/bin/uname /usr/bin/hostname /usr/bin/hostid \\  /usr/bin/uptime /usr/bin/chcon /usr/bin/runcon /usr/bin/chroot \\  /usr/bin/env /usr/bin/nice /usr/bin/nohup /usr/bin/stdbuf \\  /usr/bin/timeout /usr/bin/kill /usr/bin/sleep /usr/bin/factor \\  /usr/bin/numfmt /usr/bin/seq \\  /usr/bin/\\[ /usr/bin/test /usr/bin/expr \\)UTIL_LINUX_CMDS=( \\  /sbin/mount /sbin/umount /usr/bin/findmnt /usr/bin/mountpoint \\  /sbin/losetup /usr/sbin/blkid /usr/bin/lsblk /sbin/fdisk \\  /usr/sbin/sfdisk /usr/sbin/partx /usr/bin/swapon /usr/bin/swapoff \\  /usr/sbin/mkswap /usr/bin/dmesg /sbin/hwclock /usr/bin/logger \\  /usr/bin/wall /usr/bin/write /sbin/agetty /usr/bin/login \\  /usr/bin/uuidgen /usr/sbin/uuidd /usr/bin/rename /usr/bin/col \\  /usr/bin/colcrt /usr/bin/colrm /usr/bin/column \\  /usr/bin/hexdump /usr/bin/hd /usr/bin/look /usr/bin/ul \\  /usr/bin/chfn /usr/bin/chsh /usr/bin/chrt /usr/bin/taskset \\  /usr/bin/lslogins /usr/bin/loginctl /usr/bin/fallocate \\  /usr/bin/blockdev /usr/bin/mkfs /usr/bin/mkfs.bfs \\  /usr/sbin/ctrlaltdel /usr/bin/su \\)PROCPS_CMDS=( /usr/bin/ps /usr/bin/top /usr/bin/free /usr/bin/uptime )NET_TOOLS_CMDS=( /sbin/ifconfig /sbin/ip /usr/sbin/ss /usr/sbin/netstat /usr/sbin/route )SHELL_CANDIDATES=( /bin/bash /bin/sh /bin/ash /bin/dash )NET_UTILS=( /sbin/ifconfig /sbin/ip /usr/sbin/ss /usr/sbin/netstat /usr/sbin/route )EXTRA=( /bin/hostname /usr/bin/lsof /usr/bin/wget )# Busybox (仅当 mode=busybox-static 且宿主提供时使用)HOST_BUSYBOX_PATH=&quot;$&#123;HOST_BUSYBOX_PATH:-/usr/local/bin/busybox&#125;&quot;  # 可覆盖# helpersinfo()&#123; printf &#x27;\\e[1;32m[INFO]\\e[0m %s\\n&#x27; &quot;$*&quot;; &#125;warn()&#123; printf &#x27;\\e[1;33m[WARN]\\e[0m %s\\n&#x27; &quot;$*&quot;; &#125;err()&#123; printf &#x27;\\e[1;31m[ERROR]\\e[0m %s\\n&#x27; &quot;$*&quot;; exit 1; &#125;mkdir_p() &#123; mkdir -p -- &quot;$@&quot;; &#125;# gather list of binaries to copy (resolve real paths)collect_bins() &#123;  local -n out=$1  out=()  for f in &quot;$&#123;LIBC_BIN_CMDS[@]&#125;&quot; &quot;$&#123;CORE_CMDS[@]&#125;&quot; &quot;$&#123;UTIL_LINUX_CMDS[@]&#125;&quot; &quot;$&#123;PROCPS_CMDS[@]&#125;&quot; &quot;$&#123;NET_TOOLS_CMDS[@]&#125;&quot; &quot;$&#123;EXTRA[@]&#125;&quot;; do    if [ -x &quot;$f&quot; ]; then      out+=(&quot;$f&quot;)    else      # try which      cmdname=&quot;$(basename &quot;$f&quot;)&quot;      path=&quot;$(command -v &quot;$cmdname&quot; 2&gt;/dev/null || true)&quot;      if [ -n &quot;$path&quot; ]; then        out+=(&quot;$path&quot;)      fi    fi  done  # ensure we have a shell and an init candidate  SHELL_BIN=&quot;&quot;  for s in &quot;$&#123;SHELL_CANDIDATES[@]&#125;&quot;; do    if [ -x &quot;$s&quot; ]; then      SHELL_BIN=&quot;$s&quot;      break    fi  done  if [ -n &quot;$SHELL_BIN&quot; ]; then    out+=(&quot;$SHELL_BIN&quot;)  else    warn &quot;No shell found on host; you&#x27;ll need to provide one (bash/sh) or use busybox-static mode.&quot;  fi&#125;# copy binary and its parent dir structurecopy_bin() &#123;  local src=&quot;$1&quot;  local dstroot=&quot;$2&quot;  if [ ! -f &quot;$src&quot; ]; then    warn &quot;binary not found: $src&quot;    return  fi  local dst=&quot;$dstroot$&#123;src&#125;&quot;  mkdir_p &quot;$(dirname &quot;$dst&quot;)&quot;  cp -a -- &quot;$src&quot; &quot;$dst&quot;  # preserve permissions  chmod --reference=&quot;$src&quot; &quot;$dst&quot;&#125;# collect shared libs via ldd and copy themcopy_libs_for_bin() &#123;  local bin=&quot;$1&quot;  local dstroot=&quot;$2&quot;  # ldd may fail for statically linked; handle gracefully  if ldd_output=&quot;$(ldd &quot;$bin&quot; 2&gt;/dev/null)&quot; ; then    while IFS= read -r line; do      # lines like: linux-vdso.so.1 (0x00007fff...)      # or: libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f...)      # or: /lib64/ld-linux-x86-64.so.2 (0x...)      libpath=&quot;&quot;      if [[ &quot;$line&quot; =~ &quot;=&gt;&quot; ]]; then        libpath=&quot;$(echo &quot;$line&quot; | awk &#x27;&#123;for(i=1;i&lt;=NF;i++)&#123; if ($i == &quot;=&gt;&quot;)&#123; print $(i+1); break &#125;&#125;&#125;&#x27;)&quot;      else        # fallback: first token that starts with /        libpath=&quot;$(echo &quot;$line&quot; | awk &#x27;&#123;for(i=1;i&lt;=NF;i++)&#123; if ($i ~ /^\\//)&#123; print $i; break &#125;&#125;&#125;&#x27;)&quot;      fi      if [ -n &quot;$libpath&quot; ] &amp;&amp; [ -f &quot;$libpath&quot; ]; then        # copy into same relative path under rootfs        mkdir_p &quot;$dstroot$(dirname &quot;$libpath&quot;)&quot;        cp -a -- &quot;$libpath&quot; &quot;$dstroot$libpath&quot;        # if it&#x27;s a symlink, also copy the target        if [ -L &quot;$libpath&quot; ]; then          target=&quot;$(readlink &quot;$libpath&quot;)&quot;          # handle relative symlinks          if [[ &quot;$target&quot; != /* ]]; then            target=&quot;$(dirname &quot;$libpath&quot;)/$target&quot;          fi          if [ -f &quot;$target&quot; ]; then            mkdir -p &quot;$dstroot$(dirname &quot;$target&quot;)&quot;            cp -a -- &quot;$target&quot; &quot;$dstroot$target&quot;          fi        fi      fi    done &lt;&lt;&lt; &quot;$ldd_output&quot;  else    # not a dynamic ELF or ldd failed    :  fi&#125;create_basic_tree() &#123;  local r=&quot;$1&quot;  info &quot;Creating base directories under $r&quot;  mkdir_p &quot;$r&quot;/&#123;bin,sbin,etc,proc,sys,dev,lib,lib64,usr,usr/bin,usr/sbin,tmp,var,root,home&#125;  chmod 1777 &quot;$r/tmp&quot;&#125;write_etc_files() &#123;  local r=&quot;$1&quot;  info &quot;Writing /etc/passwd, /etc/group, /etc/inittab, /etc/fstab, /etc/profile&quot;  cat &gt; &quot;$r/etc/passwd&quot; &lt;&lt;&#x27;EOF&#x27;root:x:0:0:root:/root:/bin/shEOF  cat &gt; &quot;$r/etc/group&quot; &lt;&lt;&#x27;EOF&#x27;root:x:0:EOF  # minimal shadow (empty password; you can set hashed passwd if needed)  cat &gt; &quot;$r/etc/shadow&quot; &lt;&lt;&#x27;EOF&#x27;root:*:18500:0:99999:7:::EOF  chmod 600 &quot;$r/etc/shadow&quot;  # fstab - auto mount proc/sys/devtmpfs if using our init scripts  cat &gt; &quot;$r/etc/fstab&quot; &lt;&lt;&#x27;EOF&#x27;proc    /proc   proc    defaults    0 0sysfs   /sys    sysfs   defaults    0 0devtmpfs /dev   devtmpfs defaults    0 0EOF  # simple profile  cat &gt; &quot;$r/etc/profile&quot; &lt;&lt;&#x27;EOF&#x27;export PATH=/bin:/sbin:/usr/bin:/usr/sbinexport HOME=/rootexport TERM=$&#123;TERM:-vt100&#125;EOF  # simple /etc/inittab for busybox-style init (if using busybox init)  cat &gt; &quot;$r/etc/inittab&quot; &lt;&lt;&#x27;EOF&#x27;::sysinit:/etc/init.d/rcSttyS0::respawn:/bin/sh::ctrlaltdel:/sbin/rebootEOF&#125;write_init_script() &#123;  local r=&quot;$1&quot;  info &quot;Writing simple /etc/init.d/rcS and /sbin/init wrapper&quot;  mkdir -p &quot;$r/etc/init.d&quot;  cat &gt; &quot;$r/etc/init.d/rcS&quot; &lt;&lt;&#x27;EOF&#x27;#!/bin/shset -eu# mount pseudo fs if not mountedmount -t proc proc /proc 2&gt;/dev/null || truemount -t sysfs sys /sys 2&gt;/dev/null || true# Prefer devtmpfs, otherwise bind host /dev if running from hostif ! mountpoint -q /dev; then  if mount -t devtmpfs devtmpfs /dev 2&gt;/dev/null; then    :  else    echo &quot;Warning: devtmpfs not available; try bind mounting /dev from host&quot;  fifi# devpts for pty allocationif [ ! -d /dev/pts ] ; then  mkdir -p /dev/ptsfiif ! mountpoint -q /dev/pts; then  mount -t devpts devpts /dev/pts -o gid=5,mode=620 2&gt;/dev/null || truefi# ensure /tmp existsmkdir -p /tmpchmod 1777 /tmpecho &quot;Minimal rootfs boot complete.&quot;# helper: try to spawn agetty on given device, return 0 on successtry_getty() &#123;  dev=$1  if [ -c &quot;/dev/$&#123;dev&#125;&quot; ] || [ -e &quot;/dev/$&#123;dev&#125;&quot; ]; then    if [ -x /sbin/agetty ]; then      echo &quot;Starting agetty on $&#123;dev&#125;...&quot;      exec /sbin/agetty -L &quot;$&#123;dev&#125;&quot; 115200 vt100      return 0    fi  fi  return 1&#125;# Preferred tty devices to try (adjust order for your env)for tty in tty1 ttyS0 console; do  # only try when device exists and is a character device (or present)  if [ -e &quot;/dev/$tty&quot; ]; then    try_getty &quot;$tty&quot; &amp;&amp; exit 0  fidone# If no suitable tty or agetty absent, spawn a shell on current stdioecho &quot;No usable tty for getty found. Dropping to shell.&quot;exec /bin/sh -lEOF  chmod +x &quot;$r/etc/init.d/rcS&quot;  # Provide a PID 1 init that simply runs the rcS and then respawns a shell.  # If busybox is used as init, it will replace this.  cat &gt; &quot;$r/sbin/init&quot; &lt;&lt;&#x27;EOF&#x27;#!/bin/sh# very small pid 1 wrapper/bin/sh /etc/init.d/rcS# if that returns, keep a shell on consoleexec /bin/shEOF  chmod +x &quot;$r/sbin/init&quot;&#125;copy_binaries_and_libs() &#123;  local r=&quot;$1&quot;  local -n bins_ref=$2  info &quot;Copying binaries to $r and collecting shared libs&quot;  for b in &quot;$&#123;bins_ref[@]&#125;&quot;; do    info &quot; -&gt; $b&quot;    copy_bin &quot;$b&quot; &quot;$r&quot;    copy_libs_for_bin &quot;$b&quot; &quot;$r&quot;  done&#125;copy_loader() &#123;  # copy dynamic linker (ld-linux*) if exists  local r=&quot;$1&quot;  # try common locations  for ld in /lib64/ld-linux-x86-64.so.2 /lib/ld-linux.so.2 /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 /lib/ld-musl-x86_64.so.1; do    if [ -f &quot;$ld&quot; ]; then      mkdir -p &quot;$r$(dirname &quot;$ld&quot;)&quot;      cp -a -- &quot;$ld&quot; &quot;$r$ld&quot;    fi  done&#125;# ----------# Main# ----------info &quot;Build rootfs into: $ROOTFS (mode=glibc)&quot;if [ -e &quot;$ROOTFS&quot; ]; then  warn &quot;$ROOTFS already exists — existing files may be overwritten&quot;ficreate_basic_tree &quot;$ROOTFS&quot;write_etc_files &quot;$ROOTFS&quot;write_init_script &quot;$ROOTFS&quot;# glibc mode: collect binaries from listsdeclare -a BINScollect_bins BINS# ensure we have at least a shellif [ $&#123;#BINS[@]&#125; -eq 0 ]; thenwarn &quot;No binaries discovered to copy. Exiting.&quot;exit 1ficopy_binaries_and_libs &quot;$ROOTFS&quot; BINScopy_loader &quot;$ROOTFS&quot;# ensure /bin/sh exists: prefer host shell or symlink to busybox if providedif [ ! -e &quot;$ROOTFS/bin/sh&quot; ]; then  # try to find a shell binary inside rootfs  if [ -x &quot;$ROOTFS/bin/bash&quot; ]; then    ln -s /bin/bash &quot;$ROOTFS/bin/sh&quot;  elif [ -x &quot;$ROOTFS/bin/dash&quot; ]; then    ln -s /bin/dash &quot;$ROOTFS/bin/sh&quot;  elif [ -x &quot;$ROOTFS/bin/ash&quot; ]; then    ln -s /bin/ash &quot;$ROOTFS/bin/sh&quot;  elif [ -x &quot;$ROOTFS/bin/$(basename &quot;$HOST_BUSYBOX_PATH&quot;)&quot; ]; then    ln -s /bin/$(basename &quot;$HOST_BUSYBOX_PATH&quot;) &quot;$ROOTFS/bin/sh&quot;  else    warn &quot;/bin/sh not found in rootfs. You may not be able to get a shell.&quot;  fifi# finalize permissionschmod 755 &quot;$ROOTFS&quot; || trueinfo &quot;Rootfs build complete at $ROOTFS&quot;cat &lt;&lt;EOFNext steps / hints:- To test with qemu:  qemu-system-x86_64 -kernel /path/to/bzImage -initrd rootfs.cpio.gz -nographic -append &quot;console=ttyS0 root=/dev/ram0 rdinit=/sbin/init&quot;- Or create an initramfs:  cd $ROOTFS  find . | cpio -H newc -o --owner root:root &gt; ../rootfs.cpio  gzip -9 ../rootfs.cpio- Or chroot into it (as root):  mount --bind /proc $ROOTFS/proc  mount --bind /sys  $ROOTFS/sys  mount --bind /dev  $ROOTFS/dev  chroot $ROOTFS /sbin/initEOF\n#启动 rootfs\n#!/bin/shset -exunshare --mount-proc --uts --ipc --net --pid --fork --user --map-root-user chroot ./rootfs /sbin/init\n","categories":["programming","linux"],"tags":["linux"]},{"title":"Bash shell 脚本编程","url":"/2021/01/29/shell-basic/","content":"#注释\n#单行注释\n以#开始, 从#开始直到行尾都会被当做注释\n#多行注释\n:&lt;&lt;EOF注释内容...注释内容...注释内容...EOF\nEOF可以换成任意内容\n#变量\n#定义变量\n变量名=值\n注意等号前后不能有空格\n$ a=1$ b=&quot;Hello world&quot;\n#使用变量\n读取变量的值时需要在前面添加$\n$ echo $a$ echo $&#123;a&#125;$ echo $b\n#修改变量\n再次修改变量的值时不需要添加$\n$ a=2$ b=&quot;World Hello&quot;\n#只读变量\nreadonly 变量名\n可以将一个变量设置为只读\n#删除变量\nunset 变量名\n#数据类型\n#数字 &amp; 字符串\n可以用单引号括起来也可以用双引号, 区别是单引号括起来的字符串不会转义\n$ a=1$ echo &#x27;$a\\&quot;\\&quot;b&#x27;$a\\&quot;\\&quot;b$ echo &quot;$a\\&quot;\\&quot;b&quot;1&quot;&quot;b\n#拼接字符串\n$ greeting=&quot;hello&quot;;$ name=&quot;world&quot;$ msg=&quot;$greeting $name&quot;$ echo $msghello world\n#获取字符串长度\n$ echo $&#123;#msg&#125;11\n#截取字符串\n格式见示例, 两个参数分别为起始位置和长度\n$ echo $&#123;msg:4:3&#125;  o w\n#数组\n数组名=(元素1 元素2 ... 元素n)\n$ a=(1 2 3)$ b=(&quot;Hello&quot; &quot;,&quot; &quot;World&quot;)\n也可以单独定义数组的各个分量\n$ arr[0]=1$ arr[1]=&quot;2&quot;$ arr[5]=3$ arr[100]=4\nshell 中数组的下标从 0 开始, 可以不是连续的\n#读取数组元素\n$&#123;数据名[下标]&#125;\n下标使用 @ 符号可以获取数组中的所有元素\n$ a=(1 2 3 4 5)$ echo $&#123;a[0]&#125; ## 1$ echo $&#123;a[3]&#125; ## 4$ echo $&#123;a[@]&#125; ## 1 2 3 4 5\n#获取数组的长度\n$ echo $&#123;#arr[@]&#125;  或$ echo $&#123;#arr[*]&#125;\n#控制流\n#条件分支\nif condition1then    command1elif condition2then    command2else    commandNfi\n#循环语句\nfor-in 循环\nfor index in 1 2 3 4 5; do    echo $indexdone\nfor-i 循环\nfor ((i=0; i&lt;5; i++)); do    echo $idone\nwhile 循环\ni=1while(( $i&lt;=5 ))do    echo $i    let &quot;i++&quot;done\n#调用参数\n调用脚本时可以传递一些参数, 在脚本内可以通过如下转义表达式获取参数\n\n\n\n参数处理\n说明\n\n\n\n\n$#\n传递到脚本的参数个数\n\n\n$n\n传递到脚本的第 n 个参数, 0 为脚本文件名\n\n\n$*\n以一个单字符串显示所有向脚本传递的参数。\n\n\n$$\n脚本运行的当前进程 ID 号\n\n\n$!\n后台运行的最后一个进程的 ID 号\n\n\n$@\n与$*相同，但是每个参数都会带引号。\n\n\n$-\n显示 Shell 使用的当前选项，与 set 命令功能相同。\n\n\n$?\n显示最后命令的退出状态。0 表示没有错误，其他任何值表明有错误。\n\n\n\n#!/bin/bashecho &quot;Shell 传递参数实例！&quot;echo &quot;参数个数为: $#&quot;echo &quot;执行的文件名: $0&quot;echo &quot;第一个参数为: $1&quot;echo &quot;第二个参数为: $2&quot;echo &quot;第三个参数为: $3&quot;echo &quot;所有参数为: $*&quot;echo &quot;当前进程: $$&quot;\n","categories":["programming","linux"],"tags":["shell"]},{"title":"\"su user\" 和 \"su - user\" 的区别\n","url":"/2020/12/28/su-wo-slash/","content":"\n\n$ su user启动 nologin shell\n\n\n$ su - user启动 login shell\n\n\n","categories":["programming","linux"],"tags":["Linux"]},{"title":"dlmalloc 内存分配器","url":"/2025/04/13/dlmalloc/","content":"dlmalloc 是一个 C 语言实现的流行的内存分配器实现，由纽约州立大学 Oswego 分校计算机系教授 Doug Lea 在 1980 年代编写，许多人称之为 Doug Lea 的 malloc，或者简称 dlmalloc。\n\nDoug Lea 是计算机科学领域的知名学者，尤其在内存管理和并发编程方面有着深厚的造诣。他的研究工作对操作系统、编程语言和计算机体系结构等领域产生了深远的影响。\nDoug Lea 曾经是 JCP（Java Community Process）委员会委员，参与了 JSR 166（Java Concurrency Utilities）的设计和实现。他的工作为 Java 的并发编程模型奠定了基础。\n\n由于具备高效且占用空间较小等特点，dlmalloc 被广泛使用，用 Doug Lea 自己的话说，就是“它在一些 linux 版本里面作为默认的 malloc 被使用，被编译到一些公共的软件包里，并且已经被用于各种 PC 环境及嵌入式系统，以及许多甚至我也不知道的地方”。\n#dlmalloc 的历史\n1987 年，Doug Lea 在编写了一些几乎完全依赖于分配动态内存的 C++ 程序之后，发现它们的运行速度比我预期的要慢得多，并且/或者总内存消耗比预期的要多得多。这是由于所运行的系统（主要是当时的 SunOS 和 BSD 版本）上的内存分配器的特性所致。为了解决这个问题，起初 Doug Lea 用 C++ 编写了一些专用分配器，通过为各种类重载 operator new 来实现的。\n然而，随着时间的推移，他意识到为每个类编写一个分配器并不是一个好的解决方案，因为这会导致代码的重复和维护的复杂性。需要一个更通用的内存分配器 —— 编写一个在正常 C++ 和 C 负载下足够好的分配器，这样程序员就不会被诱惑去编写专用的分配器，除非在非常特殊的情况下。于是 Doug Lea 编写了 dlmalloc，并一直在维护和改进它（在许多志愿者贡献者的帮助下）。\n该分配器的代码已被放置在公共领域（可从 ftp://g.oswego.edu/pub/misc/malloc.c 获得），并且已经被广泛使用：\n\n它在某些版本的 Linux 中用作 malloc 的默认本机版本；它被编译成几个常用的软件包（覆盖本机 malloc）\n在各种 PC 环境以及嵌入式系统中使用\nAndroid 的 Dalvik 虚拟机使用了 dlmalloc 作为其内存分配器\n\n#dlmalloc 的设计目标\n一个好的 malloc 库需要平衡这些目标：\n\n最大化兼容性：满足 POSIX 标准\n最大化便携性\n最小化空间\n最小化时间\n最大化可定制性：允许用户微调参数\n最大化局部性\n最大化错误检测\n最小化异常\n\n1995 年的一篇综述论文中讨论了这些目标的权衡，认为最大限度地减少浪费（通常是由碎片引起的）必须是任何分配器的主要目标。一个极端的例子是按顺序分配下一块可用内存，不进行 free 的 malloc，它具有最快的性能，但是会导致巨大的内存浪费。内存浪费最终等价于对金钱的浪费。\n虽然时间和空间问题占据主导地位，但是权衡和妥协几乎无穷无尽。例如：\n\n为了满足对齐要求，可能会导致浪费一些内存\n为了满足可定制性要求（例如 debug 模式），会导致性能下降\n为了满足兼容性要求，可能会限制功能的灵活性：例如 POSIX 标准要求 malloc 负数内存应该返回 NULL\n为了满足兼容性要求，导致性能下降：例如 dlmalloc 为了兼容一个早期错误的 malloc 实现，允许 realloc 一块 free 过的内存\n一些启发式方法，可以改善小程序的性能，但会导致大程序的性能下降\n\n任何一套沿着这些思路的妥协都不可能完美。然而，多年来，分配器已经发展到能够做出大多数用户都能接受的权衡。持续影响 dlmalloc 演进的驱动力包括：\n\n已有的实证研究表明 dlmalloc 的性能已经跻身于最好的分配器之列\n目标工作负载的变化\n系统和处理器的变化\n来自用户和贡献者的建议、使用报告和代码。\n\n#dlmalloc 的设计\ndlmalloc 算法的两个核心元素自最早版本以来一直保持不变：\n\n边界标签\n分箱\n\n#边界标签\n内存块携带大小信息字段，这些字段位于块的前后。这允许实现两个重要功能：\n\n两个相邻的未使用内存块可以合并成一个更大的块。这最小化了不可用的小块的数量。\n所有内存块都可以从任何已知的块开始，向前或向后遍历。\n\n\ndlmalloc 的原始版本正是以上图的方式实现了边界标记。较新的版本省略了程序正在使用的块上的尾部字段。这本身是一个小的权衡：当块处于活动状态时，这些字段从未被使用，因此不需要存在。消除它们可以减少开销和浪费。然而，缺少这些字段会稍微削弱错误检测能力，因为无法检查用户是否错误地覆盖了应该具有已知值的字段。\n#分箱\n可用块被保存在箱中，按大小分组。存在大量（128 个）固定宽度的箱，大小大约呈对数分布。小于 512 字节的块箱只包含恰好一个大小（间隔 8 字节，简化了 8 字节对齐的执行）。可用块的搜索按从小到大的顺序处理，最佳匹配方案（各种类型和近似方法）与其它如首次匹配等通用方法相比，在真实负载下往往产生最少的碎片化。正如 Wilson 等人所展示的。\n\n直到 1995 年发布的版本，箱内的块是无序的，因此最佳匹配策略只是近似的。较新版本的软件则按大小对箱内的块进行排序，并按最早创建的规则解决冲突。（这是在发现少量时间投入是值得的，以避免观察到的不良情况后进行的。）\n因此，该算法的一般分类是最佳匹配合并：释放的块与相邻块合并，并按大小顺序保存在箱中。\n这种方法会导致每个数据块产生固定的账务开销。由于每个可用数据块都必须保留大小信息和桶链接，因此在 32 位指针的系统中最小可分配数据块为 16 字节，在 64 位指针的系统中最小可分配数据块为 24 字节。这些最小尺寸比大多数人希望看到的要大——例如，在分配许多小型链表节点的情况下，它们可能导致显著的浪费。然而，至少 16 字节的最低要求是任何需要 8 字节对齐且存在 malloc 账务开销的系统的一个特征。\n即使这个基本算法依赖于搜索机制来找到最佳匹配，但通过使用索引技术、利用特殊情况以及精心编码，平均情况下只需要几十条指令，当然这取决于机器和分配模式。\n虽然通过边界标签合并和通过桶分配最佳匹配代表了算法的主要思想，但进一步的考虑导致了一系列启发式改进。这包括局部性保持、荒野保持、内存映射和缓存。\n#dlmalloc 的实现\n整个 dlmalloc 的实现代码只有一个文件 malloc.c。\n#Public API\n#ifndef USE_DL_PREFIX#define dlcalloc               calloc#define dlfree                 free#define dlmalloc               malloc#define dlmemalign             memalign#define dlposix_memalign       posix_memalign#define dlrealloc              realloc#define dlrealloc_in_place     realloc_in_place#define dlvalloc               valloc#define dlpvalloc              pvalloc#define dlmallinfo             mallinfo#define dlmallopt              mallopt#define dlmalloc_trim          malloc_trim#define dlmalloc_stats         malloc_stats#define dlmalloc_usable_size   malloc_usable_size#define dlmalloc_footprint     malloc_footprint#define dlmalloc_max_footprint malloc_max_footprint#define dlmalloc_footprint_limit malloc_footprint_limit#define dlmalloc_set_footprint_limit malloc_set_footprint_limit#define dlmalloc_inspect_all   malloc_inspect_all#define dlindependent_calloc   independent_calloc#define dlindependent_comalloc independent_comalloc#define dlbulk_free            bulk_free#endif /* USE_DL_PREFIX */\n#分配算法\nvoid* dlmalloc(size_t bytes) &#123;  /*    基本算法：    对于较小的内存请求（小于 256 字节，不算每个 chunk 的管理开销）：      1. 优先使用对应 smallbin 中没有剩余空间的 chunk         （即多余字节太少，无法再拆分成一个合法的 chunk）。      2. 若 dv chunk 的大小足够，则直接使用 dv chunk。         dv chunk 通常是最近一次小内存分配所使用 chunk 的相邻块。      3. 否则，从 bin 中选择最小的可用 chunk进行拆分，并将剩余部分保存到 dv。      4. 如果上述方式都不可行，但 top chunk 足够大，则使用 top chunk。      5. 再不行则向系统申请新的内存并使用。    否则，对于一个大请求：      6. 在各个 bin 中查找能够满足请求的最小 chunk；         如果它比 dv chunk 更合适，则使用该 chunk，必要时进行拆分。      7. 如果 dv chunk 的匹配程度优于所有 bin 中的 chunk，则使用 dv chunk。      8. 否则，若 top chunk 足够大，则使用 top chunk。      9. 当请求大小达到或超过 mmap 阈值时，尝试直接通过 mmap 分配内存。    10. 最后，若仍无法满足，则向系统申请内存并使用。    这里那些难看的 goto 的存在，是为了确保在所有执行路径上    都会执行后续的收尾处理（postaction）。  */#if USE_LOCKS  ensure_initialization(); /* initialize in sys_alloc if not using locks */#endif  if (!PREACTION(gm)) &#123;    void* mem;    size_t nb;    if (bytes &lt;= MAX_SMALL_REQUEST) &#123;      bindex_t idx;      binmap_t smallbits;      nb = (bytes &lt; MIN_REQUEST)? MIN_CHUNK_SIZE : pad_request(bytes);      idx = small_index(nb);      smallbits = gm-&gt;smallmap &gt;&gt; idx;      if ((smallbits &amp; 0x3U) != 0) &#123; /* Remainderless fit to a smallbin. */        mchunkptr b, p;        idx += ~smallbits &amp; 1;       /* Uses next bin if idx empty */        b = smallbin_at(gm, idx);        p = b-&gt;fd;        assert(chunksize(p) == small_index2size(idx));        unlink_first_small_chunk(gm, b, p, idx);        set_inuse_and_pinuse(gm, p, small_index2size(idx));        mem = chunk2mem(p);        check_malloced_chunk(gm, mem, nb);        goto postaction;      &#125;      else if (nb &gt; gm-&gt;dvsize) &#123;        if (smallbits != 0) &#123; /* Use chunk in next nonempty smallbin */          mchunkptr b, p, r;          size_t rsize;          bindex_t i;          binmap_t leftbits = (smallbits &lt;&lt; idx) &amp; left_bits(idx2bit(idx));          binmap_t leastbit = least_bit(leftbits);          compute_bit2idx(leastbit, i);          b = smallbin_at(gm, i);          p = b-&gt;fd;          assert(chunksize(p) == small_index2size(i));          unlink_first_small_chunk(gm, b, p, i);          rsize = small_index2size(i) - nb;          /* Fit here cannot be remainderless if 4byte sizes */          if (SIZE_T_SIZE != 4 &amp;&amp; rsize &lt; MIN_CHUNK_SIZE)            set_inuse_and_pinuse(gm, p, small_index2size(i));          else &#123;            set_size_and_pinuse_of_inuse_chunk(gm, p, nb);            r = chunk_plus_offset(p, nb);            set_size_and_pinuse_of_free_chunk(r, rsize);            replace_dv(gm, r, rsize);          &#125;          mem = chunk2mem(p);          check_malloced_chunk(gm, mem, nb);          goto postaction;        &#125;        else if (gm-&gt;treemap != 0 &amp;&amp; (mem = tmalloc_small(gm, nb)) != 0) &#123;          check_malloced_chunk(gm, mem, nb);          goto postaction;        &#125;      &#125;    &#125;    else if (bytes &gt;= MAX_REQUEST)      nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */    else &#123;      nb = pad_request(bytes);      if (gm-&gt;treemap != 0 &amp;&amp; (mem = tmalloc_large(gm, nb)) != 0) &#123;        check_malloced_chunk(gm, mem, nb);        goto postaction;      &#125;    &#125;    if (nb &lt;= gm-&gt;dvsize) &#123;      size_t rsize = gm-&gt;dvsize - nb;      mchunkptr p = gm-&gt;dv;      if (rsize &gt;= MIN_CHUNK_SIZE) &#123; /* split dv */        mchunkptr r = gm-&gt;dv = chunk_plus_offset(p, nb);        gm-&gt;dvsize = rsize;        set_size_and_pinuse_of_free_chunk(r, rsize);        set_size_and_pinuse_of_inuse_chunk(gm, p, nb);      &#125;      else &#123; /* exhaust dv */        size_t dvs = gm-&gt;dvsize;        gm-&gt;dvsize = 0;        gm-&gt;dv = 0;        set_inuse_and_pinuse(gm, p, dvs);      &#125;      mem = chunk2mem(p);      check_malloced_chunk(gm, mem, nb);      goto postaction;    &#125;    else if (nb &lt; gm-&gt;topsize) &#123; /* Split top */      size_t rsize = gm-&gt;topsize -= nb;      mchunkptr p = gm-&gt;top;      mchunkptr r = gm-&gt;top = chunk_plus_offset(p, nb);      r-&gt;head = rsize | PINUSE_BIT;      set_size_and_pinuse_of_inuse_chunk(gm, p, nb);      mem = chunk2mem(p);      check_top_chunk(gm, gm-&gt;top);      check_malloced_chunk(gm, mem, nb);      goto postaction;    &#125;    mem = sys_alloc(gm, nb);  postaction:    POSTACTION(gm);    return mem;  &#125;  return 0;&#125;\n","categories":["programming","malloc"],"tags":["内存管理"]},{"title":"Slub 分配器 -- Linux 内核对 Slab 的优化","url":"/2026/01/12/slub/","content":"1996 年在 Linux 2.0 版本中引入了 slab allocator 之后, 逐渐暴露出一些问题.\n因此在 2.6.22 版本对原有的算法进行了简化, 得到 Slub.\n\n深入理解 Linux 内存管理（八）slab，slob 和 slub 介绍\n\n\n\n每个 node 节点有三个链表，分别记录空闲 slab、部分空闲 slab 和非空闲 slab。当回收操作来不及时，三个链表记录的页框会较长时间停留到 slab 管理器中，不利于提高内存的使用率。针对这点，slub 只保留了一个链表，就是部分空闲 slub。\n每个 cpu 私有数据记录的是 object 的地址，这些 object 可能来自不同的 slab，那么不利于 slab 的回收。slub 改成记录一个实际可用的 slub，不会影响其他 slub 的回收。\nshared 共享链表可能导致一个 slab 持有较多 slab，无法即使释放给伙伴系统。slub 去掉了该链表。可见，slub 出现的主要目的是为了减少 slab 的数量，提高内存的使用率。同时，出于对内存使用率的极致追求，slub 去除了 slab 的着色做法，取而代之是 slub 复用，通过 slub 复用减轻 cache 冲突的情况。\n\n\n","categories":["programming","malloc"],"tags":["内存管理"]},{"title":"[USTC'94] The Slab Allocator: An Object-Caching Kernel Memory Allocator 论文阅读","url":"/2026/01/11/the-slab-allocator-an-object-caching-kernel/","content":"\n本文发表于 USENIX Summer 1994 Technical Conference, 属于 USENIX ATC 的前身.\n\nSlab 分配器最早是有 Jeff Bonwick 在 SunOS 5.4 内核中引入的新内存分配器.\n#1. 引言\n对象分配和释放是内核中最常见的操作之一. 因此一个快速的内存分配器至关重要.\n然而, 很多情况下, 对对象进行初始化和销毁的开销超过了对其进行内存分配和释放的开销.\n因此, 通过缓存常用对象, 使其基本结构在两次使用之间得以保留, 可以获得更大的性能提升.\n#2. 对象缓存\n对象缓存是一种处理频繁分配和释放对象的技术.\n其核心思想是在对象使用之间保留其初始状态 (即构造之后的状态) 的不变部分, 从而避免每次使用对象时都进行销毁和重新创建.\n例如, 包含 mutex 的对象只需在首次分配时调用一次 mutex_init() 即可. 之后可以多次 free 和重新 alloc, 而无需每次都重新调用 mutex_destroy() 和 mutex_init().\n对象的嵌入式锁, 条件变量, 引用计数, 其他对象的列表 以及 只读数据 通常都属于构造状态.\n对象缓存的设计很直白:\n\n\n分配对象:\nif (缓存中有对象) 直接获取 (无需构造过程)\nelse { 分配内存; 构造对象; }\n\n\n释放对象:\n将其返回缓存 (无需销毁过程)\n\n\n从缓存中回收内存:\n从缓存中取出一些对象;\n销毁这些对象;\n释放底下的内存\n\n\n考虑一个例子:\nstruct foo &#123;    kmutex_t foo_lock;    kcondvar_t foo_cv;    struct bar *foo_barlist;    int foo_refcnt;&#125;;foo = kmem_alloc(sizeof(struct foo), KM_SLEEP);mutex_init(&amp;foo-&gt;foo_lock,...);cv_init(&amp;foo-&gt;foo_cv,...);foo-&gt;foo_refcnt = 0;foo-&gt;foo_barlist = NULL;// use foo;ASSERT(foo-&gt;foo_barlist == NULL);ASSERT(foo-&gt;foo_refcnt == 0);cv_destroy(&amp;foo-&gt;foo_cv);mutex_destroy(&amp;foo-&gt;foo_lock);kmem_free(foo);\n#2.1. 中心化对象缓存的必要性\n如果在每个子系统都实现一套对象缓存, 会有以下缺点:\n\n对象缓存需要保留内存, 而系统的其他部分则需要回收这些内存, 两者之间存在着天然的矛盾. 私有管理的缓存无法有效地处理这种矛盾. 它们对系统的整体内存需求了解有限, 彼此之间也缺乏了解. 同样, 系统的其他部分也不知道这些缓存的存在, 因此无法从中&quot;提取&quot;内存.\n由于私有缓存绕过了中央分配器, 它们也绕过了分配器可能具备的任何审计机制和调试功能. 这使得操作系统更难监控和调试.\n重复代码会导致内核代码量增加和维护成本上升.\n\n因此, 对象缓存需要分配器与其客户端之间比标准 kmem_alloc(9F)/kmem_free(9F) 接口程度更高的协作性.\n#2.2. 对象缓存接口\n基于两点观察:\n\n对象的描述信息 (名称、大小、对齐方式、构造函数和析构函数) 应该放在客户端, 而不是中心化的分配器中. 分配器感知块大小语义价值不大.\n内存管理策略应该由中心化的分配器负责, 而不是客户端. 客户端只需要快速分配和释放内存, 而不应该关心如何高效管理底层内存.\n\n由 1 可知, 对象缓存的创建必须由客户端驱动, 并且必须包含对象的完整规范:\nstruct kmem_cache *kmem_cache_create(    char *name,    size_t size,    int align,    void (*constructor)(void *, size_t),    void (*destructor)(void *, size_t));\n由 2 可知, 客户端只需要两个简单接口来分配释放对象:\nvoid *kmem_cache_alloc(    struct kmem_cache *cp,    int flags);void kmem_cache_free(    struct kmem_cache *cp,    void *buf);void kmem_cache_destroy(    struct kmem_cache *cp);\n一个例子:\nvoidfoo_constructor(void *buf, int size)&#123;    struct foo *foo = buf;    mutex_init(&amp;foo-&gt;foo_lock,...);    cv_init(&amp;foo-&gt;foo_cv,...);    foo-&gt;foo_refcnt = 0;    foo-&gt;foo_barlist = NULL;&#125;voidfoo_destructor(void *buf, int size)&#123;    struct foo *foo = buf;    ASSERT(foo-&gt;foo_barlist == NULL);    ASSERT(foo-&gt;foo_refcnt == 0);    cv_destroy(&amp;foo-&gt;foo_cv);    mutex_destroy(&amp;foo-&gt;foo_lock);&#125;foo_cache = kmem_cache_create(&quot;foo_cache&quot;, sizeof (struct foo), 0, foo_constructor, foo_destructor);foo = kmem_cache_alloc(foo_cache, KM_SLEEP);// use foo;kmem_cache_free(foo_cache, foo);\n#3. Slab 分配器实现\n这一章介绍 SunOS 5.4 内核内存分配器 – Slab Allocator 的设计.\nSlab Allocator 得名于其主要数据结构 - “Slab”.\n#3.1. 缓存\n每个缓存都有一个前端和后端, 两者设计上尽量解耦:\n\n前端 是分配器的公共接口. 它负责在缓存中移动对象, 并在需要更多对象时调用后端.\n后端 管理实际内存通过缓存的流动. Influx 例程 (kmem_cache_grow()) 从虚拟机系统获取内存, 创建对象, 并将这些对象送入缓存. 当虚拟机系统需要回收部分内存时 (例如, 在分页开始时), 会调用流出例程 (kmem_cache_reap()).\n请注意, 所有后端活动都完全由内存压力触发. 当缓存需要更多对象时, 内存流入; 当系统其他部分需要更多页面时, 内存流出; 没有任意的限制或水位线.\n滞后控制由 working-set 算法提供, 详见 3.4 节.\nSlab 分配器并非一个单一的整体, 而是由多个独立缓存组成的松散联盟. 这些缓存之间没有共享状态, 因此分配器可以采用逐缓存锁定, 而无需使用全局锁来保护整个区域 (内核堆). 逐缓存锁定允许同时访问任意数量的不同缓存, 从而提高了可扩展性.\n每个缓存都维护着自己的统计数据 – 总分配量、已分配和空闲缓冲区数量等. 这些按缓存划分的统计信息有助于深入了解系统的整体行为. 它们可以指示系统中哪些部分消耗的内存最多, 并有助于识别内存泄漏. 此外, 如果分配器流量能够准确反映各个子系统的活跃程度, 这些统计信息也能起到类似的作用 (例如, 流消息分配量可以直接衡量流的活跃程度).\nSlab 分配器在操作上与 “CustoMalloc”、“QuickFit” 和 “Zone” 分配器都维护着各自独立的空闲列表, 其中包含最常用的缓冲区大小. Grunwald 和 Weinstock 的论文都证明, 定制的隔离存储分配器 (预先知道最常用的分配大小) 通常在空间和时间上都是最优的. Slab 分配器也属于此类, 但它的优势在于其定制是在运行时由客户端驱动的, 而不是在编译时硬编码的 (Zone 分配器也是如此).\n标准的非缓存分配例程, kmem_alloc(9F) 和 kmem_free(9F), 在内部也使用对象缓存. 启动时, 系统会创建大约 30 个缓存, 大小从 8 字节到 9KB 不等, 增量约为 10-20%. kmem_alloc() 函数会从大小最接近的缓存中执行 kmem_cache_alloc() 操作. 大于 9KB 的内存分配 (这种情况很少见) 由后端页面提供程序直接处理.\n#3.2. Slab\n在 Slab 分配器中, slab 是主要的计量单位. 例如, 当分配器需要扩展缓存时, 它会一次性获取一整个 slab 的对象. 类似地, 分配器通过释放一个完整的 slab 来回收未使用的内存 (缩小缓存).\n一个 Slab 由一个或多个几乎连续的内存页组成, 这些内存页被分割成大小相等的块, 并通过引用计数来指示已分配的块的数量. 使用这种简单的数据结构来管理 Arena 的好处相当显著:\n\n\n回收未使用的内存非常简单: 当 slab 引用计数变为零时, 相关的页面可以返回给 VM 系统. 因此, 简单的引用计数取代了大多数其他分配器中发现的复杂树、位图和合并算法.\n\n\n内存的分配和释放是快速的、常数时间的操作: 我们只需要将对象移入或移出空闲列表, 并更新引用计数即可.\n\n\n严重的外部碎片 (空闲列表中出现未使用的缓冲区) 不太可能发生: 随着时间的推移, 许多分配器会积累大量小型、不可用的缓冲区. 这是因为分配器会将现有的空闲缓冲区拆分以满足较小的请求. 例如, 正确的 32 字节和 40 字节分配顺序可能会导致大量 8 字节空闲缓冲区的积累 – 即使从未请求过 8 字节缓冲区.\n隔离存储 (segregated-storage) 分配器不会出现这种情况, 因为填充其 8 字节空闲列表的唯一方法是实际分配和释放 8 字节缓冲区. 任何 32 字节和 40 字节分配序列 – 无论多么复杂 – 都只会导致 32 字节和 40 字节空闲列表被填充. 由于先前的分配是未来分配的良好预测指标 , 因此这些缓冲区很可能会再次被使用.\nslab 减少外部碎片的另一个原因是, slab 中的所有对象都是同一类型, 因此它们具有相同的生命周期分布. 由此产生的在 slab 粒度上对短生命周期对象和长生命周期对象的隔离, 降低了由于单个长生命周期分配而导致整个页面被占用的可能性.\n\n\n内部碎片 (每个缓冲区浪费的空间) 极少: 每个缓冲区的大小都恰好合适 (即缓存对象的大小), 因此唯一浪费的空间是 slab 末尾未使用的部分. 例如, 假设页面大小为 4096 字节, 则 400 字节对象缓存中的每个 slab 将包含 10 个缓冲区, 剩余 96 字节. 我们可以将其视为每个 400 字节缓冲区浪费了 9.6 字节的空间, 或 2.4% 的内部碎片.\n一般来说, 如果一个内存块包含 n 个缓冲区, 那么内部碎片最多为 1/n; 因此, 分配器实际上可以通过控制内存块的大小来控制内部碎片量. 然而, 更大的内存块更容易导致外部碎片, 因为随着每个内存块的缓冲区数量增加, 回收内存块的概率会降低. SunOS 5.4 的实现将内部碎片限制在 12.5% (1/8), 因为这是内部碎片和外部碎片之间权衡的最佳平衡点.\n\n\n#3.2.1. Slab 的逻辑布局\n每个 slab 的内容由 kmem_slab 数据结构管理, 该数据结构维护 slab 在缓存中的链接、引用计数和空闲缓冲区列表. 反过来, slab 中的每个缓冲区都由 kmem_bufctl 结构管理, 该结构保存空闲列表链接、缓冲区地址以及指向控制 slab 的反向指针.\nslab 的结构如下图所示 (图中未显示 bufctl 到 slab 的反向指针):\n\n#3.2.2. Slab 的布局 - 小对象\n对于小于 1/8 个 page 的对象, 构建 slab 的方法是: 分配一个页面, 将 slab 数据放在页面末尾, 并将剩余部分划分为大小相等的缓冲区:\n\n当在空闲列表中时, 每个缓冲区都充当其自身的 bufctl. 实际上只需要链接操作, 因为其他所有操作都是可以算出来的.\n这些对于小型缓冲区来说至关重要 – 否则, 我们最终分配给 bufctl 的内存几乎与分配给缓冲区本身的内存一样多.\n为了便于调试, 空闲列表链接位于缓冲区末尾而非开头. 这是基于经验观察: 数据结构的开头通常比结尾更活跃. 如果缓冲区在释放后被修改, 而堆结构 (空闲列表链接) 仍然完好无损, 则更容易诊断问题.\n分配器会为已构造的对象保留一个额外的 word, 以防止链接覆盖任何已构造的状态.\n#3.2.3. Slab 的布局 - 大对象\n上述方案对于小型对象高效, 但对于大型对象则效率低下. 由于嵌入了 slab 数据, 4K 页面只能容纳一个 2K 缓冲区. 此外, 对于大型 (多页) slab, 我们无法根据缓冲区地址确定 slab 数据地址. 因此, 对于大型对象, 物理布局与逻辑布局相同. 所需的 slab 和 bufctl 数据结构来自它们各自的 (小型对象!) 缓存. 每个缓存的自伸缩哈希表提供缓冲区到 bufctl 的转换.\n#3.3. 空闲列表 (Freelist) 管理\n每个 cache 维护一个包含所有 slab 的双向循环链表. 该 slab 列表是部分排序的, 即空 slab (所有缓冲区均已分配) 排在最前面, 其次是部分 slab (部分缓冲区已分配, 部分缓冲区空闲), 最后是完全 slab (所有缓冲区均空闲, refcnt == 0).\n缓存的空闲列表指针指向其第一个非空 slab. 每个 slab 又各自拥有一个包含可用缓冲区的空闲列表. 这种两级空闲列表结构简化了内存回收.\n当分配器回收一个 slab 时, 它不必从缓存的空闲列表中取消链接每个缓冲区 – 它只需取消链接该 slab 即可.\n#3.4. 回收内存\n当 kmem_cache_free() 检测到 slab 引用计数为零时, 它不会立即回收内存. 相反, 它会将该 slab 移动到空闲列表的尾部, 那里存放着所有完整的 slab. 这确保了只有在所有部分 slab 都已耗尽的情况下, 才会拆分完整的 slab.\n当系统内存不足时, 它会请求内存分配器释放尽可能多的内存. 分配器会照办, 但会保留一个 15 秒的最近使用过的内存块 working-set, 以防止抖动.\n测量结果表明, 系统性能对内存块 working-set 的间隔并不敏感. 这可能是因为两种极端情况 – 零 working-set (按需回收所有已完成的内存块) 和无限 working-set (永远不回收任何内存) – 虽然并非最优, 但都是合理的策略.\n#4. 硬件缓存效应\n现代硬件依赖于良好的缓存利用率, 因此在设计软件时必须考虑缓存效应.\n对于内存分配器而言, 需要考虑两大类缓存效应: 缓冲区地址的分布以及分配器自身的缓存占用空间. 后者已得到一些关注, 但缓冲区地址分布对缓存利用率和总线平衡的影响却鲜为人知.\n#4.1. 缓冲区地址分布对缓存利用率的影响\n中等大小缓冲区的地址分布会影响系统的整体缓存利用率. 特别是, 2 的幂次分配器 (所有缓冲区均为 2 字节且 2​​ 字节对齐) 的效率极低.\n例如, 假设每个 inode (约 300 字节) 都被分配一个 512 字节的缓冲区, 并以 512 字节对齐, 并且只有 inode 的前十几个字段 (48 字节) 经常被引用.\n那么, 大部分与 inode 相关的内存流量将位于这些缓冲区的模 512 余 0-47 的地址中.\n因此, 靠近 512 字节边界的缓存行会被大量加载, 而其余部分则处于空闲状态. 实际上, 只有 9% (48/512) 的缓存可供 inode 使用.\n全相联缓存不会出现这个问题, 但目前的硬件发展趋势是采用更简单的缓存, 而不是更复杂的缓存.\n\n\n直相联映射: cache 被组织成多个 sets(组)，每个 set 只有一个 cache line\n全相联缓存: 一个 Main Memory Block 可以对应到任意 Cache line 中\n组相联映射: Cache 被分为 n 个 set,每个 set 包含 m 条 Cache line。一个 Main Memory 首先被映射到一个 set 中，然后被放到这个 Set 的任意一条 Cache line 中\n\n\n当然, inode 本身并没有什么特别之处. 内核中包含许多其他中等大小的数据结构 (例如 100-500 字节), 它们具有相同的基本特征: 数量众多, 仅包含少数几个常用字段, 并且这些字段集中在结构的开头或附近. 这种数据结构演化方式的固有特性此前并未被认为是分配器设计中的一个重要因素.\n#4.2. 缓冲区地址分布对总线平衡的影响\n在采用多条主总线交错内存的机器上, 上述效应也会对总线利用率产生显著影响. 例如, SPARCcenter 2000 采用两条主总线上的 256 字节交错内存. 继续以上述示例为例, 我们可以看到, 任何 2 的幂次方分配器都会将每个 inode 的前半部分 (热点部分) 映射到总线 0, 后半部分映射到总线 1. 因此, 几乎所有与 inode 相关的缓存未命中都由总线 0 处理. 由于所有 inode 都在争夺一小部分缓存, 未命中率的增加会加剧这种情况.\n这些效果可能非常显著. 在运行 SunOS 5.4 开发内核的 SPARCcenter 2000 上, 使用 LADDIS 协议, 将旧的分配器 (一种基于 2 的幂次方伙伴分配系统 ) 替换为 Slab 分配器后, 总线不平衡率从 43% 降低到仅 17%. 此外, 主缓存未命中率也下降了 13%.\n#4.3. Slab 染色\nSlab 分配器采用了一种简单的染色方案, 将缓冲区均匀地分布在整个缓存中, 从而实现了出色的缓存利用率和总线平衡.\n其原理很简单: 每次创建一个新的 slab 时, 缓冲区地址都会从 slab 基址 (始终按页对齐) 略微不同的偏移量 (颜色) 开始. 例如, 对于一个 200 字节对象、8 字节对齐的缓存, 第一个 slab 的缓冲区地址相对于 slab 基址分别为 0、200、400… 下一个 slab 的缓冲区地址则分别为 8、208、408…, 以此类推. 最大 slab 颜色取决于 slab 中未使用的空间大小. 在本例中, 假设每页 4K, 我们可以在一个 4096 字节的 slab 中容纳 20 个 200 字节的缓冲区. 缓冲区占用 4000 字节, kmem_slab 数据占用 32 字节, 剩余的 64 字节可用于染色. 因此, 最大 slab 颜色为 64, slab 颜色序列为 0, 8, 16, 24, 32, 40, 48, 56, 64, 0, 8,…\n这种染色方案的一个特别好的特性是, 中等大小的 2 的幂次方缓冲区会获得最大的染色量, 因为它们是最不合适的. 例如, 虽然 128 字节可以完美地放入 4096 中, 但它几乎是 4096 - 32 的最小值, 而 32 才是实际可用的空间 (因为嵌入了 slab 数据).\n#4.4. Arena 管理\n分配器的 Arena 管理策略决定了其动态缓存占用空间. 这些策略分为三大类: 顺序适应方法、伙伴方法和隔离存储 (segregated-storage) 方法.\n顺序适应分配器通常需要搜索多个节点才能找到合适的缓冲区. 这类方法本质上会导致缓存占用空间较大: 它们必须检查大量彼此距离很远的节点. 这不仅会导致缓存未命中, 还会导致 TLB 未命中.\n伙伴系统分配器的合并阶段也具有类似的特性.\n隔离存储分配器 (例如 Slab 分配器) 会为不同大小的缓冲区维护单独的空闲列表. 由于分配缓冲区非常简单, 这类分配器通常具有良好的缓存局部性. 分配器只需确定正确的空闲列表 (通过计算、查表或接收参数), 然后从中取出一个缓冲区即可. 释放缓冲区同样简单. 由于只需要加载少量指针, 因此缓存占用空间很小.\nSlab 分配器还有一个额外的优势: 对于中小型缓冲区, 大部分相关信息 (slab 数据、bufctl 和缓冲区本身) 都位于单个页面上. 因此, 单个 TLB 条目即可涵盖大部分操作.\n#6. 调试功能\n导致内核堆损坏的编程错误 – 例如修改已释放的内存、重复释放、释放未初始化的指针或写入缓冲区末尾之外的数据 – 通常难以调试. 幸运的是, 经过全面检测的内核内存分配器可以检测到许多此类问题.\n本节介绍 Slab 分配器的调试功能. 这些功能可以在任何 SunOS 5.4 内核 (不仅限于特殊的调试版本) 中启用, 方法是使用 kadb (内核调试器) 启动并设置相应的标志. * 当分配器检测到问题时, 它会在系统控制台上提供详细的诊断信息.\n#6.1. 审计\n在审计模式下, 分配器会将其活动记录在一个循环事务日志中. 它将此信息存储在扩展版本的 bufctl 结构中, 该结构包含线程指针、高分辨率时间戳和事务堆栈跟踪. 当通过任何其他方法检测到损坏时, 可以确定受影响缓冲区的先前所有者 (可能的嫌疑人).\n#6.2. Free 地址验证\n大型对象缓存使用的缓冲区到缓冲区内容计数器 (bufctl) 哈希表可用作调试功能: 如果 kmem_cache_free() 中的哈希查找失败, 则调用者必定试图释放一个无效地址. 分配器可以通过将&quot;大对象&quot;阈值更改为零来验证所有已释放的地址.\n#6.3. 检测已释放内存的使用情况\n当一个对象被释放时, 分配器会调用其析构函数, 并将内存填充为模式 0xdeadbeef. 下次分配该对象时, 分配器会检查它是否仍然包含 deadbeef 模式. 如果仍然包含, 则将内存填充为 0xbaddcafe 并调用其构造函数. deadbeef 和 baddcafe 模式的选择是为了便于调试过程中人眼识别. 它们分别代表已释放的内存和未初始化的数据.\n#6.4. 红区检查\n红区检查用于检测超出缓冲区末尾的写入操作. 分配器通过在每个缓冲区末尾添加一个保护字, 并在释放缓冲区时验证该保护字是否未被修改, 来检查是否存在红区违规.\n#6.5. 同步 unmap\n通常情况下, slab working-set 算法会将完整的 slab 保留一段时间. 在同步 unmap 模式下, 分配器会立即销毁完整的 slab. kmem_slab_destroy() 函数会将底层内存返回给后端页面提供程序, 后者会取消映射相应的页面. 之后对该 slab 中任何对象的引用都会导致内核数据错误.\n#6.6. 每个缓冲区单独一页 (Page-per-buffer) 模式\n在每缓冲区一页模式下, 每个缓冲区都会被分配一个完整的页面 (或多个页面), 以便在释放缓冲区时可以取消映射. Slab 分配器通过将所有缓存的对齐方式增加到系统页面大小来实现这一点. (此功能需要大量的物理内存.)\n#6.7. 泄漏检测\n审计提供的时间戳使得在用户级别实现一个简易的内核内存泄漏检测器变得容易. 用户级程序只需定期扫描内存区域 (通过 /dev/kmem), 查找新出现的持久性内存分配即可. 例如, 任何一小时前分配的缓冲区, 如果现在仍然处于分配状态, 都可能存在内存泄漏.\n#7. 未来方向\n#7.1. 管理其他类型的内存\nSlab 分配器通过 kmem_getpages() 和 kmem_freepages() 例程从 segkmem 获取页面; 它对底层段驱动程序、资源映射、转换设置等没有任何依赖. 由于分配器遵循此防火墙, 因此可以轻松地接入其他后端页面提供程序. 可以将 ‘getpages’ 和 ‘freepages’ 例程作为附加参数传递给 kmem_cache_create().\n这将使我们能够使用单个分配器管理多种类型的内存 (例如普通内核内存、设备内存、可分页内核内存、NVRAM 等).\n#7.2. Per-处理器的内存分配\nMcKenney 和 Slingwine 提出的基于处理器的分配技术可以很好地应用于 Slab 分配器之上. 他们定义了一个四层分配层次结构, 速度和局部性依次递减: 基于 CPU 的分配、全局分配、合并到页面的分配和合并到 VM 块的分配. 后三层分别与 Slab 分配器的前端层、后端层和页面提供者层紧密对应. 即使在没有锁争用的情况下, 较小的 Per-处理器空闲列表也可以通过消除锁定成本和减少失效流量来提高性能.\n#7.3. 用户级应用程序\nSlab 分配器也可以用作用户级内存分配器. 后端页面提供程序可以是 mmap(2) 或 sbrk(2).\n#8. 结论\nSlab 分配器是一种简单、快速且空间利用率高的内核内存分配器. 它基于对象缓存接口, 该接口降低了分配和释放复杂对象的成本, 并允许分配器按大小和生命周期分布对对象进行隔离. slab 利用对象大小和生命周期隔离分别减少内部和外部碎片. slab 还通过使用简单的引用计数而非合并来简化回收. Slab 分配器在其客户端和虚拟机系统之间建立推送/拉取关系, 从而无需使用任意限制或水位线来控制回收. 分配器的染色方案将缓冲区均匀分布在整个缓存中, 从而提高了系统的整体缓存利用率和总线平衡性. 在几个重要方面, Slab 分配器提供了显著更优的系统性能.\n#参考资料\n\nThe slab allocators of past, present, and future\n细节拉满，80 张图带你一步一步推演 slab 内存池的设计与实现\n\n\n其中 slab 的实现，最早是由 Sun 公司的 Jeff Bonwick 大神在 Solaris 2.4 系统中设计并实现的，由于 Jeff Bonwick 大神公开了 slab 的实现方法，因此被 Linux 所借鉴并于 1996 年在 Linux 2.0 版本中引入了 slab，用于 Linux 内核早期的小内存分配场景。\n由于 slab 的实现非常复杂，slab 中拥有多种存储对象的队列，队列管理开销比较大，slab 元数据比较臃肿，对 NUMA 架构的支持臃肿繁杂（slab 引入时内核还没支持 NUMA），这样导致 slab 内部为了维护这些自身元数据管理结构就得花费大量的内存空间，这在配置有超大容量内存的服务器上，内存的浪费是非常可观的。\n针对以上 slab 的不足，内核大神 Christoph Lameter 在 2.6.22 版本（2007 年发布）中引入了新的 slub 实现。slub 简化了 slab 一些复杂的设计，同时保留了 slab 的基本思想，摒弃了 slab 众多管理队列的概念，并针对多处理器，NUMA 架构进行优化，放弃了效果不太明显的 slab 着色机制。slub 与 slab 相比，提高了性能，吞吐量，并降低了内存的浪费。成为现在内核中常用的 slab 实现。\n\n","categories":["programming","malloc"],"tags":["内存管理"]},{"title":"KAIST CS492: Design and Analysis of Concurrent Programs","url":"/2025/07/06/concurrent-program/","content":"#基本概念\n#Progress\nA program is making progress if, when the program threads are run for a sufficiently long time, at least one of the threads makes progress (for some sensible definition of progress).\n#Make Progress\n在多线程编程中，Make Progress 是指线程在全局时间上取得有用的进展。CAS 操作不算进展。\n使用锁的问题是可能存在一段时间，程序所有线程都没有 Make Progress。例如，当一个线程拿到锁，然后被调度器切换到其他线程，此时，整个程序就没有任何 Progress。\n#Lock-free\n| An algorithm is lock-free if, when the program threads are run for a sufficiently long time, at least one of the threads makes progress (for some sensible definition of progress).\nLock-free 表示程序在任意时间内，至少有一个线程能够 Make Progress。\n#Wait-free\n| An algorithm is wait-free if every operation has a bound on the number of steps the algorithm will take before the operation completes.\nWait-free 表示程序在任意时间内，所有线程都能够 Make Progress。\n关系: Wait-free ⊆ lock-free ⊆ obstruction-free ⊆ “nonblocking”\n#Spinlocks\n#Naive Spinlock\n/// A spin lock.#[derive(Debug)]pub struct SpinLock &#123;    inner: AtomicBool,&#125;unsafe impl RawLock for SpinLock &#123;    type Token = ();    fn lock(&amp;self) &#123;        let backoff = Backoff::new();        while self            .inner            .compare_exchange(false, true, Acquire, Relaxed)            .is_err()        &#123;            backoff.snooze();        &#125;    &#125;    unsafe fn unlock(&amp;self, _token: ()) &#123;        self.inner.store(false, Release);    &#125;&#125;unsafe impl RawTryLock for SpinLock &#123;    fn try_lock(&amp;self) -&gt; Result&lt;(), ()&gt; &#123;        self.inner            .compare_exchange(false, true, Acquire, Relaxed)            .map(|_| ())            .map_err(|_| ())    &#125;&#125;\n#Ticket Lock\nTicket Lock 是一种自旋锁，它使用一个全局的计数器来实现锁。每个线程在进入临界区之前，需要先获取一个自增的 ticket，然后 spin 等待另一个计数器达到自己的 ticket 值。\n加锁操作:\n\n线程获取一个自增的 ticket 值。\n线程等待另一个计数器达到自己的 ticket 值。\n线程进入临界区。\n\n解锁操作:\n\n线程将另一个计数器加 1。\n线程退出临界区。\n\nstruct TicketLock &#123;    next_ticket: AtomicUsize,    now_serving: AtomicUsize,&#125;impl TicketLock &#123;    pub fn new() -&gt; Self &#123;        Self &#123;            next_ticket: AtomicUsize::new(0),            now_serving: AtomicUsize::new(0),        &#125;    &#125;    pub fn lock(&amp;self) &#123;        let ticket = self.next_ticket.fetch_add(1, Relaxed);        while self.now_serving.load(Relaxed) != ticket &#123;            std::hint::spin_loop();        &#125;    &#125;    pub fn unlock(&amp;self) &#123;        let next = self.now_serving.load(Relaxed);        self.now_serving.store(next + 1, Release);    &#125;&#125;\n#MCS Lock\nTicket Lock 有一个问题，就是多个 CPU 同时争抢一把锁时，会导致严重的总线竞争，导致性能下降。\nMCS Lock (Mellor-Crummey and Scott locks)[1] 通过使用一个链表来实现锁，每个线程在进入临界区之前，需要先获取一个自增的 ticket，然后 spin 等待另一个线程的锁。\n数据结构:\n\nMCS 锁实例 ​​: 一条单向链表的指针，初始化为指向一个未拥有的节点。\n锁节点 ​​: 每次加锁时创建一个节点，包含 next 指针和 locked 状态。\n\nnext 指针指向下一个正在等待锁的节点。\nlocked: volitile bool 状态表示当前线程是否正在持有锁。\n\n\n\n加锁操作:\n\n当前线程创建锁节点，next 设为 nullptr。\n通过 lock.getAndSet()将自己加入队列尾部，并获取前驱节点 pred。\n如果 pred 为空，则说明没有线程在排队，直接结束。\n将当前锁节点的 locked 设为 true。\n将 pred-&gt;next 指针指向当前节点。\n自旋等待 locked 变成 false。\n\n解锁操作:\n\n获取当前线程的锁节点 I。\n如果 I-&gt;next 为空，则说明没有线程在排队。\n\n通过 lock.cas(I, nullptr)将队列清空\n如果成功，则直接结束；否则，说明有后继节点过来了\nSpin 等待后继线程把 I-&gt;next 设置好\n\n\n将 I-&gt;next-&gt;locked 设为 false。\n\ntype qnode = record    next : ^qnode       // 指向下一个正在等待锁的节点    locked : Boolean    // 当前线程是否正在持有锁type lock = ^qnode      // initialized to nil// parameter I, below, points to a qnode record allocated// (in an enclosing scope) in shared memory locally-accessible// to the invoking processorprocedure acquire_lock (L : ^lock, I : ^qnode)    I-&gt;next := nil    pred : ^qnode := fetch_and_store (L, I)  // 节点入队    if pred != nil          // 如果之前有线程正在持有锁，说明需要等待        I-&gt;locked := true        pred-&gt;next := I     // 将当前节点加入到前驱节点的next指针        repeat while I-&gt;locked              // 自旋等待前驱节点释放锁procedure release_lock (L : ^lock, I: ^qnode)    if I-&gt;next = nil        // 没有后继节点在排队        if compare_and_store (L, I, nil)    // 确实没有后继节点在排队，则结束            return        repeat while I-&gt;next = nil          // 等待后继节点设置好next指针    I-&gt;next-&gt;locked := false  // 后继节点可以获得锁了\n#CLH Lock\nCLH Lock (Craig, Landin, and Hagersten locks)[2] 是一种基于 ​​ 单向链表实现的可扩展、高性能、公平的自旋锁，它使用一个链表来实现锁。加锁过程相当于将当前线程加入到队列尾部，并等待前驱节点释放锁。解锁过程相当于将当前节点从队列中移除。\n数据结构:\n\nCLH 锁实例 ​​: 一条单向链表的指针，初始化为指向一个未拥有的节点。\n锁节点 ​​: 每次加锁时创建一个节点，包含 prev 指针和 succ_must_wait 状态。\n\nprev 指针指向前一个线程的锁节点。\nsucc_must_wait: volitile bool 状态表示当前线程是否正在持有锁。\n\n\n\n加锁操作:\n\n当前线程创建锁节点，succ_must_wait 设为 true。\n通过 lock.getAndSet()将自己加入队列尾部，并获取前驱节点。\n在前驱节点的 succ_must_wait 上自旋，直到其变为 false。\n\n解锁操作:\n\n获取前驱节点。\n将当前锁节点的 succ_must_wait 设为 false。\n返回前一个锁节点，用于下一次加锁使用 (前一个锁节点一定不再被使用，这里复用前一个锁节点以避免内存分配和释放)。\n\ntype qnode = record    prev : ^qnode             // 前一个正在等待锁/持有锁的节点    succ_must_wait : Boolean  // 当前线程是否正在持有锁type lock = ^qnode  // initialized to point to an unowned qnodeprocedure acquire_lock (L : ^lock, I : ^qnode)    I-&gt;succ_must_wait := true    pred : ^qnode := I-&gt;prev := fetch_and_store (L, I)  // 节点入队    repeat while pred-&gt;succ_must_waitprocedure release_lock (ref I : ^qnode)    pred : ^qnode := I-&gt;prev    I-&gt;succ_must_wait := false    I := pred       // 返回前一个锁节点，用于下一次加锁使用。\nCLH 锁的优缺点:\n\n空间复杂度低，N 个线程，L 个锁，每个线程获取一个锁，则存储空间为O(N+L)\nNUMA 架构的 CPU 上，性能差。每个 CPU 都有自己的内存，如果前驱节点不在同一个 CPU 内存上，由于内存位置比较远，在自旋判断前驱节点的 locked 属性时，性能很差。不过在 SMP 架构的 CPU 上，不存在这个问题，性能依旧很高。\n\nJava 的 AbstractQueuedSynchronizer(简称为 AQS)就是基于 CLH Lock 思想实现的。\n#K42 Lock\nK42 Lock 是一种 MCS Lock 的变种，由 IBM K42 Group 提出，避免了加锁时需要多传一个参数的问题。\n// Locks and queue nodes use the same data structure:type lnode = record    next : ^lnode    union        locked : Boolean    // for queue nodes        tail : ^lnode       // for lockstype lock = lnode// If threads are waiting for a held lock, next points to the queue node// of the first of them, and tail to the queue node of the last.// A held lock with no waiting threads has value &lt;&amp;head, nil&gt;.// A free lock with no waiting threads has value &lt;nil, nil&gt;.procedure acquire_lock (L : ^lnode)    I : lnode    loop        predecessor : ^lnode := L-&gt;tail        if predecessor = nil            // lock appears not to be held            if compare_and_store (&amp;L-&gt;tail, nil, &amp;L-&gt;next)                // I have the lock                return        else            // lock appears to be held            I.next := nil            if compare_and_store (&amp;L-&gt;tail, predecessor, &amp;I)                // I&#x27;m in line                I.locked := true                predecessor-&gt;next := &amp;I                repeat while I.locked             // wait for lock                // I now have the lock                successor : ^lnode := I.next                if successor = nil                    L-&gt;next := nil                    if ! compare_and_store (&amp;L-&gt;tail, &amp;I, &amp;L-&gt;next)                        // somebody got into the timing window                        repeat                            successor := I.next                        while successor = nil     // wait for successor                        L-&gt;next := successor                    return                else                    L-&gt;next := successor                    returnprocedure release_lock (L : ^lnode)    successor : ^lnode := L-&gt;next    if successor = nil        // no known successor        if compare_and_store (&amp;L-&gt;tail, &amp;L-&gt;next, nil)            return        repeat            successor := L-&gt;next        while successor = nil             // wait for successor    successor-&gt;locked := false\n#HCLH Lock\nA Hierarchical CLH Queue Lock\n#MCS Parking Lock\n#总结\n\nNon-scalable locks are dangerous.\n各种 Scalable Spinlock 的总结伪代码: https://www.cs.rochester.edu/research/synchronization/pseudocode/ss.html\n\n#Lock-based Concurrency\n#Sequence Lock\n序列锁是一种乐观读写锁，主要在 Linux 内核中使用。\n数据结构:\n\n序列锁实例 ​​: 一个整数 seq，初始化为 0。\n\n加锁(写锁)操作:\n\n读取 seq 的值 cur_seq。\n如果 cur_seq 的值是偶数\n\n尝试 CAS(seq, cur_seq, cur_seq + 1)\n如果成功，返回 cur_seq\n\n\n回到第 1 步重试。\n\n解锁(写锁)操作:\n\nseq = seq + 2\n\n加锁(读锁)操作:\n\n读取 seq 的值 cur_seq。\n如果 cur_seq 的值是偶数，则返回 cur_seq\n回到第 1 步重试。\n\n解锁(读锁)操作:\n\nseq = seq + 1\n\n#Lock-free Data Structures\n#Treiber’s Stack\nTreiber Stack 是一种无锁栈实现，它通过使用一个链表来实现栈。\nPush 操作:\n\n创建一个新的节点 n，并将其数据初始化。\ntop = load(top)\nn-&gt;next = top\nCAS(top, top, n)\n如果失败，则回到第 2 步重试。\n\nPop 操作:\n\n如果栈为空则返回 nullptr。\ntop = load(top)\nnext = top-&gt;next\nCAS(top, top, next)\n如果 CAS 成功，则返回 top，否则回到第 2 步重试。\n\nimpl&lt;T&gt; Stack&lt;T&gt; &#123;    /// Pushes a value on top of the stack.    pub fn push(&amp;self, t: T) &#123;        let mut node = Owned::new(Node &#123;            data: MaybeUninit::new(t),            next: ptr::null(),        &#125;);        // SAFETY: We don&#x27;t dereference any pointers obtained from this guard.        let guard = unsafe &#123; crossbeam_epoch::unprotected() &#125;;        let mut head = self.head.load(Relaxed, guard);        loop &#123;            node.next = head.as_raw();            match self                .head                .compare_exchange(head, node, Release, Relaxed, guard)            &#123;                Ok(_) =&gt; break,                Err(e) =&gt; &#123;                    head = e.current;                    node = e.new;                &#125;            &#125;        &#125;    &#125;    /// Attempts to pop the top element from the stack.    ///    /// Returns `None` if the stack is empty.    pub fn pop(&amp;self) -&gt; Option&lt;T&gt; &#123;        let mut guard = crossbeam_epoch::pin();        loop &#123;            let head = self.head.load(Acquire, &amp;guard);            let h = unsafe &#123; head.as_ref() &#125;?;            let next = Shared::from(h.next);            if self                .head                .compare_exchange(head, next, Relaxed, Relaxed, &amp;guard)                .is_ok()            &#123;                // Since the above `compare_exchange()` succeeded, `head` is detached from                // `self` so is unreachable from other threads.                // SAFETY: We are returning ownership of `data` in `head` by making a copy of it via                // `assume_init_read()`. This is safe as no other thread has access to `data` after                // `head` is unreachable, so the ownership of `data` in `head` will never be used                // again.                let result = unsafe &#123; h.data.assume_init_read() &#125;;                // SAFETY: `head` is unreachable, and we no longer access `head`.                unsafe &#123; guard.defer_destroy(head) &#125;;                return Some(result);            &#125;            // Repin to ensure the global epoch can make progress.            guard.repin();        &#125;    &#125;&#125;\n#Michael-Scott’s Queue\nMichael-Scott’s queue 是一种无锁队列实现，它通过使用一个链表来实现队列。\nEnqueue 操作:\n\n创建一个新的节点 n，并将其数据初始化。\ntail = load(tail)\nn-&gt;next = tail\nCAS(tail, tail, n)\n如果失败，则回到第 2 步重试。\n\nDequeue 操作：和 Treiber Stack 的 Pop 操作一样\n#Harris’s Sorted Linked List\nHarris’s Sorted Linked List[3] 是一种无锁、有序单链表实现。\n数据结构:\nclass Node&lt;KeyType&gt; &#123;    KeyType key;    Node *next;    Node (KeyType key) &#123;        this.key = key;    &#125;&#125;class List&lt;KeyType&gt; &#123;    Node&lt;KeyType&gt; *head;    Node&lt;KeyType&gt; *tail;    List() &#123;        head = new Node&lt;KeyType&gt; ();        tail = new Node&lt;KeyType&gt; ();        head.next = tail;    &#125;&#125;\n使用逻辑删除来实现删除操作，即不真正删除节点，而是将节点标记为已删除。\n完整回收实现需要结合 Hazard Pointers 等内存回收算法使用。\n#Memory Reclamation Algorithms In Lock-free Data Structures\n#Reference Counting\nReference Counting 是最容易想到的一种方案，但是仔细想想，需要同时原子地更新指针和引用计数，需要特殊的硬件指令支持，因此在实际中很少使用。\n#Read-Copy-Update (RCU)\nRCU 的思想是，在不会存在并发执行的、合适的时间点（例如，无中断的情况下）统一执行删除操作。\n#Hazard Pointers\nHazard Pointers[4] 是一种在 Lock-free Data Structures 中使用的内存回收算法，主要用于解决无锁数据结构中，插入和删除操作可以并发执行的情况下，删除的内存何时回收的问题。\nHazard Pointers 数据结构:\nclass alignas(hardware_destructive_interference_size) hazptr_rec &#123;  std::atomic&lt;const void*&gt; hazptr_&#123;nullptr&#125;; // the hazard pointer  hazptr_rec* next_; // Next in the main hazard pointer list. Immutable.&#125;;thread_local std::vector&lt;hazptr_rec&gt; retired_list; // 线程本地回收列表\nHazard Pointers 读操作:\n\n从 hprec 列表中获取一个空闲的 hprec。\n令 hprec 的 hazptr 字段指向要保护的对象。\n\n读取被保护的对象。\n\n\n读取完成后，令 hprec 的 hazptr=nullptr。\n\nHazard Pointers 写操作:\n\n创建一个新的 hprec，hazptr 字段指向新版本的对象节点。\n将新的 hprec 通过 CAS 插入到链表中。\n如果不成功，则回到第 1 步重试。\n将 CAS 得到的旧版本 hprec 添加到 thread-local 的 retired_list 中。\n\nHazard Pointers 回收算法:\n\n扫描 thread-local 的 retired_list，获取所有数据非空的 hprec。\n将 retired_list 中的对象节点回收。\n\n#Epoch-Based Reclamation\n\n\n\nMellor-Crummey, M. and Scott, M.L. 1981. Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. ACM TOCS, February 1991. ↩︎\n\nDiscovered independently by Travis Craig at the University of Washington (UW TR 93-02-02, February 1993), and by Anders Landin and Eric Hagersten of the Swedish Institute of Computer Science (IPPS, 1994). ↩︎\n\nHarris, T. A Pragmatic Implementation of Non-Blocking Linked-Lists. DISC 2001. ↩︎\n\nAndrei Alexandrescu, Maged Michael. Lock-Free Data Structures with Hazard Pointers. ↩︎\n\n\n\n","categories":["programming","concurrent"],"tags":["C++","并发"]},{"title":"每日小结","url":"/2021/01/07/2021-01-07/","content":"#关于URL末尾的斜杠\nURL末尾有无斜杠会对按相对路径加载的资源造成影响\n\n\n\n路径\n当前渲染的文件\n当前目录\n\n\n\n\nfoobar.com/example/\n/example/index.html\n/example\n\n\nfoobar.com/example\n/example.html\n/\n\n\n\n按绝对路径加载的资源不会受到影响\n#uwsgi作为daemon启动\n#方法1\n添加-d参数\n$ uwsgi -d --ini app.ini\n#方法2\n在ini文件中添加daemonize = /var/log/uwsgi.log\n","categories":["misc","daily","2021"],"tags":["uwsgi","URL"]},{"title":"Numpy, SciPy, Matplotlib 和 Pandas 学习","url":"/2021/03/14/2021-03-14/","content":"时间过的真快啊，转眼间已经是 3 4 月了！\nNumpy 使用\n#array 对象\nMatplotlib 使用\nimport numpy as npimport matplotlib.pyplot as plt\nPandas 使用\n#Series 对象\n#Dataframe 对象\n博客更新日志：\n\n日记分类名字从「每日小结」改为「每日小结 Day Day Up」\n日记文章标题从今往后将取有意义的名字\n日记文件名改成更加标准的 ISO-8601 命名法\n\n","categories":["misc","daily","2021"],"tags":["Python","numpy","matplotlib","pandas"]},{"title":"Python 自定义 JSON Encoder","url":"/2021/06/15/2021-06-15/","content":"#Python 自定义 JSON Encoder\n继承 json.JSONEncoder 类, 重载 default 方法\ndef convert_time(dt: DateTime):    pdt = datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute,                   int(dt.second), int(dt.second * 1000000 % 1000000))    return pdtclass CustomJSONEncoder(JSONEncoder):    def default(self, o):        if isinstance(o, DateTime):            return super(CustomJSONEncoder, self).encode(convert_time(o).isoformat())        return super(CustomJSONEncoder, self).default(o)\n","categories":["misc","daily","2021"],"tags":["Python","json"]},{"title":"Rabin-Karp 算法","url":"/2021/07/02/2021-07-02/","content":"Rabin-Karp 算法\n字符串匹配算法之一，利用哈希进行匹配比较，利用滚动哈希优化哈希函数\n#滚动哈希\n将滑动窗口中的字符串哈希成一个数字\nhi=∑p=0L−1si+pbL−1−ph_i = \\sum_{p = 0}^{L - 1}{s_{i + p} b^{L - 1 - p}}\nhi​=p=0∑L−1​si+p​bL−1−p\n其中LLL为滑动窗口大小，sss为探针字符串，bbb为基数\n\n\n\nindex\n\n-1\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n\ns\n…\na\nb\nc\nd\ne\nf\ng\n\n…\n\n\nhi−1h_{i-1}hi−1​\n\nb5b^5b5\nb4b^4b4\nb3b^3b3\nb2b^2b2\nbbb\n111\n\n\n\n\n\nhih_{i}hi​\n\n\nb5b^5b5\nb4b^4b4\nb3b^3b3\nb2b^2b2\nbbb\n111\n\n\n\n\n\n则有递推公式\nhi=∑p=0L−1si+pbL−1−p=b⋅∑p=0L−1si+p−1bL−1−p−si−1bL+si+L−1=(hi−1−si−1bL−1)b+si+L−1\\begin{aligned}\nh_i &amp;= \\sum_{p = 0}^{L - 1}{s_{i + p} b^{L - 1 - p}}\\\\\n&amp;= b \\cdot \\sum_{p = 0}^{L - 1}{s_{i + p - 1} b^{L - 1 - p}} - s_{i-1} b^{L} + s_{i+L-1}\\\\\n&amp;= (h_{i-1} - s_{i-1} b^{L-1}) b + s_{i+L-1}\n\\end{aligned}\nhi​​=p=0∑L−1​si+p​bL−1−p=b⋅p=0∑L−1​si+p−1​bL−1−p−si−1​bL+si+L−1​=(hi−1​−si−1​bL−1)b+si+L−1​​\n代码：\nint rk_hash(const std::string s, int start, int base, int L, int mod)&#123;    int hash = 0;    for (int i = 0; i &lt; L; i++)    &#123;        hash = (hash * base) % mod;        hash = (hash + s[start + i]) % mod;    &#125;    return hash;&#125;int powmod(int a, int n, int m) &#123;    if (n == 1) return a;    if (n % 2 == 0) return powmod((long long)a * a % m, n / 2, m) % m;    return ((powmod((long long)a * a % m, n / 2, m) % m) * ((long long)a % m)) % m;&#125;int rk_cont_hash(const std::string s, int start, int last_h, int base, int L, int mod)&#123;    int hash = last_h - s[start - 1] * powmod(base, L - 1, mod);    hash = (hash * base) % mod;    hash = (hash + s[start + L - 1]) % mod;    return hash;&#125;","categories":["misc","daily","2021"],"tags":["算法"]},{"title":"VIM YouCompleteMe自动补全插件配置","url":"/2021/10/17/2021-10-17-vim-youcompleteme-plugin/","content":"YouCompleteMe 插件\n配置步骤\n#编译支持 C++17 的 GCC\ngcc 源代码以 ftp 的形式发布，除了 gnu 官方的服务器外，在世界各地都有镜像站，国内靠谱的镜像站有阿里云等。\n下载源码并解压:\n这里使用 9.4.0 版\nwget https://mirrors.aliyun.com/gnu/gcc/gcc-9.4.0/gcc-9.4.0.tar.gztar -zxf gcc-9.4.0.tar.gz\n创建编译用的目录，我一般习惯命名为 build：\nmkdir build &amp;&amp; cd build\n编译配置：\nGCC 可配置的选项极多，具体含义可以在gcc 网站上查询. 如果不知道应该加什么参数，可以参考系统上已有的 gcc 的配置参数：gcc -v。\n$ ../configure -v \\    --prefix=/usr/local    --enable-languages=c,c++ \\    --with-gcc-major-version-only \\    --enable-shared \\    --enable-linker-build-id \\    --without-included-gettext \\    --enable-threads=posix \\    --enable-nls \\    --enable-bootstrap \\    --enable-clocale=gnu \\    --enable-libstdcxx-debug \\    --enable-libstdcxx-time=yes \\    --with-default-libstdcxx-abi=new \\    --enable-gnu-unique-object \\    --disable-vtable-verify \\    --enable-libmpx \\    --enable-plugin \\    --enable-default-pie \\    --with-system-zlib \\    --with-target-system-zlib \\    --enable-multiarch \\    --disable-werror \\    --with-arch-32=i686 \\    --with-abi=m64 \\    --with-multilib-list=m32,m64,mx32 \\    --enable-multilib \\    --with-tune=generic \\    --enable-offload-targets=nvptx-none \\    --without-cuda-driver \\    --enable-checking=release \\    --build=x86_64-linux-gnu \\    --host=x86_64-linux-gnu \\    --target=x86_64-linux-gnu\n此处解释几个关键配置\n\n\n--build\n当前正在使用的机器的环境\n\n\n--host\n编译产物(gcc)将会运行在的环境\n\n\n--target\n编译出来的 gcc 的编译产物的目标环境\n\n\n以上三个参数用于交叉编译，比如在 x86_64 机器上(build)编译一个即将在 arm 上运行(host)的编译器，该编译器产生在 arm 上(target)运行的程序。三个参数的格式均为cpu-company-system\n\n\n--prefix=/usr/local\n编译产物的安装目录。系统默认的 gcc 一般会安装在/usr 下，为了不影响系统的 gcc，建议设置为不同的路径\n\n\n--enable-languages\n配置 gcc 支持的语言类型，可选项有 ada, c, c++, go 等, 由于笔者只需要 c 和 c++，因此只开启了 c 和 c++.\n\n\n--enable-multilib\n是否编译 target 上的 multilib, multilib 是在 target 上编译和运行架构不同程序所需的库，比如在 x86_64 上编译运行 32 位程序。\n\n\n--with-multilib-list\n开启的 multilib 列表，对于不同 target，此处允许的值有所不同。在x86_64-*-linux*上有 m32, m64, mx32 三个可选项。\n\n\n开始编译并安装：\nmakesudo make install\n环境变量配置：\n下面的$INSTALLDIR要换成对应的值。\necho export PATH=&quot;$INSTALLDIR/bin:\\$PATH&quot; &gt;&gt; $HOME/.bashrcecho export CPPFLAGS=&quot;-I$INSTALLDIR/include \\$CPPFLAGS&quot; &gt;&gt; $HOME/.bashrcecho export LDFLAGS=&quot;-L$INSTALLDIR/lib64 -L$INSTALLDIR/lib \\$LDFLAGS&quot; &gt;&gt; $HOME/.bashrcecho export LD_LIBRARY_PATH=&quot;$INSTALLDIR/lib64:$INSTALLDIR/lib:\\$LD_LIBRARY_PATH&quot; &gt;&gt; $HOME/.bashrc\n#安装 Python 3\nYouCompleteMe 需要最低 Python3.6 的环境，如果不满足，则需要安装\n如果软件源中包含了满足要求的 python 版本，则可以直接安装；否则需要从源码编译。\na. 直接从软件源安装\nsudo apt install python3.8 python3.8-dev\nb. 从源码编译 Python3\n暂时没有用到，略过\n#编译 VIM\nVIM 的编译过程相对简单了很多\n克隆代码，可以直接克隆最新版本：\ngit clone --depth 1 https://github.com/vim/vim.git\n编译配置：\ncd vim/src./configure \\    --with-features=huge \\    --enable-multibyte \\    --enable-pythoninterp=dynamic \\    --with-python-config-dir=$(python2.7-config --configdir) \\    --enable-python3interp=dynamic \\    --with-python3-config-dir=$(python3.8-config --configdir) \\    --enable-cscope \\    --enable-gui=auto \\    --enable-gtk2-check \\    --enable-fontset \\    --enable-largefile \\    --disable-netbeans \\    --with-compiledby=&quot;xxxxx@xx.com&quot; \\    --enable-fail-if-missing\n编译安装：\nmakesudo make install\n#安装 CMAKE\n下载 cmake 并安装：\nwget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.shchmod +x cmake-3.21.3-linux-x86_64.sh./cmake-3.21.3-linux-x86_64.sh\n配置环境变量：\necho export PATH=$(pwd)/cmake-3.21.3-linux-x86_64/bin:\\$PATH &gt;&gt; $HOME/.bashrc\n#安装 YouCompleteMe 插件\n——待更新\n","categories":["misc","daily","2021"],"tags":["linux","vim"]},{"title":"Hexo 个人 blog 搭建记录","url":"/2020/11/29/2020-11-29/","content":"#Hexo 基本操作\n\n\n安装\nnpm install -g hexo-cli\n\n\n初始化 hexo\n\n\nhexo init\n\n\n当前目录必须是空的\n\n\n\n\n新建文章\n\n\nhexo new [layout] name\n\n\nlayout 可以是 post, page, draft 之一\n\n\n分别保存至 source/_posts, source, source/_drafts 中\n\n\n新建文件名可以在 _config.yml 里面指定, new_post_name, 默认为 name.md\n\n\n\n\n临时开启 server\n\nhexo server\n\n\n\n#React 入门\n#复习 JavaScript\n\n很好的文章\n\n#Vue 和 React 路由模块的区别\n#Vue 路由\nnew Router(&#123;  mode: &quot;history&quot;,  routes: [    &#123;      path: &quot;/inspect&quot;,      component: Layout,      redirect: &quot;/inspect/table1&quot;,      name: &quot;报表&quot;,      meta: &#123; title: &quot;报表&quot;, icon: &quot;table&quot; &#125;,      children: [        &#123;          path: &quot;table1&quot;,          name: &quot;Table&quot;,          component: () =&gt; import(&quot;@/views/table/index&quot;),          meta: &#123; title: &quot;Table&quot;, icon: &quot;table&quot; &#125;,        &#125;,      ],    &#125;,  ],&#125;);\n#Vue 深入 —— MVVM 框架\n\n\nObserver 数据劫持\n\n\nDep/Watcher 发布订阅\n\n\nCompiler 解析 &#123;&#123; &#125;&#125;, 创建 watcher, 求解表达式\n\n\n","categories":["misc","daily","2020"],"tags":["Hexo"]},{"title":"每日小结","url":"/2020/11/30/2020-11-30/","content":"Github Developer Settings\n\n\nGitHub Apps\n\n\nOAuth Apps\n\n需要用到Github API的App\n\n\n\nPersonal access tokens\n\n\nTokens you have generated that can be used to access the GitHub API.\n\n\n权限类似于账号+密码登录\n\n\n\n\nGithub Actions\n\n\n类似Travis-CI的持续集成平台\n\n\n使用方法\n\n\n在项目根目录创建 .github/workflows\n\n\n继续创建 pages.yml\n\n\n关键字: jobs, runs-on, steps, name, uses, with等等\n\n\n\n\nHexo博客评论系统\n\n\nGitalk\n\n\n利用Github Issues功能实现的评论系统, 开源\n\n\n不支持回复评论\n\n\n修改评论需要打开对应的Issue页修改\n\n\n登录需要的Github权限太高(Github自身问题)\n\n\n\n\nDisqus\n\n\n界面好看\n\n\n需要翻墙才能正常使用\n\n\n\n\nCode-Server\n\n浏览器端的vscode\n\n","categories":["misc","daily","2020"],"tags":["Hexo","Github"]},{"title":"每日小结","url":"/2020/12/01/2020-12-01/","content":"转换dos格式行尾为unix行尾\n\n\n工具: dos2unix\n\n\n单文件: dos2unix input output\n\n\n递归转换: find . -type f -print0 | xargs -0 dos2unix\n\n\nLinux下 nodejs 环境配置\n\n\n安装node(不是最新版): sudo apt install nodejs\n\n\n使用n来切换node版本:\n\n\nsudo npm install -g n\n\n\nsudo n stable\n\n\n\n\n安装yarn: sudo npm install -g yarn\n\n\nNodejs版本切换\n\n\n使用n\n\n\n方法:\n\n\n安装: sudo npm install -g n\n\n\n切换版本: sudo n stable\n\n\n\n\n缺点: 据说会导致node modules混乱\n\n\n\n\n使用nvm\n\n\n清理npm缓存\n\n\nnpm cache clean --force\n\n\nThe default cache direconstructory is ~/.npm on Posix (mac or linux), or %AppData%/npm-cache on Windows.\n\n\nJavascript 复习\n\n\n数据类型\n\nNumber\nString\nBoolean\nSymbol (new in ES2015)\nObject\n\nFunction\nArray\nDate\nRegExp\n\n\nnull\nundefined\n\n\n\n关于变量\n\n\n用var定义的变量具有函数作用域\n\n\nname hiding: (function () &#123; ... &#125;)();\n\n\n允许递归版(IIFEs): (function foo() &#123; ...foo()... &#125;)();\n\n\n\n\n尽量使用let和const\n\n\n\n\n关于Array:\n\n\n相当于一个下标是整数, 有一个length属性, 外表看起来像是数组的Object\n\n\n数组的length属性是数组中最大的下标+1\n\n\n当然作为Object, 也可以有非整数类型的下标\n\n\nfor…in…循环遍历实际存储的值, for…of…循环遍历下标\n  let a = []a[100] = 1a.length // 101for (let i in a) console.log(i) // 1for (let i of a) console.log(i) // undefined, undefined, ... , 1\n\n\n\n\n关于String\n\n\nString + any = String\n\n\nany + String = String\n\n\n注意结合性\n  1 + 2 + &#x27;3&#x27; // &#x27;33&#x27;&#x27;1&#x27; + 2 + 3 // &#x27;123&#x27;\n\n\n\n\n关于Object:\n\n\n对象字面量&#123;&#125;等效于new Object()\n\n\n不过Object()可以被重载\n\n\n对象字面量不能被重载\n\n\n\n\nnew XXX(): 相当于创建了一个类实例(实际上是个Object), XXX()函数可以对其进行进一步修饰\n\n\n\n\n关于function:\n\n函数是对象\n\n\n\n关于new:\n\n\nnew的功能:\n\n\n创建一个空对象{}\n\n\n设置空对象的__proto__(与浏览器实现有关)为构造函数的prototype\n\n\n让this指向空对象\n\n\n执行构造函数, 传入参数this\n\n\n如果构造函数不返回一个对象, 则返回this\n\n\n\n\n模仿一个new:\n  function fake_new(ctor, ...args) &#123;    let x = Object.create(ctor.prototype); // ES5 创建新对象    ///* before ES5 */    // function F() &#123;&#125;    // F.prototype = ctor.prototype;    // let x = new F();    let res = ctor.call(x, ...args); //借用外部传入的构造器给obj设置属性    return typeof res === &#x27;object&#x27; ? res : x; //确保构造器总是返回一个对象&#125;;\n\n\n\n\n关于this:\n\n\n如何正确绑定this\n  &#123;    foo() &#123; ... &#125;, // bad, `this` is bound to the object    foo = () =&gt; &#123; ... &#125;, // good, `this` is bound dynamically&#125;\n\n\n\n\nJavascript面向对象:\n\n\n不用new模拟对象, 利用闭包\n  function makePerson(first, last) &#123;    return &#123;        first: first,        last: last,        fullName: function() &#123; // 每次创建对象时, 这个函数都会被拷贝            return this.first + &#x27; &#x27; + this.last;        &#125;,        fullNameReversed: function() &#123; // 每次创建对象时, 这个函数都会被拷贝            return this.last + &#x27;, &#x27; + this.first;        &#125;    &#125;;&#125;var s = makePerson(&#x27;Simon&#x27;, &#x27;Willison&#x27;);s.fullName(); // &quot;Simon Willison&quot;s.fullNameReversed(); // &quot;Willison, Simon&quot;\n\n\n使用new\n  function Person(first, last) &#123;    this.first = first;    this.last = last;    this.fullName = function() &#123; // 每次创建对象时, 这个函数都会被拷贝        return this.first + &#x27; &#x27; + this.last;    &#125;;    this.fullNameReversed = function() &#123; // 每次创建对象时, 这个函数都会被拷贝        return this.last + &#x27;, &#x27; + this.first;    &#125;;&#125;var s = new Person(&#x27;Simon&#x27;, &#x27;Willison&#x27;);\n\n\n优化: 减少函数对象的拷贝, 利用原型链\n  function Person(first, last) &#123;    this.first = first;    this.last = last;&#125;Person.prototype.fullName = function() &#123;    return this.first + &#x27; &#x27; + this.last;&#125;;Person.prototype.fullNameReversed = function() &#123;    return this.last + &#x27;, &#x27; + this.first;&#125;;\n\n\n\n\nReact 复习\n\n\nJSX语法\n  const element = (    &lt;h1 className=&quot;greeting&quot; onClick=&#123;this.handleClick&#125;&gt;        Hello, world!    &lt;/h1&gt;);\n  const element = React.createElement(    &#x27;h1&#x27;, // label type    &#123;className: &#x27;greeting&#x27;, onClick: this.handleClick&#125;, // attrs    &#x27;Hello, world!&#x27; // children);\n\n\n遇到&lt;开始解析HTML, 遇到&#123;开始解析Javascript\n\n\n要比Vue的模板更加灵活\n\n\n\n\n数据流\n\n\n外层-&gt;内层组件: props, 只读\n\n\n组件自己的状态: state, 可以修改\n\n\n初始化: 在构造函数内直接赋值\n\n\n更新: 通过this.setState(&#123;&#125;)\n\n\nstate可以被传递给子组件作为子组件的props\n\n\n\n\n\n\n受控组件:\n\n\n不自己维护状态的组件\n\n\n状态变化全部交给外部来做: props.value, props.onValueChanged()\n\n\n\n\n","categories":["misc","daily","2020"],"tags":["Linux","Javascript","React","npm"]},{"title":"每日小结","url":"/2020/12/02/2020-12-02/","content":"\n\nReact\n\nchildren属性: 子元素构成的数组\n\n\n\nReact-redux\n\n\n引入: import &#123;...&#125; from 'react-redux'\n\n\nStore\n\n\nRedux maintains a store, which hold state and reducer\n\n\ncreateStore(reducer, initState)\n\nreducer and initState should have the same keys\n\n\n\n\n\nReducer: (state, action) =&gt; (newstate)\n\n\n&lt;Provider&gt;: Inject global state to its children component\n\n\nconnect(mapStateToProps)(Component)\n\n\nmap current states to Component’s props\n\n\nmapStateToProps: is a function: (state) =&gt; { …someProp }\n\n\n一般和结合起来用\n\n\n\n\n\n\nReact-Router\n\n\n引入: import &#123;&#125; from 'react-router-dom'\n\n\n&lt;Route path&gt;: 当url与path匹配时, 渲染内容\n\n\n&lt;BrowserRouter&gt;, &lt;HashRouter&gt;: 的容器\n\n\n&lt;Switch&gt;: 保证其下的所有Route只会选中一个\n\n\n参数化匹配\n\n\n参数定义: path=&quot;/hsl/:h/:s/:l&quot;\n\n\n参数获取: 在children组件中 let &#123; h, s, l &#125; = useParams();\n\n\n\n\n\n\n&lt;Link to&gt;: 跳转到to的a标签\n\n&lt;Redirect&gt;: 渲染时自动跳转\n\n\n\n嵌套使用:\n  let &#123; path, url &#125; = useRouteMatch(); // 获取上一级的path和urlreturn (    &lt;Route path=&#123;`$&#123;path&#125;/next`&#125;&gt;)\n\n\n\n\nVue-Router\n\n\n&lt;router-link :to&gt;: 跳转的a标签\n\n\n&lt;router-view&gt;: 匹配的路由内容会渲染到这里\n\n\nRouter: 在这里定义全局路由表\n\n\n\n\nUbuntu 安装 rpm 包\n\n\nsudo apt install alien\n\n\nsudo alien packagename.rpm\n\n\nsudo apt install ./packagename.deb\n\n\n一步到位: sudo alien -i packagename.rpm\n\n\n\n\nOracle 的连接方式\n\n\nSID/System IDentifier: 数据库实例的全局唯一ID, 每个实例都不一样\n\n\nSERVICE_NAME: 用于对外提供服务的名字, 被客户端使用\n\n也可以被数据库端用来区分对不同客户端提供的服务\n\n\n\nTNS/Transparent Network Substrate: 由Oracle创造的, 专门用于Oracle数据库连接的, 基于TCP/IP, SDP和命名管道的, 同构p2p连接技术\n  // STATE.WORLD是一个TNS NameSTAGE.WORLD =    (DESCRIPTION =        (ADDRESS =            (PROTOCOL = TCP)             (PORT = 1521)            (HOST = LITTLECOMPUTER.ACME.ORG) // 服务器地址        )        (CONNECT_DATA = (SID = MYSID)) // 数据库标识, 可以使用SID或者SERVICE_NAME    )// PROD.WORLD也是一个TNS NamePROD.WORLD =    (DESCRIPTION =        (ADDRESS =            (PROTOCOL = TCP)             (PORT = 1521)            (HOST = BIGCOMPUTER.ACME.ORG)        )        (CONNECT_DATA = (SERVICE_NAME = MYNAME))    )\n\n\n\n","categories":["misc","daily","2020"],"tags":["React","Oracle"]},{"title":"每日小结","url":"/2020/12/03/2020-12-03/","content":"秘书问题\n\n\n又称相亲问题、止步问题、见好就收问题、苏丹的嫁妆问题、挑剔的求婚者问题等\n\n\n要聘请一名秘书，有 n 个应聘者。每次面试一人，面试后就要及时决定是否聘他，如果当时决定不聘他，他便不会回来。面试后总能清楚了解应聘者的合适程度，并能和之前的每个人做比较。问什么样的策略，才使最佳人选被选中的概率最大。\n\n\n答案:\n  \n  展开\n\n\n这个问题的最优解是一个停止规则。在这个规则里，面试官会拒绝头 r - 1 个应聘者 (令他们中的最佳人选为 应聘者 M)，然后选出第一个比 M 好的应聘者。可见最优策略包含于这个系列的策略中。 (如果M在所有n个应聘者中也是最好的一个，那么这个策略将选不出任何人选)对于任意的截断值 r，最佳人选被选中的概率是：\nP(r)=∑i=1nP(applicant i is selected∩applicant i is the best)=∑i=1nP(applicant i is selected∣applicant i is the best)⋅P(applicant i is the best)=[∑i=1r−10+∑i=rnP(the best of the first i−1 applicantsis in the first r−1 applicants∣applicant i is the best)]⋅1n=[∑i=rnr−1i−1]⋅1n=r−1n∑i=rn1i−1.{\\displaystyle {\\begin{aligned}P(r)&amp;=\\sum _{i=1}^{n}P\\left({\\text{applicant }}i{\\text{ is selected}}\\cap {\\text{applicant }}i{\\text{ is the best}}\\right)\\\\&amp;=\\sum _{i=1}^{n}P\\left({\\text{applicant }}i{\\text{ is selected}}|{\\text{applicant }}i{\\text{ is the best}}\\right)\\cdot P\\left({\\text{applicant }}i{\\text{ is the best}}\\right)\\\\&amp;=\\left[\\sum _{i=1}^{r-1}0+\\sum _{i=r}^{n}P\\left(\\left.{\\begin{array}{l}{\\text{the best of the first }}i-1{\\text{ applicants}}\\\\{\\text{is in the first }}r-1{\\text{ applicants}}\\end{array}}\\right|{\\text{applicant }}i{\\text{ is the best}}\\right)\\right]\\cdot {\\frac {1}{n}}\\\\&amp;=\\left[\\sum _{i=r}^{n}{\\frac {r-1}{i-1}}\\right]\\cdot {\\frac {1}{n}}\\quad =\\quad {\\frac {r-1}{n}}\\sum _{i=r}^{n}{\\frac {1}{i-1}}.\\end{aligned}}}\nP(r)​=i=1∑n​P(applicant i is selected∩applicant i is the best)=i=1∑n​P(applicant i is selected∣applicant i is the best)⋅P(applicant i is the best)=[i=1∑r−1​0+i=r∑n​P(the best of the first i−1 applicantsis in the first r−1 applicants​​applicant i is the best)]⋅n1​=[i=r∑n​i−1r−1​]⋅n1​=nr−1​i=r∑n​i−11​.​\n\n\n当n趋近于无穷大时\nP(x)=x∫x11t dt=−xln⁡(x).{\\displaystyle P(x)=x\\int _{x}^{1}{\\frac {1}{t}}\\,dt=-x\\ln(x).}\nP(x)=x∫x1​t1​dt=−xln(x).\n\n\n求出最优的x值为\n1e\\frac {1}{e}\ne1​\n\n\n  \n\n\nNginx 配置\n\n\nroot: 表示去哪个目录下寻找对应url的文件, 实际上是添加前缀\n  location /aaaa/ &#123;    root /home/tom/www/&#125;\n\n\n请求: http://(hostname)/aaaa/hello.txt\n\n\n返回: /home/tom/www/aaaa/hello.txt\n\n\n\n\nalias: 表示把匹配成功的路径替换成alias\n  location /aaaa/ &#123;    alias /home/tom/www/&#125;\n\n\n请求: http://(hostname)/aaaa/hello.txt\n\n\n返回: /home/tom/www/hello.txt\n\n\n\n\nPS: Nginx会自动将两个连续的斜杠替换成一个\n\n\nFlask 嵌套路由配置\n\n\n使用register_blueprint时的url_prefix参数\n  # create app, blueprints, etc.app.register_blueprint(myblueprint, url_prefix=&#x27;/somepath&#x27;)# ...\n\n\n视频网站设计思考\n\n视频文件存储到哪里: OSS对象存储服务\n\n数据库隔离级别\n\n\n读未提交(Read Uncommitted)\n\n\n读已提交(Read Committed)/不可重复读 大多数数据库默认的隔离级别\n\n\n可重复读(Repeatable-Read) mysql数据库所默认的级别\n\n\n序列化(serializable)\n\n\n","categories":["misc","daily","2020"],"tags":["Database","数学基础","Nginx"]},{"title":"每日小结","url":"/2020/12/04/2020-12-04/","content":"#Java 包(Package), Jar 和模块(Module)的区别\n\n\n包 (Package)\n\n\n是 Java 源文件的集合, 包含 Class/Interface/Annotation 的定义\n\n\n与文件系统中的目录对应\n\n\n主要目的是防止命名冲突\n\n\n\n\nJar (Java Archive File)\n\n\n是编译后的 Java 代码(.class)的压缩包\n\n\n实际上是一个 zip 格式文件\n\n\n对 Jar 的操作用jar命令, 其语法类似 Linux 下的tar\n\n\n创建 Jar 包: jar cvf test.jar test\n\n\n解压 Jar 包: jar xvf test.jar\n\n\n列举 Jar 包的内容: jar tvf test.jar\n\n\n\n\n\n\n模块 (Module)\n\n\n是 Java 语言内置的一种管理组件之间依赖关系的方法, 在 Java 9 被引入\n\n\nModule 解决的问题:\n\n\n在 Java 9 之前, 主要使用 package 作为封装方式，使用 Jar 作为模块, 封装方式由 package 和访问修饰符 (private, protected, public,包私有) 控制\n\n\n任何模块都能访问其他模块的 public 的代码，不同 Jar 下的同名包可以相互访问包私有的代码。\n\n\n无法控制非平台开发者对平台内部包的访问，如应用代码可以访问 sun.misc、com.sun.security 这样和具体平台绑定的包。\n\n\n没有明确的依赖信息，模块开发者无法设置必须的依赖，模块使用者不清楚该模块必须依赖哪些模块。只能使用外部的工具，如 Maven、Gradle、OSGI 等。\n\n\n\n\n\n\nModule 的优势\n\n\n明确的依赖配置，JPMS 会在编译和运行之前检查当前环境是否满足依赖的要求。JPMS 会检查版本冲突，即当有两个模块暴露了相同的包名时，系统会抛出异常。JPMS 支持传递性依赖。\n\n\n强大的封装，模块可以明确指定哪些包能暴露给哪些模块，JPMS 不允许代码使用反射的方式访问不对外开放的包。\n\n\n性能优化，JPMS 完全清楚哪些模块是需要的，所以不需要的模块不会被 JVM 载入。\n\n\n\n\nModule 的不足:\n\n缺乏对模块的版本的支持\n\n\n\n特点:\n![](https://frezcirno.github.io/static/images/2020-12-04-java-module.jpg)\n\n\n\n\n\n#Linux (Ubuntu) 硬件管理\n\n\n通用\n\nlshw -short: 列举所有硬件信息\n\n\n\nCPU\n\n\nlscpu: 查看 CPU 信息\n\n\ncat /proc/cpuinfo: 查看每个 CPU 的信息\n\n\n\n\n内存\n\n\nfree [-m|-g|-k|-b|-h]: 查看内存使用情况\n\n\ncat /proc/meminfo: 查看内存详细使用情况\n\n\ndmidecode -t memory: 查看内存硬件信息\n\n\n\n\n硬盘\n\n\nlsblk: 查看硬盘和分区信息\n\n\nfdisk -l: 查看详细分区表\n\n\ndf -h: 查看硬盘剩余空间\n\n\n\n\n主板 BIOS\n\ndmidecode -t bios: 查看 bios 信息\n\n\n\nPCI 设备\n\nlspci\n\n\n\n网卡\n\n\nlspci | grep -i 'Ethernet': 查看网卡硬件信息 (Ethernet 以太网)\n\n\nlspci | grep -i 'Wireless': 查看无线网卡硬件信息\n\n\nifconfig -a: 查看系统的所有网络接口\n\n\n\n\nUSB 设备\n\nlsusb\n\n\n\n#apt remove 与 apt purge 的区别\n\npurge 会删除配置文件, 而 remove 只会删除程序文件\n\n#Java 学习\n\n\nJava 多线程\n\ninterrupt() 向该进程发送中断信号, 具体怎么处理由该进程实现\n\n\n\n","categories":["misc","daily","2020"],"tags":["Linux","Java"]},{"title":"每日小结","url":"/2020/12/05/2020-12-05/","content":"Java Spring 框架学习\nSpring框架，它最主要的功能就是管理一堆使App（应用）发挥功能的类，这些作为整个App的基石、主干的类，就叫做bean。\n要管理bean，也即是这堆发挥业务功能的类，就不能直接把它们new出来，这样缺乏统一的调度。所以，Spring使用.xml配置文件作为媒介，以IoC（Inversion of Control 控制反转）作为工具，将这些bean拿给Spring container作统一管理。\nGit submodule 使用\n\n\n添加子模块\n\ngit submodule add &lt;url&gt; &lt;path&gt;\n\n\n\n设置子模块的分支\n\ngit submodule set-branch --branch &lt;branch&gt; &lt;path&gt;\n\n\n\n克隆下来的项目包含子模块\n  $ git submodule init$ git submodule update\n或者\n  $ git submodule update --init --recursive\n\n\n子模块的更新\n\n\n在子模块内部, 作为一般的git项目更新(pull)\n\n\n在项目目录下git add\n\n\n\n\n子模块的删除\n\n\nrm -rf 子模块目录 删除子模块目录及源码\n\n\n删除项目目录下.gitmodules文件中子模块相关条目\n\n\nvi .git/config 删除配置项中子模块相关条目\n\n\nrm .git/module/* 删除模块下的子模块目录，每个子模块对应一个目录，注意只删除对应的子模块目录即可\n\n\n\n\nGithub 开源项目包含不想公开的密码/Token等怎么处理\n可以将隐私数据包含在项目的Secret设置中, 在Github Action中使用脚本将隐私数据注入到文件中\nYAML 语言\n基本类型:\n\n\n\nString\n\n\nBool: true, false\n\n\nInt\n\n\nNumber\n\n\nnull: ~\n\n\nTime: ISO8601 格式, 例2001-12-14t21:59:43.10-05:00\n\n\nDate: 1976-07-31\n\n\n\n注释: #\n使用左端对齐的键值对表示对象:\n- key1: abc  key2: 123  key3: sss\n使用以-开头的对齐的值表示数组:\nfruits:   - apple  - banana  - orange\n数组套对象:\nobjs:  - name: object A    desp: I&#x27;am an object  - name: Object B    desp: I&#x27;am another object  - name: Object C    desp: I&#x27;am still an object\n数组套数组:\nmatrix:   -     - a11    - a12    - a13  -     - a21    - a22    - a23  -     - a31    - a32    - a33\n对象套数组:\n- name: frezcirno  tags:    - Boy    - Handsome    - Student    - Cool","categories":["misc","daily","2020"],"tags":["Java","Spring","Git"]},{"title":"每日小结","url":"/2020/12/07/2020-12-07/","content":"Visio 画图如何添加自定义连接点\n在工具栏选中&quot;连接点&quot;\n按住Ctrl键, 在画布上点击即可\n有时候可能需要多点几次\n为什么校园网这么卡\nLinux 创建用户时忘了使用-m参数创建用户目录怎么办\n$ mkhomedir_helper &lt;username&gt;\nxargs 命令\nxargs命令用于将上一个命令通过管道输出的内容, 作为命令行参数传递给下一个命令\nxargs后面的命令默认是echo\n即: stdin -&gt; **argv\ntee 命令\ntee命令用于将标准输入复制多份, 分别输出到不同的文件, 并输出到标准输出中去\n可以用来复制文件\n\n输出管道中某一段的内容:\n\n$ ... | tee &gt;(xargs echo) &gt;/dev/null | ...\n\n不退出vim使用sudo保存文件\n\n命令:w !&#123;cmd&#125;表示把当前缓冲区的内容传给后面的命令\n:w !sudo tee % \necho 命令\necho命令用于将命令行参数输出到标准输出\n即: stdin -&gt; stdout\nawk 命令\n强大的字符串处理工具\n最基本的用法:\nawk '&#123;print$1,$2&#125;'\n","categories":["misc","daily","2020"],"tags":["Visio"]},{"title":"每日小结","url":"/2020/12/09/2020-12-09/","content":"Spring学习\n#IoC容器\nSpring类似一个类实例的工厂, 我们提供配置文件(.Java, .xml), Spring按照配置文件装配产品.\nSpring的核心IoC容器接口是ApplicationContext，并提供了多种实现类:\n例如读取xml文件的ClassPathXmlApplicationContext, 使用注解的AnnotationConfigApplicationContext\n#Java的Web框架\n\n\nStruts: 最古老的一个MVC框架，目前版本是2，和1.x有很大的区别；\n\n\nWebWork(Struts 2.0): 一个比Struts设计更优秀的MVC框架；\n\n\nTurbine: 一个重度使用Velocity，强调布局的MVC框架；\n\n\n其他100+MVC框架……（略）\n\n\nSpring本身也开发了一个MVC框架，就叫Spring MVC。\n#Spring连接数据库\n\n\n使用jdbcTemplate\n\n\n使用Hibernate集成框架\n\n\n使用MyBatis集成框架\n\n\n","categories":["misc","daily","2020"],"tags":["Java"]},{"title":"每日小结","url":"/2020/12/10/2020-12-10/","content":"#为什么需要 CORS? CORS 是为了保护谁?\nCORS 标准允许服务器指定谁可以访问该服务器上的资源, 以及该如何访问这些资源\n所以说其实是保护服务器端, 防止恶意网站窃取数据\n#信息安全之盲水印\n妙啊\n","categories":["misc","daily","2020"],"tags":["Java","ORM框架","CORS"]},{"title":"每日小结","url":"/2020/12/11/2020-12-11/","content":"#Linux 内核提权\n\n获取内核版本\n\nuname -a\nlsb_release -a\n\n查询可用的 exploit\n\n在 Kali 中 searchsploit Linux x.x.xx priv\n人工筛选\n\n传到目标机器上碰运气\n\n#GCC 编译参数\n目标机器上的 header/lib 可能不能正常使用, 此时可以下载一套 header/lib 放在用户目录, 用下面的方式指定\n指定 include 目录: -I&lt;path&gt;\n指定连接库目录: -L&lt;path&gt; -l&lt;libname&gt;\n#忘了报名 NCRE, 悲\n","categories":["misc","daily","2020"],"tags":["信息安全","GCC","NCRE"]},{"title":"每日小结","url":"/2020/12/13/2020-12-13/","content":"#C++ STL 中 unique 的用法\ntemplate &lt;class ForwardIterator&gt;  ForwardIterator unique ( ForwardIterator first, ForwardIterator last );template &lt;class ForwardIterator, class BinaryPredicate&gt;  ForwardIterator unique ( ForwardIterator first, ForwardIterator last,                           BinaryPredicate pred );\n将[first, last)之间, 相邻的多个相同元素合并为一个, 返回指向不重复部分末尾的迭代器. 多和 sort 函数一起使用\n例: 使用 unique 实现离散化:\n// vector&lt;int&gt; array;// ...auto disc = array;sort(disc.begin(), disc.end());disc.erase(unique(disc.begin(), disc.end()), disc.end())for(int i = 0; i &lt; disc.size(); i++) &#123;  disc[i] = lower_bound(array.begin(), array.end(), disc[i]) - array.begin();&#125;// 那么disc就可以作为离散化之后的索引// disc[i] = array中第i种元素的最小下标\n#CCFCSP\n今天考了 CCF CSP.\n","categories":["misc","daily","2020"],"tags":["C++","STL","CCFCSP"]},{"title":"每日小结","url":"/2020/12/15/2020-12-15/","content":"#为什么要使用 mongodb\n\n\n不需要预先定义 schema 即可存储数据, 适合于快速原型开发, 以及需求经常变更的情况\n\n\n存储使用的 BSON 格式非常灵活, 可以表示像数组, 字典这样的数据, 而在 RDB 中需要拆分成多个表\n\n\n#Sitemap\n\n站点地图（英语：Sitemaps，旧称 Google Sitemaps，也写为 Sitemap；又称网站地图）是一种列有某个网站所有网址（URL）的 XML 文件，由 Google 最先发起。利用 Sitemaps 协议，网站管理员可以列出网站上可以供搜索引擎抓取的 URL，并通知给后者。Sitemaps 中包含有关每个 URL 的其他信息，如 URL 上次更新的时间、更新的频率以及相对于网站其他 URL 的重要性。搜索引擎的爬虫可以通过 Sitemaps 更有效地抓取网站内容，并找到可能与网站其他内容没有相互链接的 URL。Sitemaps 协议是对 robots.txt 的补充。\n\n简单来说就是列举了网站所有页面的一个 xml 文件, 附带了一些 meta 信息, 有利于搜索引擎抓取, SEO 方法之一.\n一般通过程序生成\n","categories":["misc","daily","2020"],"tags":["SEO","Sitemap","MongoDB"]},{"title":"每日小结","url":"/2020/12/17/2020-12-17/","content":"#Spring-doc OpenAPI 注解\n@Operation\n@Response\n@Parameter\n#Ubuntu 设置代理\n启动 clash for linux\n\n\n配置代理地址配置文件\n\n\n配置 restAPI 地址和访问密码\n\n\n将 clash 作为一个 daemon 进程:\n\n\n使用 systemd:\n先创建配置文件/etc/systemd/system/clash.service\n[Unit]Description=Clash Daemon[Service]ExecStart=/usr/local/bin/clash -d /etc/clash/[Install]WantedBy=multi-user.target\n然后 clash 就成为了一个 service, 可以通过 systemctl 命令启动, 通过 journalctl 命令查看日志等\n\n\n使用 pm2: pm2 start clash\n\n\n\n\n在.bashrc 或者.zshrc 文件底部设置\n\n\n$ export all_proxy=&#x27;socks5://localhost:7891&#x27;$ export http_proxy=&#x27;http://localhost:7890&#x27;$ export https_proxy=&#x27;http://localhost:7890&#x27;$ export ftp_proxy=&#x27;http://localhost:7890&#x27;$ export no_proxy=&#x27;localhost,127.0.0.1&#x27;\n#Shell 的类型\n#shell 的类型\n常见的 shell 有 sh, bash, zsh 等,\n查看当前使用的 shell: echo $SHELL\n查看系统中的所有 shell: cat /etc/shells\n#登录 shell 和非登录 shell\n#交互式 shell 和非交互式 shell\n#.profile 和 .xxshrc\n\n\n/etc/profile 为系统的每个用户设置环境信息,当第一个用户登录时, 该文件被执行, 并从/etc/profile.d 目录的配置文件中搜集 shell 的设置\n\n\n/etc/xxshrc 为每一个运行 xx shell 的用户执行此文件. 当 xx shell 被打开时, 该文件被读取。有些 linux 版本中的/etc 目录下已经没有了该文件。\n\n\n~/.profile 每个用户都可使用该文件输入专用于自己使用的 shell 信息, 当用户登录时,该文件仅仅执行一次 默认情况下,它设置一些环境变量,然后执行用户的.xxshrc 文件.\n\n\n~/.xxshrc 该文件包含专用于某个用户的 xx shell 的配置, 当该用户登录时以及每次打开新的 xx shell 时,该文件被读取.\n\n\n#Hexo layout\nhexo 中每种不同的页面样式称为一个 layout\n根据 md 放置的位置不同, 使用的 layout 也不同\n","categories":["misc","daily","2020"],"tags":["Linux","Java","Spring","Swagger"]},{"title":"每日小结","url":"/2020/12/18/2020-12-18/","content":"#Oracle Apex 怎么调用 procedure\nbegin  ANALYSIS(6, 1000);end;\n","categories":["misc","daily","2020"]},{"title":"每日小结","url":"/2020/12/19/2020-12-19/","content":"#Spring 中 Filter 和 Interceptor 的区别\nFilter 和 Interceptor 都可以实现对请求的拦截\n#功能定位\n都可以用来实现用户认证\n#区别\n\n\n定义规范不同\nFilter 是 Servlet 的规范; 而 Interceptor 是 Spring 框架提供的类似的拦截器\n\n\n拦截范围不同\nFilter 的拦截范围是 Servlet 层; Interceptor 的拦截范围是 Controller 层\n    │   ▲    ▼   │    ┌───────┐    │Filter1│    └───────┘    │   ▲    ▼   │    ┌───────┐    │Filter2│    └───────┘    │   ▲    ▼   │┌─────────────────┐│DispatcherServlet│&lt;───┐└─────────────────┘    ││              ┌────────────┐│              │ModelAndView││              └────────────┘│ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ▲│    ┌───────────┐    │├─┼─&gt;│Controller1│──┼─┤│    └───────────┘    ││ │                 │ ││    ┌───────────┐    │└─┼─&gt;│Controller2│──┼─┘    └───────────┘└ ─ ─ ─ ─ ─ ─ ─ ─ ┘\n\n\n管理方式不同\nFilter 由 Servlet 容器管理, 需要使用DelegatingFilterProxy代理类才能被 Spring 集成; Interceptor 直接由 Spring IoC 容器管理\n\n\n","categories":["misc","daily","2020"],"tags":["Spring"]},{"title":"每日小结","url":"/2020/12/25/2020-12-25/","content":"#Java 答辩结束了, Yeah\n今天开始刷题\n#Hash Map, Hash Set vs Tree Map, Tree Set\nc++中的map, set默认是 treemap 和 treeset, 底层基于红黑树, 元素始终保持有序\n而unordered_map, unordered_set底层基于哈希表\n#对比\n查询/插入/删除\n\n\n\n数据结构\n底层\nfind/get/contain\nset\ndelete\n\n\n\n\nmap\ntree\nO(1)\nO(1)\nO(1)\n\n\nunordered_map\nhash\nO(1)\nO(1)\nO(1)\n\n\n\n\n\n\n数据结构\n底层\nfind/contain\ninsert\ndelete\n\n\n\n\nset\ntree\nO(1)\nO(1)\nO(1)\n\n\nunordered_set\nhash\nO(1)\nO(1)\nO(1)\n\n\n\n遍历\nmap和set都支持按序遍历, 而unordered_map, unordered_set天生就不适合按序遍历(底层不是按序存储), 但是可以无序遍历\n#Java 项目总结\n#前端\n#后端\n#Java Web 项目的基本结构\nController 层: 前端实际调用的接口, 包括用户鉴权, 页面跳转等等, 接口内部调用 Service 层来实现功能.\nService 层: 实际的业务逻辑, 一般按照功能分包, 尽量做到与 Controller 层脱钩, 一般都是些无状态函数(对环境不做任何假设)\nDAO 层: 与数据库相关的代码, 也被称为持久层\n#Spring 框架\nSpring 框架的核心:\n\n\n通过依赖注入来降低代码之间的耦合(IoC 控制反转)\n例如: @Autowired, @Bean, @Resource\n\n\n对 AOP 的支持\n这个没怎么用到\n\n\n对各种工具的集成和支持\n例如: Spring Data JPA, Spring Security 等等\n\n\n#Spring Boot 框架\nSpring Boot 能够自动识别项目中开启的功能(通过扫描安装的依赖包 or 配置文件?)\n项目配置全部写在application.[properties|yml]中, 框架会自动识别\n也可以自己定义一些 key, 在代码中通过@Value(&quot;xxx.yyy&quot;)注入\n#JWT 鉴权\nJWT的三部分: Header.Payload.Signature\nHeader = &#123;&quot;alg&quot;:&quot;HS256&quot;,&quot;typ&quot;:&quot;JWT&quot;&#125;Payload = &#123;&quot;sub&quot;:&quot;1234567890&quot;,&quot;name&quot;:&quot;John Doe&quot;,&quot;iat&quot;:1516239022&#125;_Header64 = base64UrlEncode(Header)_Payload64 = base64UrlEncode(Payload)Signature = HMACSHA256(`$&#123;_Header64&#125;.$&#123;_Payload64&#125;`, $&#123;your-256-bit-secret&#125;)JWT = `$&#123;_Header64&#125;.$&#123;_Payload64&#125;.$&#123;Signature&#125;`\n","categories":["misc","daily","2020"],"tags":["数据结构","答辩"]},{"title":"每日小结","url":"/2020/12/27/2020-12-27/","content":"配环境心得\n放平心态\nLinux之图形化界面\n#X vs. Wayland\nX是最广泛使用的linux显示服务器, 是底层的图形化界面显示标准\n#Display Manager\n显示管理器(或登录管理器)是一个在启动最后显示的图形界面. 负责管理用户登录, 启动X等等. 有些集成在桌面环境中.\n用户一般直接与显示管理器打交道.\n常见的显示管理器有:\n\nGDM(GNOME)\nLXDM(LXDE)\nlightDM(支持多种DE)\n等等\n\n#Desktop Environment\n桌面环境通过汇集使用相同组件库的程序, 为用户提供了完全的图形用户界面.\n常见的桌面环境有:\n\nGNOME\nKDE\nXfce\n等等\n\n#远程桌面\n\nxdmcp\nvnc\n\n","categories":["misc","daily","2020"],"tags":["Linux"]},{"title":"每日小结","url":"/2020/12/30/2020-12-30/","content":"计网答辩顺利\n开心!\nWiFi协议(802.11)扫盲\n#基本概述\n\n\n有线与无线网络\n当前有线网络中最著名的是以太网(Ethernet), 无线网络中最有前景的是WiFi.\n无线网络的缺点:\n\n通信双方因为是通过无线进行通信，所以通信之前需要建立连接；而有线网络就直接用线缆连接，不用这个过程了。\n通信双方通信方式是半双工的通信方式；而有线网络可以是全双工。\n通信时在网络层以下出错的概率非常高，所以帧的重传概率很大，需要在网络层之下的协议添加重传的机制（不能只依赖上面TCP/IP的延时等待重传等开销来保证）；而有线网络出错概率非常小，无需在网络层有如此复杂的机制。\n数据是在无线环境下进行的，所以抓包非常容易，存在安全隐患。\n因为收发无线信号，所以功耗较大，对电池来说是一个考验。\n相对有线网络吞吐量低，这一点正在逐步改善，802.11n协议可以达到600Mbps的吞吐量。\n\n\n\n协议\nEthenet和Wifi采用的协议都属于IEEE 802协议集。其中，Ethenet以802.3协议做为其网络层以下的协议；而Wifi以802.11做为其网络层以下的协议。无论是有线网络，还是无线网络，其网络层以上的部分，基本一样。\n\n\n术语\n讲述之前，我们需要对无线网络中一些常用的术语有所了解。这里先列出一些，后面描述中出现的新的术语，将会在描述中解释。\n\n\nLAN：即局域网，是路由和主机组成的内部局域网，一般为有线网络。\n\n\nWAN：即广域网，是外部一个更大的局域网。\n\n\nWLAN（Wireless LAN，即无线局域网）：前面我们说过LAN是局域网，其实大多数指的是有线网络中的局域网，无线网络中的局域网，一般用WLAN。\n\n\nAP（Access point的简称，即访问点，接入点）：是一个无线网络中的特殊节点，通过这个节点，无线网络中的其它类型节点可以和无线网络外部以及内部进行通信。这里，AP和无线路由都在一台设备上（即Cisco E3000）。\n\n\nStation（工作站）：表示连接到无线网络中的设备，这些设备通过AP，可以和内部其它设备或者无线网络外部通信。\n\n\nAssosiate：连接。如果一个Station想要加入到无线网络中，需要和这个无线网络中的AP关联（即Assosiate）。\n\n\nSSID：用来标识一个无线网络，后面会详细介绍，我们这里只需了解，每个无线网络都有它自己的SSID。\n\n\nBSSID：用来标识一个BSS，其格式和MAC地址一样，是48位的地址格式。一般来说，它就是所处的无线接入点的MAC地址。某种程度来说，它的作用和SSID类似，但是SSID是网络的名字，是给人看的，BSSID是给机器看的，BSSID类似MAC地址。\n\n\nBSS（Basic Service Set）：由一组相互通信的工作站组成，是802.11无线网络的基本组件。主要有两种类型的IBSS和基础结构型网络。IBSS又叫ADHOC，组网是临时的，通信方式为Station&lt;-&gt;Station，这里不关注这种组网方式；我们关注的基础结构形网络，其通信方式是Station&lt;-&gt;AP&lt;-&gt;Station，也就是所有无线网络中的设备要想通信，都得经过AP。在无线网络的基础形网络中，最重要的两类设备：AP和Station。\n\n\n\n\n","categories":["misc","daily","2020"],"tags":["计算机网络","答辩"]},{"title":"1032. Sharing","url":"/2020/12/18/1032/","content":"寻找最长公共后缀\n注意:\n边界情况: 两条链没有交集, 其中一条链是另一条链的子链\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;#ifdef _DEBUGtemplate &lt;class T&gt; void ERR(T x, string name)&#123;    cout &lt;&lt; &quot;[DEBUG]&#123;&quot; &lt;&lt; name &lt;&lt; &quot;&#125; = &quot; &lt;&lt; x &lt;&lt; endl;&#125;#    define debug(x) ERR(x, #x)#else#    define debug(...) 0#endifmap&lt;string, pair&lt;char, string&gt;&gt; M;int main()&#123;#ifdef _DEBUG    freopen(__FILE__ &quot;.txt&quot;, &quot;r&quot;, stdin);    // freopen(__FILE__ &quot;.out&quot;, &quot;w&quot;, stdout);#endif    string w1, w2;    int n;    cin &gt;&gt; w1 &gt;&gt; w2 &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        string a, b;        char c;        cin &gt;&gt; a &gt;&gt; c &gt;&gt; b;        M[a] = make_pair(c, b);    &#125;    stack&lt;string&gt; s1, s2;    debug(&quot;s1&quot;);    for (string addr = w1; addr != &quot;-1&quot;; addr = M[addr].second) &#123;        s1.push(addr);        debug(addr);    &#125;    debug(&quot;s2&quot;);    for (string addr = w2; addr != &quot;-1&quot;; addr = M[addr].second) &#123;        s2.push(addr);        debug(addr);    &#125;    string top = &quot;-1&quot;;    while (s1.size() &amp;&amp; s2.size() &amp;&amp; s1.top() == s2.top()) &#123;        top = s1.top();        debug(top);        s1.pop();        s2.pop();    &#125;    cout &lt;&lt; top &lt;&lt; endl;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1035. Password","url":"/2021/01/13/1035/","content":"水题\n#include &lt;iostream&gt;#include &lt;sstream&gt;using namespace std;int main()&#123;    int n;    cin &gt;&gt; n;    int mod = 0;    ostringstream out;    for (int i = 0; i &lt; n; i++) &#123;        string name, pass;        int flag = 0;        cin &gt;&gt; name &gt;&gt; pass;        for (auto &amp;&amp;ch : pass) &#123;            switch (ch) &#123;                case &#x27;l&#x27;: ch = &#x27;L&#x27;; flag = 1; break;                case &#x27;1&#x27;: ch = &#x27;@&#x27;; flag = 1; break;                case &#x27;0&#x27;: ch = &#x27;%&#x27;; flag = 1; break;                case &#x27;O&#x27;: ch = &#x27;o&#x27;; flag = 1; break;                default: break;            &#125;        &#125;        if (flag) &#123;            mod++;            out &lt;&lt; name &lt;&lt; &quot; &quot; &lt;&lt; pass &lt;&lt; endl;        &#125;    &#125;    if (mod) &#123;        cout &lt;&lt; mod &lt;&lt; endl &lt;&lt; out.str();    &#125; else &#123;        cout &lt;&lt; &quot;There &quot; &lt;&lt; (n == 1 ? &quot;is&quot; : &quot;are&quot;) &lt;&lt; &quot; &quot; &lt;&lt; n &lt;&lt; &quot; account&quot;             &lt;&lt; (n == 1 ? &quot;&quot; : &quot;s&quot;) &lt;&lt; &quot; and no account is modified&quot; &lt;&lt; endl;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1036. Boys vs Girls","url":"/2021/01/11/1036/","content":"送分题\n#include &lt;iostream&gt;using namespace std;struct student &#123;    string name;    string major;&#125;;int main()&#123;    int n;    cin &gt;&gt; n;    student s, male_min, female_max;    int score, male_score_min = 101, female_score_max = -1;    string gender;    for (int i = 0; i &lt; n; i++) &#123;        cin &gt;&gt; s.name &gt;&gt; gender &gt;&gt; s.major &gt;&gt; score;        if (gender == &quot;M&quot;) &#123;            if (score &lt; male_score_min) &#123;                male_min = s;                male_score_min = score;            &#125;        &#125; else &#123;            if (score &gt; female_score_max) &#123;                female_max = s;                female_score_max = score;            &#125;        &#125;    &#125;    if (female_score_max != -1) &#123;        cout &lt;&lt; female_max.name &lt;&lt; &quot; &quot; &lt;&lt; female_max.major &lt;&lt; endl;    &#125; else &#123;        cout &lt;&lt; &quot;Absent&quot; &lt;&lt; endl;    &#125;    if (male_score_min != 101) &#123;        cout &lt;&lt; male_min.name &lt;&lt; &quot; &quot; &lt;&lt; male_min.major &lt;&lt; endl;    &#125; else &#123;        cout &lt;&lt; &quot;Absent&quot; &lt;&lt; endl;    &#125;    if (female_score_max != -1 &amp;&amp; male_score_min != 101) &#123;        cout &lt;&lt; female_score_max - male_score_min &lt;&lt; endl;    &#125; else &#123;        cout &lt;&lt; &quot;NA&quot; &lt;&lt; endl;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1046. Shortest Distance","url":"/2021/01/08/1046/","content":"#题意\n仅由一个环的图, 求任意两个节点之间的最短距离\n#思路\n将环剪开成一条线, 记录每个节点距离起始节点的距离, 查询时取两个方向的距离的最小值\n#感想\n意外的很简单\n#include &lt;iostream&gt;using namespace std;int D[100001];int main()&#123;#ifdef _DEBUG    freopen(__FILE__ &quot;.txt&quot;, &quot;r&quot;, stdin);#endif    int n;    cin &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        int dist;        cin &gt;&gt; dist;        D[i + 1] = D[i] + dist;    &#125;    int len = D[n];    int q;    cin &gt;&gt; q;    for (int i = 0; i &lt; q; i++) &#123;        int x, y;        cin &gt;&gt; x &gt;&gt; y;        if (x &gt; y) &#123;            swap(x, y);        &#125;        cout &lt;&lt; min(D[y - 1] - D[x - 1], len - D[y - 1] + D[x - 1]) &lt;&lt; endl;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1048. Find Coins","url":"/2021/01/14/1048/","content":"寻找{(x,y)|x+y=m,x,y∈S}\n#include &lt;iostream&gt;#include &lt;unordered_map&gt;using namespace std;int main()&#123;    int n, m;    cin &gt;&gt; n &gt;&gt; m;    unordered_map&lt;int, int&gt; S;    for (int i = 0; i &lt; n; i++) &#123;        int x;        cin &gt;&gt; x;        S[x]++;    &#125;    int minx = 0;    for (auto &amp;&amp;k : S) &#123;        int x = k.first;        int y = m - x;        if (x &lt; y &amp;&amp; S.find(y) != S.end() || x == y &amp;&amp; S[x] &gt; 1) &#123;            if (!minx || x &lt; minx)                minx = x;        &#125;    &#125;    if (minx) &#123;        cout &lt;&lt; minx &lt;&lt; &quot; &quot; &lt;&lt; m - minx &lt;&lt; endl;    &#125; else &#123;        cout &lt;&lt; &quot;No Solution&quot; &lt;&lt; endl;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1055. The World's Richest","url":"/2021/01/22/1055/","content":"#题意\n数据表按照某一列筛选后按另一列排序输出, 重复多次\n#优化\n按照输出的最大数量缩减原数据表\n#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;unordered_map&gt;using namespace std;struct person &#123;    string name;    int age;    int worth;    person(const string &amp;name, const int &amp;age, const int &amp;worth)        : name(name), age(age), worth(worth)    &#123;    &#125;&#125;;int main()&#123;    vector&lt;person&gt; P;    int n, k;    cin &gt;&gt; n &gt;&gt; k;    for (int i = 0; i &lt; n; i++) &#123;        string name;        int age;        int worth;        cin &gt;&gt; name &gt;&gt; age &gt;&gt; worth;        P.emplace_back(name, age, worth);    &#125;    sort(P.begin(), P.end(), [](const person &amp;p1, const person &amp;p2) &#123;        return (p1.worth != p2.worth ?                    p1.worth &gt; p2.worth :                    (p1.age != p2.age ? p1.age &lt; p2.age : p1.name &lt; p2.name));    &#125;);    vector&lt;person&gt; P1;    unordered_map&lt;int, int&gt; AgeCount;    for (int i = 0; i &lt; n; i++) &#123;        if (AgeCount[P[i].age] &lt;= 100) &#123;            AgeCount[P[i].age]++;            P1.emplace_back(P[i]);        &#125;    &#125;    for (int i = 0; i &lt; k; i++) &#123;        int m, amin, amax, count = 0;        cin &gt;&gt; m &gt;&gt; amin &gt;&gt; amax;        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; i + 1 &lt;&lt; &quot;:&quot; &lt;&lt; endl;        for (auto &amp;&amp;p : P1) &#123;            if (p.age &gt;= amin &amp;&amp; p.age &lt;= amax) &#123;                cout &lt;&lt; p.name &lt;&lt; &quot; &quot; &lt;&lt; p.age &lt;&lt; &quot; &quot; &lt;&lt; p.worth &lt;&lt; endl;                count++;                if (count &gt;= m)                    break;            &#125;        &#125;        if (count == 0) &#123;            cout &lt;&lt; &quot;None&quot; &lt;&lt; endl;        &#125;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1063. Set Similarity","url":"/2020/12/25/1063/","content":"给出n个集合和k次查询, 计算集合相似度:\n\n集合相似度 = 交集元素数量 / 并集元素数量\n\n思路: 重复查询缓存结果\ntips:\n\nC++ STL中有set_intersection, set_union, set_difference可以直接使用\ninsert_iterator(Container container, Iterator iter)可以创建一个&quot;自动插入迭代器&quot;, 将对迭代器的copy(to)操作转化为插入操作\n\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;set&lt;int&gt; S[50];float F[50][50];float compare(int _a, int _b)&#123;    if (F[_a][_b]) &#123;        return F[_a][_b];    &#125;    set&lt;int&gt; &amp;a = S[_a], &amp;b = S[_b];    set&lt;int&gt; U, I;    set_intersection(a.begin(), a.end(), b.begin(), b.end(), insert_iterator&lt;set&lt;int&gt;&gt;(I, I.begin()));    set_union(a.begin(), a.end(), b.begin(), b.end(), insert_iterator&lt;set&lt;int&gt;&gt;(U, U.begin()));    float res = static_cast&lt;float&gt;(I.size()) / U.size();    if (res == 0) res = 0.0001F;    return F[_a][_b] = F[_b][_a] = res;&#125;int main()&#123;    int n;    cin &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        int s;        cin &gt;&gt; s;        for (int j = 0; j &lt; s; j++) &#123;            int x;            cin &gt;&gt; x;            S[i].insert(x);        &#125;    &#125;    int k;    cin &gt;&gt; k;    for (int i = 0; i &lt; k; i++) &#123;        int a, b;        cin &gt;&gt; a &gt;&gt; b;        printf(&quot;%.1f%%\\n&quot;, 100 * compare(a - 1, b - 1));    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1093. Count PAT's","url":"/2021/01/10/1093/","content":"#题意\n计算PAT字串个数\n#思路\n动态规划?\n\n\n\n增加一个\nPAT个数\nPA个数\nP个数\n\n\n\n\nP\n不变\n不变\n增加1\n\n\nA\n不变\n增加前面的P的个数\n不变\n\n\nT\n增加前面的PA字串的个数\n不变\n不变\n\n\n\n#代码\n#include &lt;iostream&gt;using namespace std;typedef long long ll;int main()&#123;    string s;    cin &gt;&gt; s;    int n = s.size();    ll p = 0, pa = 0, pat = 0;    for (int i = 0; i &lt; n; i++) &#123;        switch (s[i]) &#123;            case &#x27;P&#x27;: p++; break;            case &#x27;A&#x27;: pa = (pa + p) % 1000000007; break;            case &#x27;T&#x27;: pat = (pat + pa) % 1000000007; break;        &#125;    &#125;    cout &lt;&lt; pat % 1000000007 &lt;&lt; endl;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1098. Insertion or Heap Sort","url":"/2020/12/19/1098/","content":"根据排序的中间结果判断使用的插入排序还是堆排序, 并给出下一轮迭代的中间结果\n堆排序: 先逐步构造一个大顶堆, 再每次从堆中取出最大的元素放到堆后面\n思路:\n插入排序的特点: 前面递增, 后面和原数组一样\n进行1次插排: sort\n进行1次堆排: pop_heap\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;int solve()&#123;    int a[100], b[100];    int n;    cin &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        cin &gt;&gt; a[i];    &#125;    for (int i = 0; i &lt; n; i++) &#123;        cin &gt;&gt; b[i];    &#125;    int p = 1;    while (p &lt; n &amp;&amp; b[p - 1] &lt;= b[p])        p++;    int index = p;    while (p &lt; n &amp;&amp; a[p] == b[p])        p++;    if (p == n) &#123;        cout &lt;&lt; &quot;Insertion Sort&quot; &lt;&lt; endl;        sort(b, b + index + 1);        for (int i = 0; i &lt; n; i++) &#123;            if (i != 0)                cout &lt;&lt; &quot; &quot;;            cout &lt;&lt; b[i];        &#125;        cout &lt;&lt; endl;    &#125; else &#123;        cout &lt;&lt; &quot;Heap Sort&quot; &lt;&lt; endl;        int p = n - 1;        while (p &gt; 1 &amp;&amp; b[p] &gt;= b[1])            p--;        pop_heap(b, b + p + 1);        for (int i = 0; i &lt; n; i++) &#123;            if (i != 0)                cout &lt;&lt; &quot; &quot;;            cout &lt;&lt; b[i];        &#125;        cout &lt;&lt; endl;    &#125;&#125;int main()&#123;#ifdef _DEBUG    freopen(__FILE__ &quot;.txt&quot;, &quot;r&quot;, stdin);#endif    solve();    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1099. Build A Binary Search Tree","url":"/2020/12/16/1099/","content":"给出二叉树结构和节点的值列表, 将值填入二叉树中, 输出层次序遍历的结果\n考点: 二叉树的直接后继\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;#ifdef _DEBUGtemplate &lt;class T&gt; void ERR(T x, string name)&#123;    cout &lt;&lt; &quot;[DEBUG]&#123;&quot; &lt;&lt; name &lt;&lt; &quot;&#125; = &quot; &lt;&lt; x &lt;&lt; endl;&#125;#    define debug(x) ERR(x, #    x)#else#    define debug(...) 0#endifstruct node &#123;    int val;    int left, right, parent;&#125; N[100];int main()&#123;#ifdef _DEBUG    freopen(__FILE__ &quot;.txt&quot;, &quot;r&quot;, stdin);    // freopen(__FILE__ &quot;.out&quot;, &quot;w&quot;, stdout);#endif    int first = 1;    int n;    cin &gt;&gt; n;    N[0].parent = -1;    for (int i = 0; i &lt; n; i++) &#123;        int l, r;        cin &gt;&gt; l &gt;&gt; r;        N[i].left = l;        N[l].parent = i;        N[i].right = r;        N[r].parent = i;    &#125;    vector&lt;int&gt; vals;    for (int i = 0; i &lt; n; i++) &#123;        int val;        cin &gt;&gt; val;        vals.push_back(val);    &#125;    sort(vals.begin(), vals.end());    int p = 0;    while (N[p].left != -1) &#123;        if (N[p].left != -1)            p = N[p].left;    &#125;    while (p != -1) &#123;        debug(p);        N[p].val = vals.front();        vals.erase(vals.begin());        if (N[p].right != -1) &#123;            // right down            p = N[p].right;            while (N[p].left != -1) &#123;                p = N[p].left;            &#125;        &#125; else if (N[p].parent != -1 &amp;&amp; N[N[p].parent].left == p) &#123;            // right up            p = N[p].parent;        &#125; else &#123;            while (N[p].parent != -1 &amp;&amp; N[N[p].parent].right == p) &#123;                p = N[p].parent;            &#125;            p = N[p].parent;        &#125;    &#125;    queue&lt;int&gt; Q;    Q.push(0);    while (Q.size()) &#123;        int top = Q.front();        Q.pop();        if (!first)            cout &lt;&lt; &quot; &quot;;        else            first = 0;        cout &lt;&lt; N[top].val;        if (N[top].left != -1)            Q.push(N[top].left);        if (N[top].right != -1)            Q.push(N[top].right);    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1104. Sum of Number Segments","url":"/2021/01/07/1104/","content":"数学题\n#大意\n给定一个数组, 计算该数组所有子序列的和的和, 即\nS=∑i=1n∑j=in∑k=ijak=∑i=1n∑j=in∑k=ijak\\begin{aligned}\nS&amp;=\\sum_{i=1}^{n}\\sum_{j=i}^{n}{\\sum_{k=i}^{j}{a_{k}}}\n\\\\&amp;=\\sum_{i=1}^{n}\\sum_{j=i}^{n}{\\sum_{k=i}^{j}{a_{k}}}\n\\end{aligned}\nS​=i=1∑n​j=i∑n​k=i∑j​ak​=i=1∑n​j=i∑n​k=i∑j​ak​​\n#求解\n对于任意x∈[1,n]x\\in{[1,n]}x∈[1,n], axa_{x}ax​被计算的次数\n就等于集合{(i,j)∣1≤i≤j≤n}\\{(i,j)|1 \\le i \\le j \\le n\\}{(i,j)∣1≤i≤j≤n}中i≤x≤ji \\le x \\le ji≤x≤j的次数\n即\nCx=∑i=1x∑j=xn1=∑i=1x(n−x+1)=x(n−x+1)\\begin{aligned}\nC_{x}&amp;=\\sum_{i=1}^{x}{\\sum_{j=x}^{n}{1}} \\\\\n&amp;=\\sum_{i=1}^{x}{(n-x+1)} \\\\\n&amp;=x(n-x+1)\n\\end{aligned}\nCx​​=i=1∑x​j=x∑n​1=i=1∑x​(n−x+1)=x(n−x+1)​\n总和\nS=∑i=1nCiai=∑i=1n(i(n−i+1)ai)\\begin{aligned}\nS&amp;=\\sum_{i=1}^{n}{C_{i}a_{i}}\\\\\n&amp;=\\sum_{i=1}^{n}{(i(n-i+1)a_{i})}\\\\\n\\end{aligned}\nS​=i=1∑n​Ci​ai​=i=1∑n​(i(n−i+1)ai​)​\n实测: 高精度?\n#include &lt;iostream&gt;#include &lt;cstdio&gt;using namespace std;int main()&#123;    long long res = 0;    int n;    cin &gt;&gt; n;    for (int i = 1; i &lt;= n; i++) &#123;        double x;        cin &gt;&gt; x;        res += (long long)(1000 * x) * i * (n - i + 1);    &#125;    printf(&quot;%.2f&quot;, res / 1000.0);    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1109. Group Photo","url":"/2021/01/22/1109/","content":"#题意\n\n将N个人按照身高排成K行\n每行N/K个人,\n后排的人比前排的人高\n每一排之中, 最高的人在中间, 然后依次在左,右排列\n相同身高的人按字典序排列\n\n#思路\n用deque模拟一排\n#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;deque&gt;using namespace std;int main()&#123;    int first = 1;    typedef pair&lt;string, int&gt; person;    vector&lt;person&gt; P;    int n, k;    cin &gt;&gt; n &gt;&gt; k;    for (int i = 0; i &lt; n; i++) &#123;        string name;        int height;        cin &gt;&gt; name &gt;&gt; height;        P.emplace_back(name, height);    &#125;    sort(P.begin(), P.end(), [](const person &amp;p1, const person &amp;p2) &#123;        return p1.second != p2.second ? p1.second &gt; p2.second :                                        p1.first &lt; p2.first;    &#125;);    int rowcount = n / k;    int rest = n % k;    deque&lt;int&gt; row;    for (int i = 0, left = 1; i &lt; n; i++) &#123;        if (left)            row.push_back(i);        else            row.push_front(i);        left = 1 - left;        if (row.size() == rowcount + rest) &#123;            rest = 0;            for (auto &amp;&amp;i : row) &#123;                cout &lt;&lt; (first ? &quot;&quot; : &quot; &quot;) &lt;&lt; P[i].first;                first = 0;            &#125;            first = 1;            cout &lt;&lt; endl;            row.clear();            left = 1;        &#125;    &#125;    for (auto &amp;&amp;i : row) &#123;        cout &lt;&lt; (first ? &quot;&quot; : &quot; &quot;) &lt;&lt; P[i].first;        first = 0;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1115. Counting Nodes in a BST","url":"/2020/12/15/1115/","content":"按给定序列构造一个二叉搜索树, 分别计算最低两层的节点数量\n思路: 如题, 考基本功\n注意: 一种新奇的写法, 获取倒数第n个元素 (想到了Python是不是XD)\n// vector&lt;int&gt; x;// ...cout &lt;&lt; x.end()[-n] &lt;&lt; endl;\n代码(有内存泄漏):\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;string&gt;#include &lt;vector&gt;#include &lt;stack&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;struct node &#123;    int val;    node *left, *right, *parent;&#125; root;void insert(int x)&#123;    if (!root.parent) &#123;        // is the first root        root.parent = &amp;root;        root.val = x;        return;    &#125;    node *p = &amp;root;    while (true) &#123;        if (x &lt;= p-&gt;val) &#123;            if (p-&gt;left) &#123;                p = p-&gt;left;            &#125; else &#123;                break;            &#125;        &#125; else &#123;            if (p-&gt;right) &#123;                p = p-&gt;right;            &#125; else &#123;                break;            &#125;        &#125;    &#125;    if (x &gt; p-&gt;val) &#123;        p-&gt;right = new node();        p-&gt;right-&gt;val = x;        p-&gt;right-&gt;parent = p;    &#125; else &#123;        p-&gt;left = new node();        p-&gt;left-&gt;val = x;        p-&gt;left-&gt;parent = p;    &#125;&#125;void trav()&#123;    queue&lt;node *&gt; Q;    if (root.parent) &#123;        Q.push(&amp;root);    &#125;    Q.push(nullptr);    int lcount = 0;    vector&lt;int&gt; counts;    while (Q.size() &gt; 1) &#123;        node *x = Q.front();        Q.pop();        if (x == nullptr) &#123;            counts.push_back(lcount);            lcount = 0;            Q.push(nullptr);            continue;        &#125;        lcount++;        if (x-&gt;left)            Q.push(x-&gt;left);        if (x-&gt;right)            Q.push(x-&gt;right);    &#125;    counts.push_back(lcount);    int back = counts.size() ? counts.back() : 0;    int back2 = counts.size() &gt; 2 ? counts.end()[-2] : 0;    cout &lt;&lt; back &lt;&lt; &quot; + &quot; &lt;&lt; back2 &lt;&lt; &quot; = &quot; &lt;&lt; back + back2 &lt;&lt; endl;&#125;int main()&#123;    int n;    cin &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        int x;        cin &gt;&gt; x;        insert(x);    &#125;    trav();    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1123. Is It a Complete AVL Tree","url":"/2020/12/18/1123/","content":"#题目\n按输入序列构造AVL树, 输出层次序遍历的结果, 和AVL树是否为完全二叉树\n#一些小技巧:\n#1. 树的初始化问题\n我们可以让insert函数返回插入后新的树根(从而可以递归调用), 调用时采用 node = insert(node, val) 的方式, 从而统一了第一个节点和其他节点的处理, 极大地简化了判断逻辑\nnode* insert(node* p, int x)&#123;    if(!p)&#123;        p=new node();        p-&gt;val=x;    &#125;else if(x&lt;p-&gt;val)&#123;        p-&gt;left=insert(p-&gt;left,x);    &#125;else&#123;        p-&gt;right=insert(p-&gt;right,x);    &#125;    return p;&#125;// ...int main()&#123;    node*root=nullptr;    root=insert(root,1);    root=insert(root,2);    root=insert(root,3);&#125;\n#体会:\n\nKISS, 简单的才是最好的\n用好递归能节约大量代码\n数据结构题不必在性能上过多纠结\n\n#代码\n(部分参考自网络)\n#include &lt;iostream&gt;#include &lt;iomanip&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;queue&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;cmath&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;climits&gt;using namespace std;struct node &#123;    int val;    node *left = nullptr, *right = nullptr;&#125;;int depth(node *p)&#123;    return p ? 1 + max(depth(p-&gt;left), depth(p-&gt;right)) : 0;&#125;node *rr(node *p)&#123;    node *lc = p-&gt;left;    p-&gt;left = lc-&gt;right;    lc-&gt;right = p;    return lc;&#125;node *lr(node *p)&#123;    node *rc = p-&gt;right;    p-&gt;right = rc-&gt;left;    rc-&gt;left = p;    return rc;&#125;node *lrr(node *p)&#123;    p-&gt;left = lr(p-&gt;left);    return rr(p);&#125;node *rlr(node *p)&#123;    p-&gt;right = rr(p-&gt;right);    return lr(p);&#125;node *insert(node *p, int x)&#123;    if (!p) &#123;        p = new node();        p-&gt;val = x;    &#125; else if (x &lt; p-&gt;val) &#123;        p-&gt;left = insert(p-&gt;left, x);        if (depth(p-&gt;left) - depth(p-&gt;right) &gt; 1) &#123;            if (x &lt; p-&gt;left-&gt;val)                p = rr(p);            else                p = lrr(p);        &#125;    &#125; else &#123;        p-&gt;right = insert(p-&gt;right, x);        if (depth(p-&gt;left) - depth(p-&gt;right) &lt; -1) &#123;            if (x &gt; p-&gt;right-&gt;val)                p = lr(p);            else                p = rlr(p);        &#125;    &#125;    return p;&#125;void trav(node *p)&#123;    int first = 1;    int empty = 0;    int comp = 1;    queue&lt;node *&gt; Q;    if (p) &#123;        Q.push(p);    &#125;    while (Q.size()) &#123;        node *top = Q.front();        Q.pop();        if (!first) &#123;            cout &lt;&lt; &quot; &quot;;        &#125;        first = 0;        cout &lt;&lt; top-&gt;val;        if (top-&gt;left) &#123;            if (empty) &#123;                comp = 0;            &#125;            Q.push(top-&gt;left);        &#125; else &#123;            empty = 1;        &#125;        if (top-&gt;right) &#123;            if (empty) &#123;                comp = 0;            &#125;            Q.push(top-&gt;right);        &#125; else &#123;            empty = 1;        &#125;    &#125;    cout &lt;&lt; endl &lt;&lt; (comp ? &quot;YES&quot; : &quot;NO&quot;) &lt;&lt; endl;&#125;int main()&#123;    node *root = nullptr;    int n;    cin &gt;&gt; n;    for (int i = 0; i &lt; n; i++) &#123;        int x;        cin &gt;&gt; x;        root = insert(root, x);    &#125;    trav(root);    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1140. Look-and-say Sequence","url":"/2021/01/10/1140/","content":"#题意\n求一个数字用行程编码压缩n-1次的结果\n#代码\n#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;string compress(const string &amp;s)&#123;    int n = s.size();    string res;    for (int i = 0, count = 0; i &lt;= n; i++) &#123;        if (i == n || i &gt; 0 &amp;&amp; s[i] != s[i - 1]) &#123;            res.push_back(s[i - 1]);            res += to_string(count);            count = 0;        &#125;        count++;    &#125;    return res;&#125;int main()&#123;    string s;    int n;    cin &gt;&gt; s &gt;&gt; n;    for (int i = 1; i &lt; n; i++) &#123;        s = compress(s);    &#125;    cout &lt;&lt; s &lt;&lt; endl;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"1151. LCA in a Binary Tree","url":"/2021/01/12/1151/","content":"二叉树上寻找最近公共祖先\n需要注意节点的key不一定连续\n#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;unordered_map&gt;using namespace std;struct node &#123;    int left, right, parent, depth;&#125;;unordered_map&lt;int, node&gt; N;int build(const vector&lt;int&gt;::iterator &amp;preord_b,    const vector&lt;int&gt;::iterator &amp;preord_e,    const vector&lt;int&gt;::iterator &amp;inord_b,    const vector&lt;int&gt;::iterator &amp;inord_e,    int parent,    int depth)&#123;    if (preord_b == preord_e)        return 0;    int key = *preord_b;    auto inord_mid = find(inord_b, inord_e, key);    int left_len = inord_mid - inord_b;    N[key].left = build(preord_b + 1, preord_b + 1 + left_len, inord_b,        inord_mid, key, depth + 1);    N[key].right = build(preord_b + 1 + left_len, preord_e, inord_mid + 1,        inord_e, key, depth + 1);    N[key].parent = parent;    N[key].depth = depth;    return key;&#125;int lca(int a, int b)&#123;    if (a == b) &#123;        return a;    &#125; else if (N[a].depth &gt; N[b].depth) &#123;        return lca(N[a].parent, b);    &#125; else if (N[a].depth &lt; N[b].depth) &#123;        return lca(N[b].parent, a);    &#125;    return lca(N[a].parent, N[b].parent);&#125;int main()&#123;    int m;    cin &gt;&gt; m;    int n;    cin &gt;&gt; n;    vector&lt;int&gt; inord(n), preord(n);    for (int i = 0; i &lt; n; i++)        cin &gt;&gt; inord[i];    for (int i = 0; i &lt; n; i++)        cin &gt;&gt; preord[i];    int root =        build(preord.begin(), preord.end(), inord.begin(), inord.end(), 0, 0);    for (int i = 0; i &lt; m; i++) &#123;        int u, v;        cin &gt;&gt; u &gt;&gt; v;        bool e1 = (N.find(u) == N.end());        bool e2 = (N.find(v) == N.end());        if (e1 &amp;&amp; e2) &#123;            cout &lt;&lt; &quot;ERROR: &quot; &lt;&lt; u &lt;&lt; &quot; and &quot; &lt;&lt; v &lt;&lt; &quot; are not found.&quot; &lt;&lt; endl;            continue;        &#125; else if (e1 || e2) &#123;            cout &lt;&lt; &quot;ERROR: &quot; &lt;&lt; (e1 ? u : v) &lt;&lt; &quot; is not found.&quot; &lt;&lt; endl;            continue;        &#125;        int a = lca(u, v);        if (a == u || a == v) &#123;            cout &lt;&lt; a &lt;&lt; &quot; is an ancestor of &quot; &lt;&lt; (a == u ? v : u) &lt;&lt; &quot;.&quot;                 &lt;&lt; endl;        &#125; else &#123;            cout &lt;&lt; &quot;LCA of &quot; &lt;&lt; u &lt;&lt; &quot; and &quot; &lt;&lt; v &lt;&lt; &quot; is &quot; &lt;&lt; a &lt;&lt; &quot;.&quot;                 &lt;&lt; endl;        &#125;    &#125;    return 0;&#125;","categories":["programming","algorithm-practice","PTA"]},{"title":"每日 PTA","url":"/2020/12/15/index/","content":"leetcode medium 纯算法题偏少, 准备每日加一道PTA.\n目标: 每天一道PTA甲级 [PAT (Advanced Level) Practice]\n题号就roll一个随机数\n","categories":["programming","algorithm-practice","PTA"]},{"title":"10. 正则表达式匹配","url":"/2020/12/16/10/","content":"实现支持.和*的正则表达式匹配.\n思路: 动态规划\n用 f[i][j]f[i][j]f[i][j] 表示sss中的前 iii 个字母能否与 ppp 中的前 jjj 个字母匹配, 按照p[j]p[j]p[j]是否为*分为两种情况\n存在*时的转移方程可以这样考虑:\n\n\n匹配 s 末尾的一个字符，将该字符扔掉，而该组合还可以继续进行匹配；(f[i−1][j]f[i-1][j]f[i−1][j])\n\n\n不匹配字符，将该组合扔掉，不再进行匹配。(f[i][j−2]f[i][j - 2]f[i][j−2])\n\n\n最终的状态转移方程:\nf[i][j]={f[i−1][j−1],p[j]≠′∗′ and s[i]=p[j]false,p[j]≠′∗′ and s[i]≠p[j]f[i][j−2] or f[i−1][j],p[j]=′∗′ and s[i]=p[j−1]f[i][j−2],p[j]=′∗′ and s[i]≠p[j−1]f[i][j] = \\begin{cases}\nf[i - 1][j - 1], &amp; p[j] \\neq &#x27;*&#x27; ~and~ s[i] = p[j] \\\\\nfalse, &amp; p[j] \\neq &#x27;*&#x27; ~and~ s[i] \\neq p[j] \\\\\nf[i][j - 2] ~or~ f[i-1][j], &amp; p[j] = &#x27;*&#x27; ~and~ s[i] = p[j-1] \\\\\nf[i][j - 2], &amp; p[j] = &#x27;*&#x27; ~and~ s[i] \\neq p[j-1]\n\\end{cases}\nf[i][j]=⎩⎨⎧​f[i−1][j−1],false,f[i][j−2] or f[i−1][j],f[i][j−2],​p[j]=′∗′ and s[i]=p[j]p[j]=′∗′ and s[i]=p[j]p[j]=′∗′ and s[i]=p[j−1]p[j]=′∗′ and s[i]=p[j−1]​\n代码:\nclass Solution &#123;public:    bool isMatch(string s, string p) &#123;        int m = s.size();        int n = p.size();        auto matches = [&amp;](int i, int j) &#123;            if (i == 0) &#123;                return false;            &#125;            if (p[j - 1] == &#x27;.&#x27;) &#123;                return true;            &#125;            return s[i - 1] == p[j - 1];        &#125;;        vector&lt;vector&lt;int&gt;&gt; f(m + 1, vector&lt;int&gt;(n + 1));        f[0][0] = true;        for (int i = 0; i &lt;= m; ++i) &#123;            for (int j = 1; j &lt;= n; ++j) &#123;                if (p[j - 1] == &#x27;*&#x27;) &#123;                    f[i][j] |= f[i][j - 2];                    if (matches(i, j - 1)) &#123;                        f[i][j] |= f[i - 1][j];                    &#125;                &#125;                else &#123;                    if (matches(i, j)) &#123;                        f[i][j] |= f[i - 1][j - 1];                    &#125;                &#125;            &#125;        &#125;        return f[m][n];    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"],"tags":["动态规划"]},{"title":"1018. 可被 5 整除的二进制前缀","url":"/2021/01/14/1018/","content":"找规律题\nclass Solution &#123;public:    vector&lt;bool&gt; prefixesDivBy5(vector&lt;int&gt;&amp; A) &#123;        vector&lt;bool&gt; ret;        int res = 0;        for(auto&amp;&amp;a:A)&#123;            res = res * 2 + a;            res %= 10;            if(res % 5 == 0)&#123;                ret.push_back(true);            &#125;else&#123;                ret.push_back(false);            &#125;        &#125;        return ret;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"1202. 交换字符串中的元素","url":"/2021/01/11/1202/","content":"#题意\n给定一些可以任意交换位置的组, 求整个字符串的最小字典序\n#思路\n\n用并查集把所有组关联在一起\n对于每个可交换组, 把所有候选字母依次填入\n\n#收获\n\nunordered_map的使用\nc++的for-range语法\n\nfor(auto&amp;[i,group]:groups)&#123;    sort(group.begin(),group.end(),greater());&#125;\n\n重温并查集\n\n#代码\n参考官方题解优化完毕\nclass Solution &#123;public:    vector&lt;int&gt;parents,rank;    int dj_root(int i)&#123;        return (parents[i]==i ? i : parents[i]=dj_root(parents[i]));    &#125;    void dj_union(int a, int b)&#123;        a=dj_root(a);        b=dj_root(b);        if(a==b)return;        if (rank[a]&lt;rank[b])swap(a,b);        rank[a]+=rank[b];        parents[a]=b;    &#125;    string smallestStringWithSwaps(string s, vector&lt;vector&lt;int&gt;&gt;&amp; pairs) &#123;        int n=s.size();        parents.resize(n);        rank.resize(n,1);        for(int i=0;i&lt;n;i++)&#123;            parents[i]=i;        &#125;        for(auto&amp;&amp;pair:pairs)&#123;            dj_union(pair[0],pair[1]);        &#125;        unordered_map&lt;int,string&gt;groups;            for(int i=0;i&lt;n;i++)&#123;            int r=dj_root(i);            groups[r].push_back(s[i]);        &#125;        for(auto&amp;[i,group]:groups)&#123;            sort(group.begin(),group.end(),greater());        &#125;        for(int i=0;i&lt;n;i++)&#123;            int g=dj_root(i);            s[i]=groups[g].back();            groups[g].pop_back();        &#125;        return s;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"123. 买卖股票的最佳时机 III","url":"/2021/01/09/123/","content":"最多可以交易两次\n#思路\n动态规划, 用四个状态变量\n官方题解\n","categories":["programming","algorithm-practice","leetcode"]},{"title":"1203. 项目管理","url":"/2021/01/12/1203/","content":"双重拓扑排序: 组内 &amp; 组间\n拓扑排序\nvector&lt;int&gt; topSort(vector&lt;int&gt;&amp; deg, vector&lt;vector&lt;int&gt;&gt;&amp; graph, vector&lt;int&gt;&amp; items) &#123;    queue&lt;int&gt; Q;    for (auto&amp; item: items) &#123;        if (deg[item] == 0) &#123;            Q.push(item);        &#125;    &#125;    vector&lt;int&gt; res;    while (!Q.empty()) &#123;        int u = Q.front();         Q.pop();        res.emplace_back(u);        for (auto&amp; v: graph[u]) &#123;            if (--deg[v] == 0) &#123;                Q.push(v);            &#125;        &#125;    &#125;    return res.size() == items.size() ? res : vector&lt;int&gt;&#123;&#125;;&#125;\n官方题解\n","categories":["programming","algorithm-practice","leetcode"],"tags":["拓扑排序"]},{"title":"1395. 统计作战单位数","url":"/2021/07/02/1395/","content":"/** * @param &#123;number[]&#125; rating * @return &#123;number&#125; */var numTeams = function (rating) &#123;    var count = 0;    for (let i = 1; i &lt; rating.length - 1; i++) &#123;        let ri = rating[i];        let countx1 = 0, countx2 = 0;        let county1 = 0, county2 = 0;        for (let x = 0; x &lt; i; x++) &#123;            if (rating[x] &lt; ri) countx1++;            if (rating[x] &gt; ri) countx2++;        &#125;        for (let x = i + 1; x &lt; rating.length; x++) &#123;            if (rating[x] &lt; ri) county1++;            if (rating[x] &gt; ri) county2++;        &#125;        count += countx1 * county2 + countx2 * county1;    &#125;    return count;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"1584. 连接所有点的最小费用","url":"/2021/01/19/1584/","content":"实际上是最小生成树\n","categories":["programming","algorithm-practice","leetcode"]},{"title":"1833. 雪糕的最大数量","url":"/2021/07/02/1833/","content":"排序\n/** * @param &#123;number[]&#125; costs * @param &#123;number&#125; coins * @return &#123;number&#125; */var maxIceCream = function (costs, coins) &#123;    costs.sort((a, b) =&gt; a - b);    let count = 0;    for (var i = 0; i &lt; costs.length; i++) &#123;        if (costs[i] &lt;= coins) &#123;            coins -= costs[i];            count++;        &#125; else &#123;            break;        &#125;    &#125;    return count;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"189. 旋转数组","url":"/2021/01/08/189/","content":"#题意\n使用O(n)时间和O(1)空间旋转数组\n#思路\n数学题, GCD\n#感想\n下次遇到这种题画个图会更好理解\nclass Solution &#123;public:    void rotate(vector&lt;int&gt;&amp; nums, int k) &#123;        int n=nums.size();        if(!n || n == 1)return;        if(!(k %= n))return;        for(int c=0, _gcd=gcd(n,k); c&lt;_gcd; c++)&#123;            int tmp = nums[c];            int x=c,_x;            while((_x=(x-k+n)%n)!=c)&#123;                nums[x]=nums[_x];                x=_x;            &#125;            nums[x]=tmp;        &#125;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"],"tags":["GCD"]},{"title":"2. 两数相加","url":"/2020/12/25/2/","content":"指针\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode() : val(0), next(nullptr) &#123;&#125; *     ListNode(int x) : val(x), next(nullptr) &#123;&#125; *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public:    ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123;        ListNode*p1=l1,*p2=l2,*p3,*last=nullptr;        int up=0;        ListNode* root=new ListNode();        p3=root;        while(p1 || p2 || up)&#123;            int n1=0,n2=0;            if(p1)&#123;n1=p1-&gt;val;p1=p1-&gt;next;&#125;            if(p2)&#123;n2=p2-&gt;val;p2=p2-&gt;next;&#125;            up+=n1+n2;            p3-&gt;val=up%10;            last=p3;            p3=p3-&gt;next=new ListNode();            up/=10;        &#125;        delete last-&gt;next;        last-&gt;next=nullptr;        return root;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"205. 同构字符串","url":"/2020/12/27/205/","content":"class Solution &#123;public:    bool isIsomorphic(string s, string t) &#123;        map&lt;char, char&gt;M;        for(int i=0;i&lt;s.size();i++)&#123;            char ss = s[i], tt = t[i];            if(M.find(ss)==M.end())&#123;                if(find_if(M.begin(),M.end(),[&amp;](const map&lt;char, char&gt;::iterator&amp;it)&#123;return it-&gt;second==tt;&#125;)!=M.end())&#123;                    return false;                &#125;else&#123;                    M[ss]=tt;                &#125;            &#125;else&#123;                if(M[ss]!=tt)&#123;                    return false;                &#125;            &#125;        &#125;        return true;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"217. 存在重复元素","url":"/2020/12/13/217/","content":"给定一个整数数组，判断是否存在重复元素。\n如果任意一值在数组中出现至少两次，函数返回 true 。如果数组中每个元素都不相同，则返回 false 。\n没啥好说的:\nclass Solution &#123;public:    bool containsDuplicate(vector&lt;int&gt;&amp; nums) &#123;        sort(nums.begin(),nums.end());        return unique(nums.begin(),nums.end())!=nums.end();    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"228. 汇总区间","url":"/2021/01/10/228/","content":"水题, 注意边界情况\nclass Solution &#123;public:    vector&lt;string&gt; summaryRanges(vector&lt;int&gt;&amp; nums) &#123;        int n = nums.size();        int last = (n&gt;0?nums[0]:0);        vector&lt;string&gt; res;        for(int i=1;i&lt;=n;i++)&#123;            if(i==n || nums[i]!=nums[i-1]+1)&#123;                if (last==nums[i-1])&#123;                    res.push_back(to_string(last));                &#125;else&#123;                    res.push_back(to_string(last)+&quot;-&gt;&quot;+to_string(nums[i-1]));                &#125;                if(i&lt;n) last=nums[i];            &#125;        &#125;        return res;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"23. 合并K个升序链表","url":"/2020/12/18/23/","content":"思路类似归并排序\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode() : val(0), next(nullptr) &#123;&#125; *     ListNode(int x) : val(x), next(nullptr) &#123;&#125; *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public:    ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123;        ListNode newList;        ListNode*head=&amp;newList;        while(true)&#123;            int min=-1,minval=0x3fffffff;            for(int i=0;i&lt;lists.size();i++)&#123;                ListNode*l=lists[i];                if(l &amp;&amp; l-&gt;val &lt; minval)&#123;                    min=i;                    minval=l-&gt;val;                &#125;            &#125;            if(min&lt;0)break;            head-&gt;next=lists[min];            lists[min]=lists[min]-&gt;next;            head=head-&gt;next;        &#125;        head-&gt;next=nullptr;        return newList.next;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"25. K 个一组翻转链表","url":"/2021/03/01/25/","content":"class Solution&#123;  public:    ListNode *reverseKGroup(ListNode *head, int k)    &#123;        ListNode _header(0, head);        ListNode *last = &amp;_header;        head = last;        int count = 0;        while (head) &#123;            head = head-&gt;next;            count++;            if (head &amp;&amp; count == k) &#123;                count -= k;                ListNode *tmp_last_next = last-&gt;next;                reverseK(last, k);                head = last = tmp_last_next;            &#125;        &#125;        return _header.next;    &#125;    void reverseK(ListNode *start, int k)    &#123;        ListNode *cur = start, *next = start-&gt;next;        int count = 0;        while (count &lt; k) &#123;            ListNode *tmp = next-&gt;next;            next-&gt;next = cur;            cur = next;            next = tmp;            count++;        &#125;        start-&gt;next-&gt;next = next;        start-&gt;next = cur;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"290. 单词规律","url":"/2020/12/16/290/","content":"判断给定字符串是不是按照给定模式的短语, 例如&quot;dog dog cat cat&quot;就是&quot;aabb&quot;式的短语\n思路: 用map记录模式字母与单词的对应关系, 如果发现不一致, 返回false, 否则返回true\n注意: 模式中不同字母对应的单词不能相同\n提示:\n按照key查询map可以使用map自带的find()函数\n按照value查询map可以使用find_if()函数\n两者都返回迭代器\nM.find(someKey);find_if(M.begin(), M.end(), [&amp;](const auto&amp; kv)&#123; /* ... */ &#125;);\nclass Solution &#123;public:    bool wordPattern(string pattern, string s) &#123;        map&lt;char, string&gt; M;        vector&lt;string&gt; S;        char *p = strtok((char *)s.c_str(), &quot; &quot;);        if (p) do &#123;            S.push_back(p);        &#125; while (p = strtok(NULL, &quot; &quot;));        if (pattern.size() != S.size()) &#123;            return false;        &#125;        for (int i = 0; i &lt; pattern.size(); i++) &#123;            char ch = pattern[i];            if (M.find(ch) == M.end()) &#123;                if (find_if(M.begin(),M.end(),[&amp;](const auto&amp;p)&#123;return p.second==S[i];&#125;)!=M.end())&#123;                    return false;                &#125;                cout&lt;&lt;ch&lt;&lt;&quot; &quot;&lt;&lt;S[i]&lt;&lt;endl;                M[ch] = S[i];            &#125; else if (M[ch] != S[i]) &#123;                cout&lt;&lt;M[ch]&lt;&lt;&quot; &quot;&lt;&lt;S[i]&lt;&lt;endl;                return false;            &#125;        &#125;        return true;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"3. 无重复字符的最长子串","url":"/2021/03/01/3/","content":"#思路\n滑动窗口\nclass Solution&#123;  public:    int lengthOfLongestSubstring(string s)    &#123;        unordered_set&lt;char&gt; cur_group;        int end = 0;        while (end &lt; s.size() &amp;&amp; cur_group.find(s[end]) == cur_group.end()) &#123;            cur_group.insert(s[end++]);        &#125;        unsigned long max_length = cur_group.size();        for (int i = 0; i &lt; s.size(); i++) &#123;            cur_group.erase(s[i]);            while (end &lt; s.size() &amp;&amp; cur_group.find(s[end]) == cur_group.end()) &#123;                cur_group.insert(s[end++]);            &#125;            max_length = std::max(max_length, cur_group.size());        &#125;        return max_length;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"318. 最大单词长度乘积","url":"/2021/11/17/318/","content":"位运算\nclass Solution &#123;public:    unsigned to_uword(const string &amp;word) &#123;        unsigned res = 0;        for (auto &amp;&amp;c: word) &#123;            res |= 1 &lt;&lt; (c - &#x27;a&#x27;);        &#125;        return res;    &#125;    int maxProduct(vector&lt;string&gt;&amp; words) &#123;        vector&lt;unsigned&gt; uwords;        uwords.reserve(words.size());        for (auto &amp;&amp;w: words) &#123;            uwords.push_back(to_uword(w));        &#125;                int m = 0;        int n = words.size();        for (int i = 0; i &lt; n; i++) &#123;            for (int j = i; j &lt; n; j++) &#123;                auto &amp;s1 = uwords[i];                auto &amp;s2 = uwords[j];                if (s1 &amp; s2) continue;                if (int r = words[i].size() * words[j].size(); r &gt; m) m = r;            &#125;        &#125;        return m;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"37. 解数独","url":"/2021/03/01/37/","content":"#思路\nDFS\nclass Solution &#123;public:    void solveSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123;        solveSudoku1(board, 0, 0);    &#125;    bool solveSudoku1(vector&lt;vector&lt;char&gt;&gt;&amp; board, int i, int j) &#123;        if (i&gt;=9||j&gt;=9) return true;        if (board[i][j]==&#x27;.&#x27;) &#123;            vector&lt;bool&gt; used_numbers(9);            for (int k=0;k&lt;9;k++) &#123;                if (board[i][k]!=&#x27;.&#x27;) &#123;                    used_numbers[board[i][k]-&#x27;1&#x27;]=true;                &#125;            &#125;            for (int k=0;k&lt;9;k++) &#123;                if (board[k][j]!=&#x27;.&#x27;) &#123;                    used_numbers[board[k][j]-&#x27;1&#x27;]=true;                &#125;            &#125;            int grid_i = i / 3;            int grid_j = j / 3;            for (int m=3*grid_i;m&lt;3+3*grid_i;m++) &#123;                for (int n=3*grid_j;n&lt;3+3*grid_j;n++) &#123;                    if (board[m][n]!=&#x27;.&#x27;) &#123;                        used_numbers[board[m][n]-&#x27;1&#x27;]=true;                    &#125;                &#125;            &#125;            for (int ii=0;ii&lt;9;ii++) &#123;                if (used_numbers[ii]==false) &#123;                    board[i][j] = ii+&#x27;1&#x27;;                    if (solveSudoku1(board, i+(j+1)/9, (j+1)%9)) &#123;                        return true;                    &#125;                &#125;            &#125;            board[i][j] = &#x27;.&#x27;;            return false;        &#125;else&#123;            return solveSudoku1(board, i+(j+1)/9, (j+1)%9);        &#125;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"376. 摆动序列","url":"/2020/12/12/376/","content":"思路: 先差分, 然后把所有0和相邻同号的数字(之一)去掉, 剩下的元素数量+2既是最终答案\n注意: 元素个数为0, 元素个数为1, 差分后有0出现\n代码:\nclass Solution&#123;  public:    int wiggleMaxLength(vector&lt;int&gt; &amp;nums)    &#123;        int size = nums.size() - 1;        if (size &lt; 1) &#123; return size + 1; &#125;        int start = -1;        for (int i = 0; i &lt; size; i++) &#123;            nums[i] -= nums[i + 1];            // find first not zero            if (start == -1 &amp;&amp; nums[i] != 0) &#123; start = i; &#125;        &#125;        if (start == -1) &#123;            // none            return 1;        &#125;        int count = 1, want = nums[start];        for (int i = start + 1; i &lt; size; i++) &#123;            if ((nums[i] &gt; 0 &amp;&amp; want &lt; 0) ||                (nums[i] &lt; 0 &amp;&amp; want &gt; 0)) &#123;  // different sign and no zero                want = nums[i];                count++;            &#125;        &#125;        return count + 1;    &#125;&#125;;\n看过题解: 复杂了, 只要统计波峰和波谷的数量就行了\n别人的代码\nclass Solution &#123;    public int wiggleMaxLength(int[] nums) &#123;        int n = nums.length;        if (n &lt; 2) return n;                int count = 1;        for (int i = 1, prev_diff = 0; i &lt; n; i++) &#123;            int diff = nums[i] - nums[i - 1];            if (diff &lt; 0) &#123;                if (prev_diff &gt;= 0)                    count++;                prev_diff = -1;            &#125;            if (diff &gt; 0) &#123;                if (prev_diff &lt;= 0)                    count++;                prev_diff = 1;            &#125;        &#125;        return count;    &#125;&#125;作者：lincs链接：https://leetcode-cn.com/problems/wiggle-subsequence/solution/java-on-solution-by-lincs-6l5r/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":["programming","algorithm-practice","leetcode"]},{"title":"389. 找不同","url":"/2020/12/18/389/","content":"送分题\nclass Solution &#123;public:    char findTheDifference(string s, string t) &#123;        map&lt;char, int&gt;count;        for(auto&amp;&amp;c:s)&#123;            count[c]++;        &#125;        for(auto&amp;&amp;c:t)&#123;            count[c]--;        &#125;        return find_if(count.begin(),count.end(),[&amp;](auto it)&#123;return it.second!=0;&#125;)-&gt;first;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"4. 寻找两个正序数组的中位数","url":"/2020/12/13/4/","content":"思路: 先归并排序, 再寻找中间位置\nclass Solution &#123;public:    double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        int i=0,j=0;        int len1=nums1.size(),len2=nums2.size(),len=len1+len2;        int t, lastt;        while(i+j&lt;=len/2)&#123;            lastt=t;            if(i&lt;len1 &amp;&amp; j&lt; len2)&#123;                int n1=nums1[i];                int n2=nums2[j];                if(n1&lt;n2)&#123;                    t=n1;                    i++;                &#125;else&#123;                    t=n2;                    j++;                &#125;            &#125;else if(i&lt;len1)&#123;                t=nums1[i++];            &#125;else&#123;                t=nums2[j++];            &#125;            cout&lt;&lt;i+j&lt;&lt;&quot;:&quot;&lt;&lt;t&lt;&lt;endl;        &#125;        return (len%2?t:(t+lastt)/2.0);    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"455. 分发饼干","url":"/2020/12/25/455/","content":"排序\nclass Solution &#123;public:    int findContentChildren(vector&lt;int&gt;&amp; g, vector&lt;int&gt;&amp; s) &#123;        sort(g.begin(),g.end()); // 1 2 3        sort(s.begin(),s.end()); // 1 1        int count=0;        for(int i=s.size()-1, gg=g.size()-1;i&gt;=0&amp;&amp;gg&gt;=0;i--,gg--,count++)&#123;            int ss=s[i];            while(gg&gt;=0&amp;&amp;g[gg]&gt;ss)&#123;                gg--;            &#125;            if(gg&lt;0) count--;        &#125;        return count;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"49. 字母异位词分组","url":"/2020/12/14/49/","content":"思路: 字符串排序后作为key, 存到map中再转成vector返回, AC\nclass Solution &#123;public:    vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123;        map&lt;string, vector&lt;string&gt;&gt; m;        for(int i=0;i&lt;strs.size();i++)&#123;            string s=strs[i];            sort(s.begin(),s.end());            m[s].push_back(strs[i]);        &#125;        vector&lt;vector&lt;string&gt;&gt; res;        for(auto &amp;k:m)&#123;            res.push_back(k.second);        &#125;        return res;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"5. 最长回文子串","url":"/2021/03/01/5/","content":"#思路\n动态规划 &lt; 中心扩展算法 &lt; manacher\nclass Solution&#123;  public:    string longestPalindrome(string s)    &#123;        int n = s.size();        int maximum = 1, maxI = 0, maxJ = 0;        vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n, 0));        for (int i = n - 1; i &gt;= 0; i--) &#123;            for (int j = i; j &lt; n; j++) &#123;                if (i == j) &#123;                    dp[i][j] = 1;                &#125; else &#123;                    dp[i][j] = max(dp[i][j - 1], dp[i + 1][j]);                    if (dp[i + 1][j - 1] == j - i - 1 &amp;&amp; s[i] == s[j])                        dp[i][j] = max(dp[i][j], dp[i + 1][j - 1] + 2);                &#125;                if (dp[i][j] &gt; maximum) &#123;                    maximum = dp[i][j];                    maxI = i;                    maxJ = j;                &#125;            &#125;        &#125;        return string(s.begin() + maxI, s.begin() + maxJ + 1);    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"],"tags":["动态规划"]},{"title":"547. 省份数量","url":"/2021/01/07/547/","content":"无向图给出邻接表, 计算连通分量个数\nclass Solution &#123;public:    int findCircleNum(vector&lt;vector&lt;int&gt;&gt;&amp; isConnected) &#123;        int len = isConnected.size();        int count = 0;        vector&lt;int&gt;flag(len,0);        for(int i=0;i&lt;len;i++)&#123;            if(!flag[i])&#123;                count++;                queue&lt;int&gt;found;                found.push(i);                flag[i]=1;                while(!found.empty())&#123;                    int cur = found.front(); found.pop();                    for(int j=0;j&lt;len;j++)&#123;                        if(!flag[j]&amp;&amp;isConnected[cur][j])&#123;                            found.push(j);                            flag[j]=1;                        &#125;                    &#125;                &#125;            &#125;        &#125;        return count;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"],"tags":["图论","连通分量"]},{"title":"645. 错误的集合","url":"/2021/07/04/645/","content":"/** * @param &#123;number[]&#125; nums * @return &#123;number[]&#125; */var findErrorNums = function(nums) &#123;    let ex = new Array(nums.length);    let dup,lost;    for(num of nums)&#123;        if (!ex[num-1])&#123;            ex[num-1]=1;        &#125;else&#123;            dup=num;        &#125;    &#125;    for(let i=0;i&lt;nums.length;i++)&#123;        if(!ex[i])&#123;            lost=i+1;            break;        &#125;    &#125;    return [dup, lost]&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"684. 冗余连接","url":"/2021/01/13/684/","content":"#思路\n有环无向图中找出环上最后出现的边\nclass Solution &#123;public:    vector&lt;int&gt; parents;    int dj_root(int a)&#123;        return a==parents[a]? a: parents[a]=dj_root(parents[a]);    &#125;    void dj_union(int a, int b)&#123;        a=dj_root(a);        b=dj_root(b);        if (a!=b) parents[a]=b;    &#125;    vector&lt;int&gt; findRedundantConnection(vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123;        int n = 0;        for (auto&amp; edge: edges) &#123;            if (edge[0] &gt; n) n = edge[0];            if (edge[1] &gt; n) n = edge[1];        &#125;        parents.resize(n);        for (int i=0; i&lt;n; i++)&#123;            parents[i] = i;        &#125;                for (auto&amp; edge: edges) &#123;            int a = edge[0]-1;            int b = edge[1]-1;            if (dj_root(a) == dj_root(b)) &#123;                return edge;            &#125;            dj_union(a, b);        &#125;        return edges.back();    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"738. 单调递增的数字","url":"/2020/12/15/738/","content":"思路: 对于数字x = an ... ai+1 ai ... a1 a0, 如果任意相邻的ai+1,ai不符合单调递增, 就给原数字减去(ai + 1) * 10 ^ i, 减去之后得到的新数字即为y\n可以发现, 若x与y不完全相同, 则最终结果在第一个不相同位之后的位必定全是9, 按此规律修正结果即可.\nclass Solution &#123;public:    int monotoneIncreasingDigits(int N) &#123;        string s = to_string(N);        string olds=s;                int bor=0;        for(int i=s.size()-1;i&gt;0;i--)&#123;            if(bor)&#123;                s[i]--;                s[i+1]+=10;                bor=0;            &#125;            if(s[i]&lt;s[i-1])&#123;                // -s[i]-1                s[i]=&#x27;9&#x27;;                s[i-1]--;                if(s[i-1]&lt;&#x27;0&#x27;) bor = 1;            &#125;        &#125;                // fix        for(int i=0;i&lt;s.size();i++)&#123;            if(s[i]&lt;olds[i])&#123;                for(int j=i+1;j&lt;s.size();j++)&#123;                    s[j]=&#x27;9&#x27;;                &#125;                break;            &#125;        &#125;                // restore        int val =0;        for(int i=0;i&lt;s.size();i++)&#123;            val=val*10+s[i]-&#x27;0&#x27;;        &#125;        return val;    &#125;&#125;;","categories":["programming","algorithm-practice","leetcode"]},{"title":"每日 leetcode","url":"/2020/12/12/index/","content":"决定开始刷leetcode了\n目标:\n每天一道如果每日一题是easy, 就加一道hard\n如果每日一题是easy, 就加一道medium\n","categories":["programming","algorithm-practice","leetcode"]}]